<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Spark on YARN 部署</title>
    <url>/2016/03/29/spark-on-yarn/</url>
    <content><![CDATA[<h1 id="1-说明"><a href="#1-说明" class="headerlink" title="1.说明:"></a>1.说明:</h1><ol>
<li>首先要部署好Hadoop集群包括HDFS和YARN,这里不再赘述.</li>
<li>Spark on YARN模式,不需要启动spark的master和worker,master的工作由YARN完成.</li>
<li>相关路径根据实际情况修改.</li>
<li>1.4和1.6版本部署过程类似.<span id="more"></span></li>
</ol>
<h1 id="2-部署"><a href="#2-部署" class="headerlink" title="2.部署"></a>2.部署</h1><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>从官网(<a href="http://spark.apache.org/)%E4%B8%8B%E8%BD%BD%E5%AF%B9%E5%BA%94Hadoop%E7%89%88%E6%9C%AC%E7%9A%84%E5%8C%85,%E5%A6%82spark-1.6.1-bin-hadoop2.4.tgz">http://spark.apache.org/)下载对应Hadoop版本的包,如spark-1.6.1-bin-hadoop2.4.tgz</a>.<br>解压至某个目录,如”&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;spark-1.6.1-bin-hadoop2.4”.<br>解压后得到如下文件</p>
<p><img src="http://thinkerblogimgs.qiniudn.com/16-4-11/66394000.jpg-l"></p>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="配置文件修改"><a href="#配置文件修改" class="headerlink" title="配置文件修改"></a>配置文件修改</h3><p>配置文件在conf目录中.<br>配置环境变量<br>cp spark-env.sh.template spark-env.sh<br>在spark-env.sh中添加如下:<br>export HADOOP_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;hadoop-2.4.1<br>export YARN_CONF_DIR&#x3D;$HADOOP_HOME&#x2F;etc&#x2F;hadoop&#x2F;</p>
<p>配置spark选项<br>cp spark-defaults.conf.template spark-defaults.conf<br>添加如下:<br>spark.master                     yarn            #指定使用yarn<br>spark.driver.memory              1g              #指定driver使用的内存<br>spark.executor.memory            512m            #指定executor使用的内存</p>
<p>选填:<br>spark.executor.extraClassPath   &#x2F;data&#x2F;hadoop&#x2F;hadoop&#x2F;hadoop-2.4.1&#x2F;share&#x2F;hadoop&#x2F;common&#x2F;lib&#x2F;hadoop-lzo-0.4.20.jar<br>spark.driver.extraClassPath     &#x2F;data&#x2F;hadoop&#x2F;hadoop&#x2F;hadoop-2.4.1&#x2F;share&#x2F;hadoop&#x2F;common&#x2F;lib&#x2F;*<br>spark.kryoserializer.buffer.max  256m</p>
<p>另外也可以设置一下Spark的log4j配置文件，使得屏幕中不打印额外的INFO信息:<br>cp log4j.properties.template log4j.properties<br>修改一行<br>log4j.rootCategory&#x3D;WARN, console</p>
<h2 id="启动一个spark应用"><a href="#启动一个spark应用" class="headerlink" title="启动一个spark应用"></a>启动一个spark应用</h2><p>执行 .&#x2F;bin&#x2F;spark-shell</p>
<p>输出将包括如下所示</p>
<p><img src="http://thinkerblogimgs.qiniudn.com/16-4-11/77012870.jpg-l"></p>
<p>浏览器访问当前机器的8088端口,将可以看到当前启动的driver的web界面,如下:</p>
<p><img src="http://thinkerblogimgs.qiniudn.com/16-4-11/49321272.jpg-l"></p>
<p>点击’Environment’标签,将可以看到相关环境变量和属性的值.</p>
<p>浏览器访问YARN的管理页面,可以看到有一个SPARK应用正在运行(如下),至此spark就可以跑在YARN上了.</p>
<p><img src="http://thinkerblogimgs.qiniudn.com/16-4-11/62275273.jpg-l"></p>
<h1 id="3-运行模式"><a href="#3-运行模式" class="headerlink" title="3.运行模式"></a>3.运行模式</h1><p>Spark on YARN有两种运行模式cluster和client,spark-shell和spark-sql交互式的应用只支持client模式.<br>两种模式区别如下:</p>
<blockquote>
<p>从广义上讲，yarn-cluster适用于生产环境；而yarn-client适用于交互和调试，也就是希望快速地看到application的输出。</p>
</blockquote>
<p>可以参考这篇博客:Spark:Yarn-cluster和Yarn-client区别与联系(<a href="http://www.iteblog.com/archives/1223">http://www.iteblog.com/archives/1223</a>).</p>
<h1 id="4-参考文献"><a href="#4-参考文献" class="headerlink" title="4.参考文献:"></a>4.参考文献:</h1><p>Running Spark on YARN<br><a href="http://spark.apache.org/docs/latest/running-on-yarn.html">http://spark.apache.org/docs/latest/running-on-yarn.html</a></p>
]]></content>
      <tags>
        <tag>Spark</tag>
        <tag>YARN</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive添加自定义UDF函数</title>
    <url>/2016/04/11/add-udf-to-hive/</url>
    <content><![CDATA[<h1 id="1-编写UDF类"><a href="#1-编写UDF类" class="headerlink" title="1 编写UDF类"></a>1 编写UDF类</h1><p>以简单的处理单个字段的UDF函数为例,开发自定义UDF函数需要继承’org.apache.hadoop.hive.ql.exec.UDF’类.<br>可以通过Maven添加,pom文件中加入(版本号跟Hive版本一致即可):</p>
<span id="more"></span>

<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.13.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>最简单的实现只需继承UDF类,并实现evaluate函数.如下UDF函数用来将IP(v4)地址转换为整数.</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> ml.liam8.hive;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.Description;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Convert IPv4 to a num which type is Long in java.</span></span><br><span class="line"><span class="comment">* Created by Liam on 2016/4/11.</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">@Description(name = &quot;IpToNum&quot;, value = &quot;_FUNC_(ip) - Convert IPv4 to a num(long).&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">IpToNum</span> <span class="keyword">extends</span> <span class="title class_">UDF</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">evaluate</span><span class="params">(String ip)</span> &#123;</span><br><span class="line">      String[] nums = ip.split(<span class="string">&quot;\\.&quot;</span>);</span><br><span class="line">      <span class="keyword">return</span> Long.parseLong(nums[<span class="number">3</span>]) + Long.parseLong(nums[<span class="number">2</span>]) * <span class="number">256</span></span><br><span class="line">         + Long.parseLong(nums[<span class="number">1</span>]) * <span class="number">65536</span> + Long.parseLong(nums[<span class="number">0</span>]) * <span class="number">16777216</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>evaluate方法的输入输出即是UDF函数的输入输出.<br>Description注解部分提供函数的帮助信息.</p>
<p>执行:<br><code>desc function test.iptonum</code><br>输出:<br><code>test.iptonum(ip) - Convert IPv4 to a num(long).</code></p>
<p>源码已上传 <a href="https://github.com/Liam8/HiveUDF">Github</a></p>
<h1 id="2-部署及创建UDF函数"><a href="#2-部署及创建UDF函数" class="headerlink" title="2 部署及创建UDF函数"></a>2 部署及创建UDF函数</h1><p>ps:Hive0.13及以后版本适用</p>
<h3 id="部署jar包"><a href="#部署jar包" class="headerlink" title="部署jar包"></a>部署jar包</h3><p>将jar包复制到HDFS.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hdfs -dfs -put udfs-0.1.jar &#x27;hdfs:///user/hadoop/hiveUDF&#x27;</span><br></pre></td></tr></table></figure>
<h3 id="创建永久函数"><a href="#创建永久函数" class="headerlink" title="创建永久函数"></a>创建永久函数</h3><p>需在Hive中执行sql语句,格式如下:</p>
<pre><code>CREATE FUNCTION [db_name.]function_name AS class_name
  [USING JAR|FILE|ARCHIVE &#39;file_uri&#39; [, JAR|FILE|ARCHIVE &#39;file_uri&#39;] ];
</code></pre>
<p>如:</p>
<pre><code>create function test.iptonum as &#39;com.liam8.hive.IpToNum&#39; using jar &#39;hdfs:///user/hadoop/hiveUDF/udfs-0.1.jar&#39;
</code></pre>
<p>函数需要属于某个库,如这里是’test’,当其他库调用时,需要加上库名,如’test.iptonum’.</p>
<p>调用方式: <code>select test.iptonum(&#39;127.0.0.1&#39;);</code></p>
<h3 id="创建临时函数"><a href="#创建临时函数" class="headerlink" title="创建临时函数"></a>创建临时函数</h3><p>临时函数只在当前session中有效,临时函数不能指定库.</p>
<pre><code>create temporary function iptonum as &#39;com.liam8.hive.IpToNum&#39; using jar &#39;hdfs:///user/hadoop/hiveUDF/udfs-0.1.jar&#39;
</code></pre>
<p>调用方式: <code>select iptonum(&#39;127.0.0.1&#39;);</code></p>
<h1 id="3-参考资料"><a href="#3-参考资料" class="headerlink" title="3 参考资料"></a>3 参考资料</h1><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-PermanentFunctions">LanguageManualDDL-PermanentFunctions</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins">HivePlugins</a></p>
]]></content>
      <tags>
        <tag>Hive</tag>
        <tag>UDF</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World, Hello Blog</title>
    <url>/2016/12/25/hello-blog/</url>
    <content><![CDATA[<h2 id="Hello"><a href="#Hello" class="headerlink" title="Hello!"></a>Hello!</h2><p>Today is this blog’s first day !</p>
<h3 id="Merry-Christmas"><a href="#Merry-Christmas" class="headerlink" title="Merry Christmas !"></a>Merry Christmas !</h3>]]></content>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark work with MySQL</title>
    <url>/2016/12/27/spark-work-with-mysql/</url>
    <content><![CDATA[<p>With spark 2.0.x,we can use DataFrameReader and DataFrameWriter.<br>Use SparkSession.read to access DataFrameReader and use Dataset.write to access DataFrameWriter.</p>
<span id="more"></span>

<p>Suppose using spark-shell.</p>
<h1 id="read-example"><a href="#read-example" class="headerlink" title="read example"></a>read example</h1><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> prop=<span class="keyword">new</span> java.util.<span class="type">Properties</span>()</span><br><span class="line">prop.put(<span class="string">&quot;user&quot;</span>,<span class="string">&quot;username&quot;</span>)</span><br><span class="line">prop.put(<span class="string">&quot;password&quot;</span>,<span class="string">&quot;yourpassword&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> url=<span class="string">&quot;jdbc:mysql://host:3306/db_name&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df=spark.read.jdbc(url,<span class="string">&quot;table_name&quot;</span>,prop)</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure>

<h1 id="read-example2"><a href="#read-example2" class="headerlink" title="read example2"></a>read example2</h1><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> jdbcDF = spark.read</span><br><span class="line">  .format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql:dbserver&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;schema.tablename&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;username&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;password&quot;</span>)</span><br><span class="line">  .load()</span><br></pre></td></tr></table></figure>

<h1 id="read-example3"><a href="#read-example3" class="headerlink" title="read example3"></a>read example3</h1><p>If you want to read data from a query result rather than a table.</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> sql=<span class="string">&quot;&quot;&quot;select * from db.your_table where id&gt;1&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">val</span> jdbcDF = spark.read</span><br><span class="line">  .format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql:dbserver&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;dbtable&quot;</span>,  <span class="string">s&quot;( <span class="subst">$sql</span> ) t&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;username&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;password&quot;</span>)</span><br><span class="line">  .load()</span><br></pre></td></tr></table></figure>

<h1 id="write-example"><a href="#write-example" class="headerlink" title="write example"></a>write example</h1><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SaveMode</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> prop=<span class="keyword">new</span> java.util.<span class="type">Properties</span>()</span><br><span class="line">prop.put(<span class="string">&quot;user&quot;</span>,<span class="string">&quot;username&quot;</span>)</span><br><span class="line">prop.put(<span class="string">&quot;password&quot;</span>,<span class="string">&quot;yourpassword&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> url=<span class="string">&quot;jdbc:mysql://host:3306/db_name&quot;</span></span><br><span class="line"><span class="comment">//df is a dataframe contains the data which you want to write.</span></span><br><span class="line">df.write.mode(<span class="type">SaveMode</span>.<span class="type">Append</span>).jdbc(url,<span class="string">&quot;table_name&quot;</span>,prop)</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Spark</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka 0.10 常用运维命令</title>
    <url>/2017/03/02/kafka-operation-command/</url>
    <content><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>Kafka是由LinkedIn开发的一个分布式的消息系统，它以可水平扩展和高吞吐率而被广泛使用，现在已经是Apache的项目。</p>
<p>Kafka系统自带了丰富的运维管理工具，都是基于命令行的，本文主要介绍一些常用的命令。</p>
<span id="more"></span>

<p>读者需要对Kafka已经有入门级的了解。</p>
<h1 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h1><p>以下命令都是在Kafka的主目录下执行的。</p>
<h2 id="启动Kafka"><a href="#启动Kafka" class="headerlink" title="启动Kafka"></a>启动Kafka</h2><p>启动命令需要指定配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bin/kafka-server-start.sh config/server.properties</span><br></pre></td></tr></table></figure>

<p>默认的启动方式并不是守护进程，可以添加’nohup’和’&amp;’让进程保持在后台运行，即使断开SSH终端连接。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nohup bin/kafka-server-start.sh config/server.properties &gt; ~/kafka-server-start.out 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<h2 id="topic-相关"><a href="#topic-相关" class="headerlink" title="topic 相关"></a>topic 相关</h2><h3 id="列出-topic"><a href="#列出-topic" class="headerlink" title="列出 topic"></a>列出 topic</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --list --zookeeper localhost:2181</span><br></pre></td></tr></table></figure>
<p>–zookeeper：指定zookeeper地址。</p>
<h3 id="创建-topic"><a href="#创建-topic" class="headerlink" title="创建 topic"></a>创建 topic</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 4 --topic test_topic</span><br></pre></td></tr></table></figure>
<p>–replication-factor：备份数，就是数据保存几份，这里设置为3，表示该topic的数据会在3个节点上各保存一份。</p>
<p>–partitions：分区数</p>
<h3 id="topic-详情"><a href="#topic-详情" class="headerlink" title="topic 详情"></a>topic 详情</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test_topic</span><br></pre></td></tr></table></figure>
<h3 id="删除-topic"><a href="#删除-topic" class="headerlink" title="删除 topic"></a>删除 topic</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic test_topic</span><br></pre></td></tr></table></figure>
<p>需要开启一个配置项才可以删除topic。</p>
<p>配置项在server.properties中：<br><code>delete.topic.enable = true</code></p>
<p>ps: 有个特殊的topic叫 __consumer_offsets，是Kafka内部使用的，不允许删除的。</p>
<h2 id="生产者消费者相关"><a href="#生产者消费者相关" class="headerlink" title="生产者消费者相关"></a>生产者消费者相关</h2><h3 id="生产者"><a href="#生产者" class="headerlink" title="生产者"></a>生产者</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test_topic</span><br></pre></td></tr></table></figure>
<p>–broker-list:kafka broker地址</p>
<h3 id="消费者"><a href="#消费者" class="headerlink" title="消费者"></a>消费者</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test_topic</span><br></pre></td></tr></table></figure>

<h2 id="分区相关"><a href="#分区相关" class="headerlink" title="分区相关"></a>分区相关</h2><h3 id="增加分区"><a href="#增加分区" class="headerlink" title="增加分区"></a>增加分区</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic test_topic --partitions 4</span><br></pre></td></tr></table></figure>
<p>–partitions:指明将分区增加至多少个。<br>分区不能减少只能增加。</p>
<h3 id="改变分区分布和副本数"><a href="#改变分区分布和副本数" class="headerlink" title="改变分区分布和副本数"></a>改变分区分布和副本数</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file test_topic.json --execute</span><br></pre></td></tr></table></figure>
<p>–reassignment-json-file:指定一个分区方案文件。</p>
<p>分区方案文件是一个json格式的文本文件，需要自己编写。</p>
<p>举个例子，test_topic.json内容如下：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;partitions&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span> <span class="string">&quot;test_topic&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span> <span class="string">&quot;test_topic&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span><span class="number">3</span><span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span> <span class="string">&quot;test_topic&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="number">3</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span> <span class="string">&quot;test_topic&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span> <span class="number">3</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>解释其中一部分，剩下的自然也会明白。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&quot;topic&quot;: &quot;test_topic&quot;, //topic名称</span><br><span class="line">&quot;partition&quot;: 0,        //分区编号，从0开始</span><br><span class="line">&quot;replicas&quot;: [1,2]      //指定保存在哪个broker,这里是填写broker id。写两个id，意思就是有两份备份啦。</span><br></pre></td></tr></table></figure>

<p>PS:这个方法不能用来增加分区数。第一个replicas就是默认leader。</p>
<p>改变分区和备份数的操作并不能立即完成，而是需要一段时间，内部操作会在后台运行。<br>所以需要…</p>
<p>检查进度</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file test_topic.json --verify</span><br></pre></td></tr></table></figure>

<p>以上。</p>
]]></content>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>一条通往服务器所有端口的隧道</title>
    <url>/2017/06/13/ssh-tunnel-with-windows/</url>
    <content><![CDATA[<p><img src="https://raw.githubusercontent.com/Liam8/img/master/ssh-tunnel/tunnel-899053_640.jpg"></p>
<h1 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h1><p>通常为了安全,服务器需要通过跳板机访问，服务器对外网暴露的端口也严格限制。这种情况下若要在本地<br>访问服务器上的服务或系统就会比较蛋疼。<br>有一个简单的解决方案，就是在本地和跳板机之间建立SSH隧道。SSH隧道提供了一个网络代理服务，<br>通过该代理服务可以直接访问跳板机所在的局域网，即服务器上的任意端口，服务都可以直接访问。</p>
<p>本文介绍的方法比一般的端口映射更方便，不需要为每个端口配置一条SSH隧道，包括Windows、Mac、Linux上的操作方法。</p>
<span id="more"></span>

<h1 id="SSH隧道建立"><a href="#SSH隧道建立" class="headerlink" title="SSH隧道建立"></a>SSH隧道建立</h1><h2 id="Mac-amp-Linux-版"><a href="#Mac-amp-Linux-版" class="headerlink" title="Mac &amp; Linux 版"></a>Mac &amp; Linux 版</h2><p>非常简单，只需一条命令即可建立SSH隧道。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh user@host -ND 127.0.0.1:1080</span><br></pre></td></tr></table></figure>
<p>其实就是在常规的SSH命令加上<code>-D</code>参数，开启动态端口转发，使SSH成为了SOCKS server，在后台提供网络服务。</p>
<p>而<code>-N</code>参数是让ssh不要返回命令行终端，因为我们不需要发送命令，只是做转发。</p>
<p>1080是绑定的本地端口，也就是SOCKS server提供服务的端口，可以换成其他端口号。</p>
<p>127.0.0.1表示只能有你本机访问这个服务，去掉IP只留下端口号的话，就没有这个限制了。</p>
<p>PS:SSH隧道相当于在服务器的防火墙上打了个洞，可能有安全隐患，所以建议加上仅限本机访问的限制。</p>
<h2 id="Windows版"><a href="#Windows版" class="headerlink" title="Windows版"></a>Windows版</h2><p>这里使用Windows上常用的XShell做说明。</p>
<h3 id="1-配置常规的SSH连接"><a href="#1-配置常规的SSH连接" class="headerlink" title="1.配置常规的SSH连接"></a>1.配置常规的SSH连接</h3><p>配置用户名，密码，主机地址(通常是跳板机)等。</p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/ssh-tunnel/xshell-1.png" alt="xshell-1"></p>
<h3 id="2-添加隧道"><a href="#2-添加隧道" class="headerlink" title="2.添加隧道"></a>2.添加隧道</h3><p><img src="https://raw.githubusercontent.com/Liam8/img/master/ssh-tunnel/xshell-2.png" alt="xshell-2"></p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/ssh-tunnel/xshell-3.png" alt="xshell-3"></p>
<h3 id="3-其他"><a href="#3-其他" class="headerlink" title="3.其他"></a>3.其他</h3><p><img src="https://raw.githubusercontent.com/Liam8/img/master/ssh-tunnel/xshell-4.png" alt="xshell-4"></p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/ssh-tunnel/xshell-5.png" alt="xshell-5"></p>
<h1 id="SSH隧道使用"><a href="#SSH隧道使用" class="headerlink" title="SSH隧道使用"></a>SSH隧道使用</h1><h2 id="让浏览器访问内网服务"><a href="#让浏览器访问内网服务" class="headerlink" title="让浏览器访问内网服务"></a>让浏览器访问内网服务</h2><p>这里介绍Chrome浏览器+SwitchyOmega插件的方法。</p>
<p>SwitchyOmega是一个Chrome插件，下载安装地址：<br><a href="https://chrome.google.com/webstore/detail/proxy-switchyomega/padekgcemlokbadohgkifijomclgjgif">Chrome应用商店</a><br>或者 <a href="https://www.switchyomega.com/download/">官网下载</a></p>
<p>安装好后，找到插件图标（一个圆圈），单击图标-&gt;选项，打开配置页面。<br>点击侧边栏的“新建情景模式（New Profile）”，添加一个代理服务器（Proxy Profile），<br>配置如下图。注意端口要填写前面SSH隧道服务的端口号。</p>
<p>如果侧边栏中已经有一个默认的’proxy‘模式，也可以直接拿来修改。</p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/ssh-tunnel/switch-1.png" alt="switch-1"></p>
<p>配置好后单击SwitchyOmega的图标，切换到新建的情景模式，然后所有的浏览器请求都会以SSH隧道作为<br>代理了，这时你应该已经可以访问服务器上的任何web服务了。</p>
<p>但是！我们并不需要所有的浏览器流量都走代理哇。SwitchyOmega其实可以根据URL的规则，自动选择走不走代理。<br>默认已经存在的一个情景模式’auto switch‘，就是一个可以根据规则来自动选择代理服务的模式。<br>比如在’auto switch‘模式中配置一条规则，填写<code>10.1.*</code>，并选择proxy模式。这就代表将<code>10.1.</code><br>开头的所有请求都用proxy模式转发，而其他不满足规则的请求，将命中最后一条’默认‘规则，<br>进行’直接访问‘，即不使用任何代理服务。</p>
<h2 id="与Proxifier配合使用"><a href="#与Proxifier配合使用" class="headerlink" title="与Proxifier配合使用"></a>与Proxifier配合使用</h2><p>可以让任意程序访问服务器上的服务及端口,实现如下操作：</p>
<ul>
<li>使用客户端（如Navicat）访问服务上的数据库(如没有对公网暴露的MySQL)；</li>
<li>让本地运行的代码访问服务器内网服务，比如HDFS,Hive,Hbase,ES等等。（对于大数据开发非常实用，<br>可以愉快的打断点调试Spark应用）</li>
</ul>
<h3 id="配置方法"><a href="#配置方法" class="headerlink" title="配置方法"></a>配置方法</h3><h4 id="1-配置代理服务器"><a href="#1-配置代理服务器" class="headerlink" title="1 配置代理服务器"></a>1 配置代理服务器</h4><p><img src="https://raw.githubusercontent.com/Liam8/img/master/ssh-tunnel/proxifier-0.png"></p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/ssh-tunnel/proxifier-1.png"></p>
<h4 id="2-配置规则"><a href="#2-配置规则" class="headerlink" title="2 配置规则"></a>2 配置规则</h4><p><img src="https://raw.githubusercontent.com/Liam8/img/master/ssh-tunnel/proxifier-2.png"></p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/ssh-tunnel/proxifier-3.png"></p>
<h3 id="3-使用说明"><a href="#3-使用说明" class="headerlink" title="3 使用说明"></a>3 使用说明</h3><p>配置好规则后，本地应用就可以直接访问服务器的内网IP了。</p>
<p>比如连接数据库：</p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/ssh-tunnel/navicat.png"></p>
]]></content>
      <tags>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title>自定义开发Spark ML机器学习类 - 1</title>
    <url>/2018/02/04/diy-spark-ml-1/</url>
    <content><![CDATA[<h1 id="初窥门径"><a href="#初窥门径" class="headerlink" title="初窥门径"></a>初窥门径</h1><p>Spark的MLlib组件内置实现了很多常见的机器学习算法,包括数据抽取,分类,聚类,关联分析,协同过滤等等.<br>然鹅,内置的算法并不能满足我们所有的需求,所以我们还是经常需要自定义ML算法.</p>
<span id="more"></span>

<p>MLlib提供的API分为两类:</p>
<ul>
<li>1.基于DataFrame的API,属于spark.ml包.</li>
<li>2.基于RDD的API, 属于spark.mllib包.</li>
</ul>
<p>从Spark 2.0开始,Spark的API全面从RDD转向DataFrame,MLlib也是如此,官网原话如下:</p>
<blockquote>
<p>Announcement: DataFrame-based API is primary API</p>
<p>The MLlib RDD-based API is now in maintenance mode.</p>
</blockquote>
<p>所以本文将介绍基于DataFrame的自定义ml类编写方法.不涉及具体算法,只讲扩展ml类的方法.</p>
<h1 id="略知一二"><a href="#略知一二" class="headerlink" title="略知一二"></a>略知一二</h1><p>官方文档并没有介绍如何自定义ml类,所以只有从源码入手,看看源码里面是怎么实现的.</p>
<p>找一个最简单的内置算法入手,这个算法就是内置的分词器,Tokenizer.</p>
<p>Tokenizer只是简单的将文本以空白部分进行分割,只适合给英文进行分词,所以它的实现及其简短,源码如下:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.spark.ml.feature</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.annotation.<span class="type">Since</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.<span class="type">UnaryTransformer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.param._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.util._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">ArrayType</span>, <span class="type">DataType</span>, <span class="type">StringType</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A tokenizer that converts the input string to lowercase and then splits it by white spaces.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @see [[RegexTokenizer]]</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Since</span>(<span class="string">&quot;1.2.0&quot;</span>)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tokenizer</span> <span class="title">@Since</span>(<span class="params">&quot;1.4.0&quot;</span>) (<span class="params">@<span class="type">Since</span>(&quot;1.4.0&quot;</span>) <span class="title">override</span> <span class="title">val</span> <span class="title">uid</span></span>: <span class="type">String</span>)</span><br><span class="line">  <span class="keyword">extends</span> <span class="type">UnaryTransformer</span>[<span class="type">String</span>, <span class="type">Seq</span>[<span class="type">String</span>], <span class="type">Tokenizer</span>] <span class="keyword">with</span> <span class="type">DefaultParamsWritable</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Since</span>(<span class="string">&quot;1.2.0&quot;</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>() = <span class="keyword">this</span>(<span class="type">Identifiable</span>.randomUID(<span class="string">&quot;tok&quot;</span>))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">createTransformFunc</span></span>: <span class="type">String</span> =&gt; <span class="type">Seq</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    _.toLowerCase.split(<span class="string">&quot;\\s&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">validateInputType</span></span>(inputType: <span class="type">DataType</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    require(inputType == <span class="type">StringType</span>, <span class="string">s&quot;Input type must be string type but got <span class="subst">$inputType</span>.&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">outputDataType</span></span>: <span class="type">DataType</span> = <span class="keyword">new</span> <span class="type">ArrayType</span>(<span class="type">StringType</span>, <span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Since</span>(<span class="string">&quot;1.4.1&quot;</span>)</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>(extra: <span class="type">ParamMap</span>): <span class="type">Tokenizer</span> = defaultCopy(extra)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Since</span>(<span class="string">&quot;1.6.0&quot;</span>)</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Tokenizer</span> <span class="keyword">extends</span> <span class="title">DefaultParamsReadable</span>[<span class="type">Tokenizer</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Since</span>(<span class="string">&quot;1.6.0&quot;</span>)</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">load</span></span>(path: <span class="type">String</span>): <span class="type">Tokenizer</span> = <span class="keyword">super</span>.load(path)</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>简单分析下源码:</p>
<ul>
<li>Tokenizer继承了UnaryTransformer类.unary是’一元’的意思,也是说这个类实现的是类似一元函数的功能,一个输入变量,一个输出.直接看UnaryTransformer的源码注释:</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* :: DeveloperApi ::</span></span><br><span class="line"><span class="comment">* Abstract class for transformers that take one input column, apply transformation, and output the</span></span><br><span class="line"><span class="comment">* result as a new column.</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure>
<p>DeveloperApi表明这是一个开发级API,开发者可以用,不会有权限问题(源码中有很多private[spark]的类,是不允许外部调用的).<br>注释的大意就是:这是一个为实现transformers准备的抽象类,以一个字段(列)为输入,输出一个新字段(列).</p>
<p>所以实际上就是实现一个Transformer,只是这个Transformer有指定的输入字段和输出字段.</p>
<ul>
<li>UnaryTransformer类中只有两个抽象方法.<br>一个是createTransformFunc,是最核心的方法,这个方法需要返回一个函数,这个函数的参数即Transformer的输入字段的值,返回值为Transformer的输出字段的值.看看Tokenizer中的实现,就明白了.</li>
</ul>
<p>另一个是outputDataType,这个方法用来返回输出字段的类型.</p>
<ul>
<li><p>validateInputType方法是用来检查输入字段类型的,看需要实现.</p>
</li>
<li><p>Tokenizer混入了DefaultParamsWritable特质,使得自己可以被保存.<br>  对应的object Tokenizer伴生对象,用来读取已保存的Tokenizer.</p>
</li>
<li><p>值得注意的是,Transformer类是PipelineStage类的子类,所以Transformer的子类,包括我们自定义的,是可以直接用在<a href="http://spark.apache.org/docs/latest/ml-pipeline.html">ML Pipelines</a>中的.这就厉害了,说明自定义的算法类,可以无缝与内置机器学习算法打配合,还能利用Pipeline的调优工具(model selection,Cross-Validation等).</p>
</li>
</ul>
<h1 id="初出茅庐"><a href="#初出茅庐" class="headerlink" title="初出茅庐"></a>初出茅庐</h1><p>看完源码,基本套路已经明了,不如动手抄一个,不,敲一个.<br>依葫芦画瓢,实现一个正则提取的Transformer.</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> util.matching.<span class="type">Regex</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.<span class="type">UnaryTransformer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.param.<span class="type">Param</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.util.<span class="type">Identifiable</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 正则提取器</span></span><br><span class="line"><span class="comment">  * 将匹配指定正则表达式的全部子字符串，提取到array[string]中.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RegexExtractor</span>(<span class="params">override val uid: <span class="type">String</span></span>)</span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">UnaryTransformer</span>[<span class="type">String</span>, <span class="type">Seq</span>[<span class="type">String</span>], <span class="type">RegexExtractor</span>] &#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>() = <span class="keyword">this</span>(<span class="type">Identifiable</span>.randomUID(<span class="string">&quot;RegexExtractor&quot;</span>))</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 参数:正则表达式</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @group param</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="keyword">final</span> <span class="keyword">val</span> regex = <span class="keyword">new</span> <span class="type">Param</span>[<span class="type">Regex</span>](<span class="keyword">this</span>, <span class="string">&quot;RegexExpr&quot;</span>, <span class="string">&quot;正则表达式&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** @group setParam */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setRegexExpr</span></span>(value: <span class="type">String</span>): <span class="keyword">this</span>.<span class="keyword">type</span> = set(regex, <span class="keyword">new</span> <span class="type">Regex</span>(value))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">outputDataType</span></span>: <span class="type">DataType</span> = <span class="keyword">new</span> <span class="type">ArrayType</span>(<span class="type">StringType</span>, <span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">validateInputType</span></span>(inputType: <span class="type">DataType</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    require(inputType == <span class="type">DataTypes</span>.<span class="type">StringType</span>,</span><br><span class="line">      <span class="string">s&quot;Input type must be string type but got <span class="subst">$inputType</span>.&quot;</span></span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">createTransformFunc</span></span>: <span class="type">String</span> =&gt; <span class="type">Seq</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    parseContent</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 数据处理</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">parseContent</span></span>(text: <span class="type">String</span>): <span class="type">Seq</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    <span class="keyword">if</span> (text == <span class="literal">null</span> || text.isEmpty) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="type">Seq</span>.empty[<span class="type">String</span>]</span><br><span class="line">    &#125;</span><br><span class="line">    $(regex).findAllIn(text).toSeq</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这个类结构与Tokenizer源码基本差不多,多用到的Param类,是一个参数的包装类.<br>作用是self-contained documentation and optionally default value.<br>其实就是把参数的值,文档,默认值等属性组合成一个类,方便调用.</p>
<p>比如上面定义的regex参数,就可以用$(regex)这样的方式直接调用.</p>
<p>另外在org.apache.spark.ml.param中有很多内置的Param类,可以直接使用.</p>
<p>同时org.apache.spark.ml.param.shared中有很多辅助引入参数的特质,比如HasInputCols特质,你的自定义Transformer只要混入这个特质就拥有了inputCols参数.不过目前shared中特质的作用域是private[ml],也就是说不能直接引用,而是要copy一份代码到自己的项目,并修改作用域才行.<br>关于这个作用域的问题,有人在spark的jira上提到,提议将其作为DeveloperApi开放出来,我也投了一票表示支持.后来在2017年11月终于resolved,该问题将在Spark2.3.0中解决.<a href="https://issues.apache.org/jira/browse/SPARK-7146">详情戳我</a></p>
<h1 id="粗懂皮毛"><a href="#粗懂皮毛" class="headerlink" title="粗懂皮毛"></a>粗懂皮毛</h1><p>自定义的类写好了,该怎么用呢? 当然是跟内置的一样啦.上栗子:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> regex=<span class="string">&quot;nidezhengze&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> tranTitle = <span class="keyword">new</span> <span class="type">RegexExtractor</span>()</span><br><span class="line">	.setInputCol(<span class="string">&quot;title&quot;</span>)</span><br><span class="line">	.setOutputCol(<span class="string">&quot;title_price_texts&quot;</span>)</span><br><span class="line">	.setRegexExpr(regex)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> pipeline = <span class="keyword">new</span> <span class="type">Pipeline</span>().setStages(<span class="type">Array</span>(</span><br><span class="line">	tranTitle</span><br><span class="line">))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> matched = pipeline.fit(data).transform(data)</span><br></pre></td></tr></table></figure>

<h1 id="打完收功"><a href="#打完收功" class="headerlink" title="打完收功"></a>打完收功</h1><p>到这里,开发简单Transform的套路已经清楚了,不过这里实现的功能比较类似于一个UDF,只能对dataset的一个字段进行处理,而且是逐行处理,并不能根据多行数据进行处理,实现窗口函数类似的功能,而且也没有涉及模型的输出.如果要开发更复杂的算法,甚至进行模型训练,就需要更深入的了解MLlib了,阅读源码是个好途径.</p>
<p>有机会再说.👋</p>
]]></content>
      <tags>
        <tag>Spark</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux配置SSH免密登陆(公私钥登陆)</title>
    <url>/2018/03/05/ssh-without-password/</url>
    <content><![CDATA[<h1 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h1><p>客户机:Mac OS X</p>
<p>服务器:CentOS 6.5</p>
<p>客户端:OpenSSH,OS X及大多数Linux都内置了OpenSSH.’ssh -v’命令可以查看版本.</p>
<span id="more"></span>

<h1 id="大致流程"><a href="#大致流程" class="headerlink" title="大致流程"></a>大致流程</h1><ul>
<li><p>1.在客户机创建一对密钥文件,包括公钥文件(<del>&#x2F;.ssh&#x2F;id_rsa.pub),私钥文件(</del>&#x2F;.ssh&#x2F;id_rsa).</p>
</li>
<li><p>2.把公钥放到服务器上（~&#x2F;.ssh&#x2F;authorized_keys）,在使用ssh登录时，ssh程序会发送私钥去和服务器上的公钥做匹配。如果匹配成功就可以自动登录了。</p>
</li>
</ul>
<h1 id="客户机配置"><a href="#客户机配置" class="headerlink" title="客户机配置"></a>客户机配置</h1><ul>
<li><p>1.查看~&#x2F;.ssh文件夹,若已经存在有公钥文件(id_rsa.pub),私钥文件(id_rsa),则可以跳过客户端配置.</p>
</li>
<li><p>2.生成密钥文件.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ssh-keygen</span><br></pre></td></tr></table></figure>
<p>然后一路回车.<br>然后~&#x2F;.ssh下会生成id_rsa.pub和id_rsa, 其中id_rsa文件起到唯一标识你的客户机的作用.</p>
</li>
</ul>
<p>注意:不要改这两个文件的文件名,ssh登陆时会读取id_rsa文件.</p>
<h1 id="服务器配置"><a href="#服务器配置" class="headerlink" title="服务器配置"></a>服务器配置</h1><h2 id="1-修改sshd配置文件-x2F-etc-x2F-ssh-x2F-sshd-config"><a href="#1-修改sshd配置文件-x2F-etc-x2F-ssh-x2F-sshd-config" class="headerlink" title="1.修改sshd配置文件(&#x2F;etc&#x2F;ssh&#x2F;sshd_config)."></a>1.修改sshd配置文件(&#x2F;etc&#x2F;ssh&#x2F;sshd_config).</h2><p>找到以下内容，并去掉注释符”#“</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">　　RSAAuthentication yes</span><br><span class="line">　　PubkeyAuthentication yes</span><br><span class="line">　　AuthorizedKeysFile  .ssh/authorized_keys</span><br></pre></td></tr></table></figure>

<h2 id="2-配置authorized-keys文件"><a href="#2-配置authorized-keys文件" class="headerlink" title="2.配置authorized_keys文件."></a>2.配置authorized_keys文件.</h2><p>若’~&#x2F;.ssh&#x2F;authorized_keys’不存在,则建立.ssh文件夹和authorized_keys文件.</p>
<p>将上文中客户机id_rsa.pub的内容拷贝到authorized_keys中.</p>
<p>PS:可以在客户机中执行命令来拷贝:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat ~/.ssh/id_rsa.pub | ssh user@host “cat - &gt;&gt; ~/.ssh/authorized_keys”</span><br></pre></td></tr></table></figure>
<p>注意:</p>
<ul>
<li>1 .ssh目录的权限必须是700</li>
<li>2 .ssh&#x2F;authorized_keys文件权限必须是600</li>
</ul>
<h2 id="3-重启sshd"><a href="#3-重启sshd" class="headerlink" title="3.重启sshd."></a>3.重启sshd.</h2><p>$ &#x2F;etc&#x2F;init.d&#x2F;sshd restart</p>
<h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><p>客户机执行:<code>ssh -v user@host (-v 调试模式)</code></p>
<p>会显示一些登陆信息.<br>若登陆失败,或者仍然要输入密码,可以在服务器查看日志文件:&#x2F;var&#x2F;log&#x2F;secure.</p>
<p>若登陆成功,则以后就可以用<code>ssh user@host</code> 直接登陆了,不用输入密码.</p>
]]></content>
      <tags>
        <tag>SSH</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark API 全集(3):Spark RDD API全集</title>
    <url>/2018/03/26/spark-rdd-RDD-api/</url>
    <content><![CDATA[<h1 id="RDD是啥"><a href="#RDD是啥" class="headerlink" title="RDD是啥"></a>RDD是啥</h1><p>Resilient Distributed Dataset (RDD)，弹性分布式数据集，是对不可修改，分区的数据集合的抽象。</p>
<p>RDD is characterized by five main properties:</p>
<ul>
<li>A list of partitions</li>
<li>A function for computing each split</li>
<li>A list of dependencies on other RDDs</li>
<li>Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)</li>
<li>Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)<span id="more"></span></li>
</ul>
<h1 id="org-spark-rdd-RDD类方法"><a href="#org-spark-rdd-RDD类方法" class="headerlink" title="org.spark.rdd.RDD类方法"></a>org.spark.rdd.RDD类方法</h1><p>RDD是一个抽象类，定义如下</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">RDD</span>[<span class="type">T</span>] <span class="keyword">extends</span> <span class="title">Serializable</span> <span class="keyword">with</span> <span class="title">Logging</span></span></span><br></pre></td></tr></table></figure>
<p>RDD类的public方法大约有80多个（包括不同参数重载的）,均在下面列出。</p>
<p>值得注意的是，RDD类中并没有定义xxxByKey形式的方法，这类方法其实是在PairRDDFunctions中定义的，通过隐式转换，键值对形式的RDD（即RDD[(K, V)）可以调用PairRDDFunctions中定义的方法。相关的隐式转换定义在RDD的伴生对象中。</p>
<h3 id="键值转换操作"><a href="#键值转换操作" class="headerlink" title="键值转换操作"></a>键值转换操作</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">filter(f: (<span class="type">T</span>) ⇒ <span class="type">Boolean</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line">过滤数据，仅留下使得f返回<span class="literal">true</span>的元素。</span><br><span class="line"></span><br><span class="line">map[<span class="type">U</span>](f: (<span class="type">T</span>) ⇒ <span class="type">U</span>)(<span class="keyword">implicit</span> arg0: <span class="type">ClassTag</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>]</span><br><span class="line">将一个<span class="type">RDD</span>中的每个数据项，通过map中的函数映射变为一个新的元素。</span><br><span class="line">输入分区与输出分区一对一，即：有多少个输入分区，就有多少个输出分区。</span><br><span class="line"></span><br><span class="line">flatMap[<span class="type">U</span>](f: (<span class="type">T</span>) ⇒ <span class="type">TraversableOnce</span>[<span class="type">U</span>])(<span class="keyword">implicit</span> arg0: <span class="type">ClassTag</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>]</span><br><span class="line">第一步和map一样，最后将所有的输出分区合并成一个。</span><br><span class="line">使用flatMap时候需要注意：</span><br><span class="line">flatMap会将字符串看成是一个字符数组。</span><br><span class="line"></span><br><span class="line">mapPartitions[<span class="type">U</span>](f: (<span class="type">Iterator</span>[<span class="type">T</span>]) ⇒ <span class="type">Iterator</span>[<span class="type">U</span>], preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>)(<span class="keyword">implicit</span> arg0: <span class="type">ClassTag</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>]</span><br><span class="line">该函数和map函数类似，只不过映射函数的参数由<span class="type">RDD</span>中的每一个元素变成了<span class="type">RDD</span>中每一个分区的迭代器。如果在映射的过程中需要频繁创建额外的对象，使用mapPartitions要比map高效的过。</span><br><span class="line">比如，将<span class="type">RDD</span>中的所有数据通过<span class="type">JDBC</span>连接写入数据库，如果使用map函数，可能要为每一个元素都创建一个connection，这样开销很大，如果使用mapPartitions，那么只需要针对每一个分区建立一个connection。</span><br><span class="line">参数preservesPartitioning表示是否保留父<span class="type">RDD</span>的partitioner分区信息。</span><br><span class="line"></span><br><span class="line">mapPartitionsWithIndex[<span class="type">U</span>](f: (<span class="type">Int</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">Iterator</span>[<span class="type">U</span>], preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>)(<span class="keyword">implicit</span> arg0: <span class="type">ClassTag</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>]</span><br><span class="line">函数作用同mapPartitions，不过提供了两个参数，第一个参数为分区的索引。</span><br><span class="line"></span><br><span class="line">keyBy[<span class="type">K</span>](f: (<span class="type">T</span>) ⇒ <span class="type">K</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">T</span>)]</span><br><span class="line">通过f函数为每个元素生成一个<span class="type">KEY</span></span><br><span class="line"></span><br><span class="line">sortBy[<span class="type">K</span>](f: (<span class="type">T</span>) ⇒ <span class="type">K</span>, ascending: <span class="type">Boolean</span> = <span class="literal">true</span>, numPartitions: <span class="type">Int</span> = <span class="keyword">this</span>.partitions.length)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">K</span>], ctag: <span class="type">ClassTag</span>[<span class="type">K</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line">通过给定的函数对元素排序</span><br><span class="line"></span><br><span class="line">zip[<span class="type">U</span>](other: <span class="type">RDD</span>[<span class="type">U</span>])(<span class="keyword">implicit</span> arg0: <span class="type">ClassTag</span>[<span class="type">U</span>]): <span class="type">RDD</span>[(<span class="type">T</span>, <span class="type">U</span>)]</span><br><span class="line">与另一个<span class="type">RDD</span>组合成（k,v)对。</span><br><span class="line"></span><br><span class="line">zipPartitions[<span class="type">B</span>, <span class="type">V</span>](rdd2: <span class="type">RDD</span>[<span class="type">B</span>], preservesPartitioning: <span class="type">Boolean</span>)(f: (<span class="type">Iterator</span>[<span class="type">T</span>], <span class="type">Iterator</span>[<span class="type">B</span>]) ⇒ <span class="type">Iterator</span>[<span class="type">V</span>])(<span class="keyword">implicit</span> arg0: <span class="type">ClassTag</span>[<span class="type">B</span>], arg1: <span class="type">ClassTag</span>[<span class="type">V</span>]): <span class="type">RDD</span>[<span class="type">V</span>]</span><br><span class="line"></span><br><span class="line">zipWithIndex(): <span class="type">RDD</span>[(<span class="type">T</span>, <span class="type">Long</span>)]</span><br><span class="line"></span><br><span class="line">zipWithUniqueId(): <span class="type">RDD</span>[(<span class="type">T</span>, <span class="type">Long</span>)]</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="聚合相关"><a href="#聚合相关" class="headerlink" title="聚合相关"></a>聚合相关</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">aggregate[<span class="type">U</span>](zeroValue: <span class="type">U</span>)(seqOp: (<span class="type">U</span>, <span class="type">T</span>) ⇒ <span class="type">U</span>, combOp: (<span class="type">U</span>, <span class="type">U</span>) ⇒ <span class="type">U</span>)(<span class="keyword">implicit</span> arg0: <span class="type">ClassTag</span>[<span class="type">U</span>]): <span class="type">U</span></span><br><span class="line">aggregate用户聚合<span class="type">RDD</span>中的元素，先使用seqOp将<span class="type">RDD</span>中每个分区中的<span class="type">T</span>类型元素聚合成<span class="type">U</span>类型，再使用combOp将之前每个分区聚合后的<span class="type">U</span>类型聚合成<span class="type">U</span>类型，特别注意seqOp和combOp都会使用zeroValue的值，zeroValue的类型为<span class="type">U</span>。</span><br><span class="line"></span><br><span class="line">treeAggregate[<span class="type">U</span>](zeroValue: <span class="type">U</span>)(seqOp: (<span class="type">U</span>, <span class="type">T</span>) ⇒ <span class="type">U</span>, combOp: (<span class="type">U</span>, <span class="type">U</span>) ⇒ <span class="type">U</span>, depth: <span class="type">Int</span> = <span class="number">2</span>)(<span class="keyword">implicit</span> arg0: <span class="type">ClassTag</span>[<span class="type">U</span>]): <span class="type">U</span></span><br><span class="line">多层级聚合</span><br><span class="line"></span><br><span class="line">reduce(f: (<span class="type">T</span>, <span class="type">T</span>) ⇒ <span class="type">T</span>): <span class="type">T</span></span><br><span class="line">根据映射函数f，对<span class="type">RDD</span>中的元素进行二元计算，返回计算结果。</span><br><span class="line"></span><br><span class="line">treeReduce(f: (<span class="type">T</span>, <span class="type">T</span>) ⇒ <span class="type">T</span>, depth: <span class="type">Int</span> = <span class="number">2</span>): <span class="type">T</span></span><br><span class="line">多级reduce归并聚合</span><br><span class="line"></span><br><span class="line">fold(zeroValue: <span class="type">T</span>)(op: (<span class="type">T</span>, <span class="type">T</span>) ⇒ <span class="type">T</span>): <span class="type">T</span></span><br><span class="line">fold是aggregate的简化，将aggregate中的seqOp和combOp使用同一个函数op。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">count(): <span class="type">Long</span></span><br><span class="line">count返回<span class="type">RDD</span>中的元素数量。</span><br><span class="line"></span><br><span class="line">countApprox(timeout: <span class="type">Long</span>, confidence: <span class="type">Double</span> = <span class="number">0.95</span>): <span class="type">PartialResult</span>[<span class="type">BoundedDouble</span>]</span><br><span class="line">近似count</span><br><span class="line"></span><br><span class="line">countApproxDistinct(relativeSD: <span class="type">Double</span> = <span class="number">0.05</span>): <span class="type">Long</span></span><br><span class="line">countApproxDistinct(p: <span class="type">Int</span>, sp: <span class="type">Int</span>): <span class="type">Long</span></span><br><span class="line">近似distinct count</span><br><span class="line"></span><br><span class="line">countByValue()(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">Map</span>[<span class="type">T</span>, <span class="type">Long</span>]</span><br><span class="line">计算每个值出现次数</span><br><span class="line"></span><br><span class="line">countByValueApprox(timeout: <span class="type">Long</span>, confidence: <span class="type">Double</span> = <span class="number">0.95</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>):</span><br><span class="line">计算每个值出现次数近似值</span><br><span class="line"></span><br><span class="line">distinct(): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line">distinct(numPartitions: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line">返回元素去重后的<span class="type">RDD</span></span><br><span class="line"></span><br><span class="line">groupBy[<span class="type">K</span>](f: (<span class="type">T</span>) ⇒ <span class="type">K</span>)(<span class="keyword">implicit</span> kt: <span class="type">ClassTag</span>[<span class="type">K</span>]): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">T</span>])]</span><br><span class="line">groupBy[<span class="type">K</span>](f: (<span class="type">T</span>) ⇒ <span class="type">K</span>, numPartitions: <span class="type">Int</span>)(<span class="keyword">implicit</span> kt: <span class="type">ClassTag</span>[<span class="type">K</span>]): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">T</span>])]</span><br><span class="line">groupBy[<span class="type">K</span>](f: (<span class="type">T</span>) ⇒ <span class="type">K</span>, p: <span class="type">Partitioner</span>)(<span class="keyword">implicit</span> kt: <span class="type">ClassTag</span>[<span class="type">K</span>], ord: <span class="type">Ordering</span>[<span class="type">K</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">T</span>])]</span><br><span class="line">按指定函数生成key，并按key分组。</span><br><span class="line">注意：性能比较差，推荐用<span class="type">PairRDDFunctions</span>.reduceByKey or <span class="type">PairRDDFunctions</span>.aggregateByKey.</span><br><span class="line">因为reduceByKey会先在分区内做聚合，再进行数据交换(shuffle)。</span><br><span class="line"></span><br><span class="line">glom(): <span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">T</span>]]</span><br><span class="line">该函数是将<span class="type">RDD</span>中每一个分区中类型为<span class="type">T</span>的元素转换成<span class="type">Array</span>[<span class="type">T</span>]，这样每一个分区就只有一个数组元素。</span><br><span class="line"></span><br><span class="line">max()(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">T</span></span><br><span class="line">最大的元素</span><br><span class="line"></span><br><span class="line">min()(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">T</span></span><br><span class="line">最小的元素</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="遍历元素"><a href="#遍历元素" class="headerlink" title="遍历元素"></a>遍历元素</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">foreach(f: (<span class="type">T</span>) ⇒ <span class="type">Unit</span>): <span class="type">Unit</span></span><br><span class="line">foreach用于遍历<span class="type">RDD</span>,将函数f应用于每一个元素。</span><br><span class="line">但要注意，如果对<span class="type">RDD</span>执行foreach，只会在<span class="type">Executor</span>端有效，而并不是<span class="type">Driver</span>端。</span><br><span class="line">比如：rdd.foreach(println)，只会在<span class="type">Executor</span>的stdout中打印出来，<span class="type">Driver</span>端是看不到的。</span><br><span class="line"></span><br><span class="line">foreachPartition(f: (<span class="type">Iterator</span>[<span class="type">T</span>]) ⇒ <span class="type">Unit</span>): <span class="type">Unit</span></span><br><span class="line">foreachPartition和foreach类似，只不过是对每一个分区使用f。</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="取元素相关"><a href="#取元素相关" class="headerlink" title="取元素相关"></a>取元素相关</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">collect(): <span class="type">Array</span>[<span class="type">T</span>]</span><br><span class="line">collect用于将一个<span class="type">RDD</span>转换成数组。</span><br><span class="line"></span><br><span class="line">first(): <span class="type">T</span></span><br><span class="line">first返回<span class="type">RDD</span>中的第一个元素，不排序。</span><br><span class="line"></span><br><span class="line">take(num: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">T</span>]</span><br><span class="line">take用于获取<span class="type">RDD</span>中从<span class="number">0</span>到num<span class="number">-1</span>下标的元素，不排序。</span><br><span class="line"></span><br><span class="line">top(num: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">Array</span>[<span class="type">T</span>]</span><br><span class="line">top函数用于从<span class="type">RDD</span>中，按照默认（降序）或者指定的排序规则，返回前num个元素。</span><br><span class="line"></span><br><span class="line">takeOrdered(num: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">Array</span>[<span class="type">T</span>]</span><br><span class="line">takeOrdered和top类似，只不过以和top相反的顺序返回元素</span><br><span class="line"></span><br><span class="line">takeSample(withReplacement: <span class="type">Boolean</span>, num: <span class="type">Int</span>, seed: <span class="type">Long</span> = <span class="type">Utils</span>.random.nextLong): <span class="type">Array</span>[<span class="type">T</span>]</span><br><span class="line">取样本元素</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="集合间运算"><a href="#集合间运算" class="headerlink" title="集合间运算"></a>集合间运算</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">++(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line">与另一个<span class="type">RDD</span> union。</span><br><span class="line"></span><br><span class="line">intersection(other: <span class="type">RDD</span>[<span class="type">T</span>], partitioner: <span class="type">Partitioner</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line">intersection(other: <span class="type">RDD</span>[<span class="type">T</span>], numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line">intersection(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line">取交集</span><br><span class="line"></span><br><span class="line">subtract(other: <span class="type">RDD</span>[<span class="type">T</span>], p: <span class="type">Partitioner</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line">subtract(other: <span class="type">RDD</span>[<span class="type">T</span>], numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line">subtract(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line">求差集</span><br><span class="line"></span><br><span class="line">union(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line">与另一个<span class="type">RDD</span>合并，类似union all,不会去重。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">persist(): <span class="type">RDD</span>.<span class="keyword">this</span>.<span class="keyword">type</span></span><br><span class="line">persist(newLevel: <span class="type">StorageLevel</span>): <span class="type">RDD</span>.<span class="keyword">this</span>.<span class="keyword">type</span></span><br><span class="line">缓存数据，可设置缓存级别(如果尚未设置过，才可以设置，本地checkpoint除外)</span><br><span class="line"></span><br><span class="line">unpersist(blocking: <span class="type">Boolean</span> = <span class="literal">true</span>): <span class="type">RDD</span>.<span class="keyword">this</span>.<span class="keyword">type</span></span><br><span class="line"><span class="type">Mark</span> the <span class="type">RDD</span> as non-persistent, and remove all blocks <span class="keyword">for</span> it from memory and disk.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cache(): <span class="type">RDD</span>.<span class="keyword">this</span>.<span class="keyword">type</span></span><br><span class="line"><span class="type">MEMORY_ONLY</span>级别缓存数据</span><br><span class="line"></span><br><span class="line">cartesian[<span class="type">U</span>](other: <span class="type">RDD</span>[<span class="type">U</span>])(<span class="keyword">implicit</span> arg0: <span class="type">ClassTag</span>[<span class="type">U</span>]): <span class="type">RDD</span>[(<span class="type">T</span>, <span class="type">U</span>)]：</span><br><span class="line">计算两个<span class="type">RDD</span>的迪卡尔积</span><br><span class="line"></span><br><span class="line">checkpoint(): <span class="type">Unit</span></span><br><span class="line">标记将该<span class="type">RDD</span>进行checkpoint处理？</span><br><span class="line"></span><br><span class="line">coalesce(numPartitions: <span class="type">Int</span>, shuffle: <span class="type">Boolean</span> = <span class="literal">false</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line">分区合并(只能减少分区)，使用<span class="type">HashPartitioner</span>。</span><br><span class="line">第一个参数为重分区的数目，第二个为是否进行shuffle，默认为<span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">repartition(numPartitions: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line">调整分区数，会导致shuffle，如果是减少分区，可以使用coalesce，避免shuffle。</span><br><span class="line"></span><br><span class="line">toDebugString: <span class="type">String</span></span><br><span class="line">返回<span class="type">RDD</span>依赖树/血统图</span><br><span class="line"></span><br><span class="line">getCheckpointFile: <span class="type">Option</span>[<span class="type">String</span>]</span><br><span class="line">获取checkpoint文件夹名称</span><br><span class="line"></span><br><span class="line">localCheckpoint(): <span class="type">RDD</span>.<span class="keyword">this</span>.<span class="keyword">type</span></span><br><span class="line">标记为使用本地checkpoint</span><br><span class="line"></span><br><span class="line">isEmpty(): <span class="type">Boolean</span></span><br><span class="line">是否含<span class="number">0</span>个元素</span><br><span class="line"></span><br><span class="line">iterator(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>]</span><br><span class="line">返回迭代器，不应直接调用，而是给<span class="type">RDD</span>的子类用的。</span><br><span class="line"></span><br><span class="line">toLocalIterator: <span class="type">Iterator</span>[<span class="type">T</span>]</span><br><span class="line">返回元素的本地迭代器</span><br><span class="line"></span><br><span class="line">pipe(command: <span class="type">String</span>): <span class="type">RDD</span>[<span class="type">String</span>]</span><br><span class="line">pipe(command: <span class="type">String</span>, env: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">RDD</span>[<span class="type">String</span>]</span><br><span class="line">pipe(command: <span class="type">Seq</span>[<span class="type">String</span>], env: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="type">Map</span>(), printPipeContext: ((<span class="type">String</span>) ⇒ <span class="type">Unit</span>) ⇒ <span class="type">Unit</span> = <span class="literal">null</span>, printRDDElement: (<span class="type">T</span>, (<span class="type">String</span>) ⇒ <span class="type">Unit</span>) ⇒ <span class="type">Unit</span> = <span class="literal">null</span>, separateWorkingDir: <span class="type">Boolean</span> = <span class="literal">false</span>, bufferSize: <span class="type">Int</span> = <span class="number">8192</span>, encoding: <span class="type">String</span> = <span class="type">Codec</span>.defaultCharsetCodec.name): <span class="type">RDD</span>[<span class="type">String</span>]</span><br><span class="line">调用外部进程处理<span class="type">RDD</span>,如通过标准输入传给shell脚本。</span><br><span class="line"></span><br><span class="line">preferredLocations(split: <span class="type">Partition</span>): <span class="type">Seq</span>[<span class="type">String</span>]</span><br><span class="line"><span class="type">Get</span> the preferred locations of a partition, taking into account whether the <span class="type">RDD</span> is checkpointed.</span><br><span class="line"></span><br><span class="line">randomSplit(weights: <span class="type">Array</span>[<span class="type">Double</span>], seed: <span class="type">Long</span> = <span class="type">Utils</span>.random.nextLong): <span class="type">Array</span>[<span class="type">RDD</span>[<span class="type">T</span>]]</span><br><span class="line">按权随机将元素分组</span><br><span class="line"></span><br><span class="line">sample(withReplacement: <span class="type">Boolean</span>, fraction: <span class="type">Double</span>, seed: <span class="type">Long</span> = <span class="type">Utils</span>.random.nextLong): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line">取样本/子集</span><br><span class="line"></span><br><span class="line">setName(_name: <span class="type">String</span>): <span class="type">RDD</span>.<span class="keyword">this</span>.<span class="keyword">type</span></span><br><span class="line">设置<span class="type">RDD</span>名字</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="保存"><a href="#保存" class="headerlink" title="保存"></a>保存</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">saveAsObjectFile(path: <span class="type">String</span>): <span class="type">Unit</span></span><br><span class="line">保存为<span class="type">SequenceFile</span></span><br><span class="line"></span><br><span class="line">saveAsTextFile(path: <span class="type">String</span>, codec: <span class="type">Class</span>[_ &lt;: <span class="type">CompressionCodec</span>]): <span class="type">Unit</span></span><br><span class="line">saveAsTextFile(path: <span class="type">String</span>): <span class="type">Unit</span></span><br><span class="line">保存为文本文件</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">context: <span class="type">SparkContext</span></span><br><span class="line">创建<span class="type">RDD</span>的<span class="type">SparkContext</span></span><br><span class="line"></span><br><span class="line">sparkContext: <span class="type">SparkContext</span></span><br><span class="line">创建<span class="type">RDD</span>的<span class="type">SparkContext</span></span><br><span class="line"></span><br><span class="line">dependencies: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]]</span><br><span class="line"><span class="type">RDD</span>的依赖列表</span><br><span class="line"></span><br><span class="line">getNumPartitions: <span class="type">Int</span></span><br><span class="line">获取<span class="type">RDD</span>的分区数</span><br><span class="line"></span><br><span class="line">getStorageLevel: <span class="type">StorageLevel</span></span><br><span class="line">获取存储等级，如果设置为none,则返回<span class="type">StorageLevel</span>.<span class="type">NONE</span> 。</span><br><span class="line"></span><br><span class="line">id: <span class="type">Int</span></span><br><span class="line">该<span class="type">RDD</span>的unique <span class="type">ID</span></span><br><span class="line"></span><br><span class="line">isCheckpointed: <span class="type">Boolean</span></span><br><span class="line">是否checkpointed and materialized, either reliably or locally.</span><br><span class="line"></span><br><span class="line">name: <span class="type">String</span></span><br><span class="line"><span class="type">RDD</span>的名字</span><br><span class="line"></span><br><span class="line">partitioner: <span class="type">Option</span>[<span class="type">Partitioner</span>]</span><br><span class="line">分区器</span><br><span class="line"></span><br><span class="line">partitions: <span class="type">Array</span>[<span class="type">Partition</span>]</span><br><span class="line">各个分区</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Spark</tag>
        <tag>API</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark API 全集(1):Spark SQL Dataset &amp; DataFrame API</title>
    <url>/2018/03/22/spark-sql-dataset-api/</url>
    <content><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>org.apache.spark.sql.Dataset是Spark SQL中核心的类，定义如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset</span>[<span class="type">T</span>] <span class="keyword">extends</span> <span class="title">Serializable</span></span></span><br></pre></td></tr></table></figure>

<p>DataFrame是Dataset[Row]的别名。</p>
<p>本文基于spark2.3.0.</p>
<p>下面是类方法简介。</p>
<span id="more"></span>

<h1 id="类方法"><a href="#类方法" class="headerlink" title="类方法"></a>类方法</h1><h2 id="Actions"><a href="#Actions" class="headerlink" title="Actions"></a>Actions</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">collect(): <span class="type">Array</span>[<span class="type">T</span>]</span><br><span class="line">返回一个数组，包含<span class="type">Dataset</span>所有行的数据。</span><br><span class="line">注意：所有数据会被加载进driver进程的内存。</span><br><span class="line"></span><br><span class="line">collectAsList(): <span class="type">List</span>[<span class="type">T</span>]</span><br><span class="line">同上，但是返回<span class="type">Java</span> list。</span><br><span class="line"></span><br><span class="line">count(): <span class="type">Long</span></span><br><span class="line">数据行数</span><br><span class="line"></span><br><span class="line">describe(cols: <span class="type">String</span>*): <span class="type">DataFrame</span></span><br><span class="line">计算指定列的统计指标，包括count, mean, stddev, min, and max.</span><br><span class="line"></span><br><span class="line">head(): <span class="type">T</span></span><br><span class="line">返回第一行</span><br><span class="line"></span><br><span class="line">head(n: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">T</span>]</span><br><span class="line">返回前<span class="type">N</span>行</span><br><span class="line"></span><br><span class="line">first(): <span class="type">T</span></span><br><span class="line">返回第一行，是head()的别名。</span><br><span class="line"></span><br><span class="line">foreach(f: (<span class="type">T</span>) ⇒ <span class="type">Unit</span>): <span class="type">Unit</span></span><br><span class="line">所有元素上应用f函数</span><br><span class="line"></span><br><span class="line">foreachPartition(f: (<span class="type">Iterator</span>[<span class="type">T</span>]) ⇒ <span class="type">Unit</span>): <span class="type">Unit</span></span><br><span class="line">所有元素分区上应用f函数</span><br><span class="line"></span><br><span class="line">reduce(func: (<span class="type">T</span>, <span class="type">T</span>) ⇒ <span class="type">T</span>): <span class="type">T</span></span><br><span class="line">根据映射函数func，对<span class="type">RDD</span>中的元素进行二元计算，返回计算结果。</span><br><span class="line">注意：提供的函数应满足交换律及结合律，否则计算结果将是非确定的。</span><br><span class="line"></span><br><span class="line">show(numRows: <span class="type">Int</span>, truncate: <span class="type">Int</span>, vertical: <span class="type">Boolean</span>): <span class="type">Unit</span></span><br><span class="line">表格形式打印出数据。numRows：显示的行数，truncate：裁剪字符串类型值到指定长度，vertical：垂直打印。</span><br><span class="line"></span><br><span class="line">show(numRows: <span class="type">Int</span>, truncate: <span class="type">Int</span>): <span class="type">Unit</span></span><br><span class="line">show(numRows: <span class="type">Int</span>, truncate: <span class="type">Boolean</span>): <span class="type">Unit</span></span><br><span class="line">show(truncate: <span class="type">Boolean</span>): <span class="type">Unit</span></span><br><span class="line">numRows=<span class="number">20</span> truncate=<span class="number">20</span></span><br><span class="line"></span><br><span class="line">show(numRows: <span class="type">Int</span>): <span class="type">Unit</span></span><br><span class="line">truncate=<span class="number">20</span></span><br><span class="line"></span><br><span class="line">show(): <span class="type">Unit</span></span><br><span class="line">numRows=<span class="number">20</span> truncate=<span class="number">20</span></span><br><span class="line"></span><br><span class="line">summary(statistics: <span class="type">String</span>*): <span class="type">DataFrame</span></span><br><span class="line">计算数据集statistics指定的指标，可指定 count, mean, stddev, min, approximate quartiles (percentiles at <span class="number">25</span>%, <span class="number">50</span>%, and <span class="number">75</span>%), and max.</span><br><span class="line">如未指定则会计算全部。</span><br><span class="line"></span><br><span class="line">take(n: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">T</span>]</span><br><span class="line">获取前n行</span><br><span class="line"></span><br><span class="line">takeAsList(n: <span class="type">Int</span>): <span class="type">List</span>[<span class="type">T</span>]</span><br><span class="line">获取前n行保存为list</span><br><span class="line"></span><br><span class="line">toLocalIterator(): <span class="type">Iterator</span>[<span class="type">T</span>]</span><br><span class="line">返回一个所有行的迭代器</span><br><span class="line"><span class="type">The</span> iterator will consume as much memory as the largest partition in <span class="keyword">this</span> <span class="type">Dataset</span>.</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="基本函数（Basic-Dataset-functions"><a href="#基本函数（Basic-Dataset-functions" class="headerlink" title="基本函数（Basic Dataset functions)"></a>基本函数（Basic Dataset functions)</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">as[<span class="type">U</span>](<span class="keyword">implicit</span> arg0: <span class="type">Encoder</span>[<span class="type">U</span>]): <span class="type">Dataset</span>[<span class="type">U</span>]</span><br><span class="line">将数据映射成指定类型<span class="type">U</span>，返回新的<span class="type">Dataset</span></span><br><span class="line"></span><br><span class="line">persist(newLevel: <span class="type">StorageLevel</span>): <span class="type">Dataset</span>.<span class="keyword">this</span>.<span class="keyword">type</span></span><br><span class="line">缓存数据，可设置缓存级别。</span><br><span class="line"></span><br><span class="line">persist(): <span class="type">Dataset</span>.<span class="keyword">this</span>.<span class="keyword">type</span></span><br><span class="line">同cache方法</span><br><span class="line"></span><br><span class="line">cache(): <span class="type">Dataset</span>.<span class="keyword">this</span>.<span class="keyword">type</span></span><br><span class="line">缓存数据,<span class="type">MEMORY_AND_DISK</span>模式。</span><br><span class="line">注意：<span class="type">RDD</span>的cache函数默认是<span class="type">MEMORY_ONLY</span>。</span><br><span class="line"></span><br><span class="line">checkpoint(eager: <span class="type">Boolean</span>): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">返回一个checkpointed的<span class="type">Dataset</span>，<span class="type">Dataset</span>的逻辑执行计划将被截断。</span><br><span class="line"></span><br><span class="line">checkpoint(): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">同上，eager=<span class="literal">true</span>.</span><br><span class="line"></span><br><span class="line">columns: <span class="type">Array</span>[<span class="type">String</span>]</span><br><span class="line">数组形式返回所有列名。</span><br><span class="line"></span><br><span class="line">dtypes: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">String</span>)]</span><br><span class="line">数组形式返回所有列名及类型。</span><br><span class="line"></span><br><span class="line">createGlobalTempView(viewName: <span class="type">String</span>): <span class="type">Unit</span></span><br><span class="line">创建全局临时视图(view)，生命周期与<span class="type">Spark</span>应用一致。</span><br><span class="line">可以跨session访问。e.g. <span class="type">SELECT</span> * <span class="type">FROM</span> global_temp.view1.</span><br><span class="line"></span><br><span class="line">createOrReplaceGlobalTempView(viewName: <span class="type">String</span>): <span class="type">Unit</span></span><br><span class="line">同上，已存在则替换。</span><br><span class="line"></span><br><span class="line">createTempView(viewName: <span class="type">String</span>): <span class="type">Unit</span></span><br><span class="line">创建本地临时视图(view)，仅当前<span class="type">SparkSession</span>可访问。</span><br><span class="line">注意：不跟任何库绑定，不能用db1.view1这样的形式访问。</span><br><span class="line"></span><br><span class="line">createOrReplaceTempView(viewName: <span class="type">String</span>): <span class="type">Unit</span></span><br><span class="line">同上，已存在则替换。</span><br><span class="line"></span><br><span class="line">explain(): <span class="type">Unit</span></span><br><span class="line">打印物理执行计划</span><br><span class="line">另有：queryExecution变量，完整执行计划。</span><br><span class="line"></span><br><span class="line">explain(extended: <span class="type">Boolean</span>): <span class="type">Unit</span></span><br><span class="line">打印物理+逻辑执行计划</span><br><span class="line"></span><br><span class="line">hint(name: <span class="type">String</span>, parameters: <span class="type">Any</span>*): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">当前dataset指定hint。<span class="comment">//todo</span></span><br><span class="line">e.g. df1.join(df2.hint(<span class="string">&quot;broadcast&quot;</span>))</span><br><span class="line"></span><br><span class="line">inputFiles: <span class="type">Array</span>[<span class="type">String</span>]</span><br><span class="line">返回组成<span class="type">Dataset</span>的输入文件（<span class="type">Returns</span> a best-effort snapshot of the files that compose <span class="keyword">this</span> <span class="type">Dataset</span>）</span><br><span class="line"></span><br><span class="line">isLocal: <span class="type">Boolean</span></span><br><span class="line">collect和take是否可以本地执行，不需要executor.</span><br><span class="line"></span><br><span class="line">localCheckpoint(eager: <span class="type">Boolean</span>): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">执行本地<span class="type">Checkpoint</span>，返回新dataset。</span><br><span class="line"></span><br><span class="line">localCheckpoint(): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">eager=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line">printSchema(): <span class="type">Unit</span></span><br><span class="line">打印schema结构</span><br><span class="line"></span><br><span class="line">rdd: <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line">dataset内部的<span class="type">RDD</span></span><br><span class="line"></span><br><span class="line">schema: <span class="type">StructType</span></span><br><span class="line">schema</span><br><span class="line"></span><br><span class="line">storageLevel: <span class="type">StorageLevel</span></span><br><span class="line">当前存储等级，没有被persist则是<span class="type">StorageLevel</span>.<span class="type">NONE</span></span><br><span class="line"></span><br><span class="line">toDF(): <span class="type">DataFrame</span></span><br><span class="line">toDF(colNames: <span class="type">String</span>*): <span class="type">DataFrame</span></span><br><span class="line">转为<span class="type">DataFrame</span>，也可以将<span class="type">RDD</span>转为<span class="type">DataFrame</span>。</span><br><span class="line"></span><br><span class="line">unpersist(): <span class="type">Dataset</span>.<span class="keyword">this</span>.<span class="keyword">type</span></span><br><span class="line">unpersist(blocking: <span class="type">Boolean</span>): <span class="type">Dataset</span>.<span class="keyword">this</span>.<span class="keyword">type</span></span><br><span class="line">删除缓存，blocking表示是否等所有blocks删除后才返回,删除期间阻塞。</span><br><span class="line"></span><br><span class="line">write: <span class="type">DataFrameWriter</span>[<span class="type">T</span>]</span><br><span class="line"><span class="type">DataFrameWriter</span>，非流式数据写接口。</span><br><span class="line"></span><br><span class="line">writeStream: <span class="type">DataStreamWriter</span>[<span class="type">T</span>]</span><br><span class="line"><span class="type">DataStreamWriter</span>，流式数据写接口。</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h2 id="流式函数（streaming）"><a href="#流式函数（streaming）" class="headerlink" title="流式函数（streaming）"></a>流式函数（streaming）</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">isStreaming: <span class="type">Boolean</span></span><br><span class="line">是否流式数据</span><br><span class="line"></span><br><span class="line">withWatermark(eventTime: <span class="type">String</span>, delayThreshold: <span class="type">String</span>): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line"><span class="type">Defines</span> an event time watermark <span class="keyword">for</span> <span class="keyword">this</span> <span class="type">Dataset</span>.</span><br><span class="line"><span class="comment">//TODO</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h2 id="强类型转换（Typed-transformations）"><a href="#强类型转换（Typed-transformations）" class="headerlink" title="强类型转换（Typed transformations）"></a>强类型转换（Typed transformations）</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">alias(alias: <span class="type">Symbol</span>): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">alias(alias: <span class="type">String</span>): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">as(alias: <span class="type">Symbol</span>): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">as(alias: <span class="type">String</span>): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">给<span class="type">Dataset</span>一个别名</span><br><span class="line"></span><br><span class="line">coalesce(numPartitions: <span class="type">Int</span>): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">分区合并(只能减少分区)</span><br><span class="line"></span><br><span class="line">distinct(): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">dropDuplicates的别名</span><br><span class="line"></span><br><span class="line">dropDuplicates(col1: <span class="type">String</span>, cols: <span class="type">String</span>*): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">dropDuplicates(colNames: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">dropDuplicates(colNames: <span class="type">Seq</span>[<span class="type">String</span>]): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">dropDuplicates(): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">根据指定字段，对数据去重。</span><br><span class="line"></span><br><span class="line">except(other: <span class="type">Dataset</span>[<span class="type">T</span>]): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">去除other中也有的行。同<span class="type">EXCEPT</span> <span class="type">DISTINCT</span> in <span class="type">SQL</span>。</span><br><span class="line"><span class="comment">//TODO</span></span><br><span class="line"></span><br><span class="line">filter(func: (<span class="type">T</span>) ⇒ <span class="type">Boolean</span>): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">filter(conditionExpr: <span class="type">String</span>): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">filter(condition: <span class="type">Column</span>): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">根据条件过滤行</span><br><span class="line">e.g.</span><br><span class="line">peopleDs.filter(<span class="string">&quot;age &gt; 15&quot;</span>)</span><br><span class="line">peopleDs.filter($<span class="string">&quot;age&quot;</span> &gt; <span class="number">15</span>)</span><br><span class="line"></span><br><span class="line">flatMap[<span class="type">U</span>](func: (<span class="type">T</span>) ⇒ <span class="type">TraversableOnce</span>[<span class="type">U</span>])(<span class="keyword">implicit</span> arg0: <span class="type">Encoder</span>[<span class="type">U</span>]): <span class="type">Dataset</span>[<span class="type">U</span>]</span><br><span class="line">第一步和map一样，最后将所有的输出合并。</span><br><span class="line"></span><br><span class="line">groupByKey[<span class="type">K</span>](func: (<span class="type">T</span>) ⇒ <span class="type">K</span>)(<span class="keyword">implicit</span> arg0: <span class="type">Encoder</span>[<span class="type">K</span>]): <span class="type">KeyValueGroupedDataset</span>[<span class="type">K</span>, <span class="type">T</span>]</span><br><span class="line">现根据func函数生成key，然后按key分组。</span><br><span class="line"></span><br><span class="line">intersect(other: <span class="type">Dataset</span>[<span class="type">T</span>]): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">求两个dataset的交集，等同于<span class="type">INTERSECT</span> in <span class="type">SQL</span>.</span><br><span class="line"></span><br><span class="line">joinWith[<span class="type">U</span>](other: <span class="type">Dataset</span>[<span class="type">U</span>], condition: <span class="type">Column</span>): <span class="type">Dataset</span>[(<span class="type">T</span>, <span class="type">U</span>)]</span><br><span class="line">inner equi-join两个dataset</span><br><span class="line"></span><br><span class="line">joinWith[<span class="type">U</span>](other: <span class="type">Dataset</span>[<span class="type">U</span>], condition: <span class="type">Column</span>, joinType: <span class="type">String</span>): <span class="type">Dataset</span>[(<span class="type">T</span>, <span class="type">U</span>)]</span><br><span class="line">joinType可选：inner, cross, outer, full, full_outer, left, left_outer, right, right_outer</span><br><span class="line"></span><br><span class="line">limit(n: <span class="type">Int</span>): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">返回前n行，与head的区别是，head是一个action，会马上返回结果数组。</span><br><span class="line"></span><br><span class="line">map[<span class="type">U</span>](func: (<span class="type">T</span>) ⇒ <span class="type">U</span>)(<span class="keyword">implicit</span> arg0: <span class="type">Encoder</span>[<span class="type">U</span>]): <span class="type">Dataset</span>[<span class="type">U</span>]</span><br><span class="line">在每一个元素应用func函数，返回包含结果集的dataset。</span><br><span class="line"></span><br><span class="line">mapPartitions[<span class="type">U</span>](func: (<span class="type">Iterator</span>[<span class="type">T</span>]) ⇒ <span class="type">Iterator</span>[<span class="type">U</span>])(<span class="keyword">implicit</span> arg0: <span class="type">Encoder</span>[<span class="type">U</span>]): <span class="type">Dataset</span>[<span class="type">U</span>]</span><br><span class="line">在每一个分区应用func函数，返回包含结果集的dataset。</span><br><span class="line"></span><br><span class="line">orderBy(sortExprs: <span class="type">Column</span>*): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">orderBy(sortCol: <span class="type">String</span>, sortCols: <span class="type">String</span>*): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">sort的别名</span><br><span class="line"></span><br><span class="line">sort(sortExprs: <span class="type">Column</span>*): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">sort(sortCol: <span class="type">String</span>, sortCols: <span class="type">String</span>*): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">按指定列排序，默认asc。</span><br><span class="line">e.g. ds.sort($<span class="string">&quot;col1&quot;</span>, $<span class="string">&quot;col2&quot;</span>.desc)</span><br><span class="line"></span><br><span class="line">sortWithinPartitions(sortExprs: <span class="type">Column</span>*): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">sortWithinPartitions(sortCol: <span class="type">String</span>, sortCols: <span class="type">String</span>*): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">分区内排序，同<span class="string">&quot;SORT BY&quot;</span> in <span class="type">SQL</span> (<span class="type">Hive</span> <span class="type">QL</span>).</span><br><span class="line"></span><br><span class="line">randomSplit(weights: <span class="type">Array</span>[<span class="type">Double</span>]): <span class="type">Array</span>[<span class="type">Dataset</span>[<span class="type">T</span>]]</span><br><span class="line">randomSplit(weights: <span class="type">Array</span>[<span class="type">Double</span>], seed: <span class="type">Long</span>): <span class="type">Array</span>[<span class="type">Dataset</span>[<span class="type">T</span>]]</span><br><span class="line">按权重随机分割数据</span><br><span class="line"></span><br><span class="line">repartition(partitionExprs: <span class="type">Column</span>*): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">repartition(numPartitions: <span class="type">Int</span>, partitionExprs: <span class="type">Column</span>*): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">repartition(numPartitions: <span class="type">Int</span>): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">按指定表达式，分区数，重新分区（hash），同<span class="string">&quot;DISTRIBUTE BY&quot;</span> in <span class="type">SQL</span>。</span><br><span class="line">默认分区数为spark.sql.shuffle.partitions</span><br><span class="line"></span><br><span class="line">repartitionByRange(partitionExprs: <span class="type">Column</span>*): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">repartitionByRange(numPartitions: <span class="type">Int</span>, partitionExprs: <span class="type">Column</span>*): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">按指定表达式，分区数，重新分区，采用<span class="type">Range</span> partition方式，按键范围分区。</span><br><span class="line">分区默认排序方式为ascending nulls first，分区内数据未排序。</span><br><span class="line"></span><br><span class="line">sample(withReplacement: <span class="type">Boolean</span>, fraction: <span class="type">Double</span>): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">sample(withReplacement: <span class="type">Boolean</span>, fraction: <span class="type">Double</span>, seed: <span class="type">Long</span>): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">sample(fraction: <span class="type">Double</span>): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">sample(fraction: <span class="type">Double</span>, seed: <span class="type">Long</span>): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">随机取样本数据</span><br><span class="line">withReplacement：<span class="type">Sample</span> <span class="keyword">with</span> replacement or not.</span><br><span class="line">fraction：<span class="type">Fraction</span> of rows to generate, range [<span class="number">0.0</span>, <span class="number">1.0</span>].</span><br><span class="line">seed：<span class="type">Seed</span> <span class="keyword">for</span> sampling.</span><br><span class="line"></span><br><span class="line">select[<span class="type">U1</span>](c1: <span class="type">TypedColumn</span>[<span class="type">T</span>, <span class="type">U1</span>]): <span class="type">Dataset</span>[<span class="type">U1</span>]</span><br><span class="line">根据列/表达式获取列数据</span><br><span class="line"></span><br><span class="line">transform[<span class="type">U</span>](t: (<span class="type">Dataset</span>[<span class="type">T</span>]) ⇒ <span class="type">Dataset</span>[<span class="type">U</span>]): <span class="type">Dataset</span>[<span class="type">U</span>]</span><br><span class="line">应用t函数转换<span class="type">Dataset</span>。</span><br><span class="line"></span><br><span class="line">union(other: <span class="type">Dataset</span>[<span class="type">T</span>]): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">等于<span class="type">UNION</span> <span class="type">ALL</span> in <span class="type">SQL</span>。</span><br><span class="line">注意是按列位置合并：</span><br><span class="line"><span class="keyword">val</span> df1 = <span class="type">Seq</span>((<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)).toDF(<span class="string">&quot;col0&quot;</span>, <span class="string">&quot;col1&quot;</span>, <span class="string">&quot;col2&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> df2 = <span class="type">Seq</span>((<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)).toDF(<span class="string">&quot;col1&quot;</span>, <span class="string">&quot;col2&quot;</span>, <span class="string">&quot;col0&quot;</span>)</span><br><span class="line">df1.union(df2).show</span><br><span class="line"></span><br><span class="line"><span class="comment">// output:</span></span><br><span class="line"><span class="comment">// +----+----+----+</span></span><br><span class="line"><span class="comment">// |col0|col1|col2|</span></span><br><span class="line"><span class="comment">// +----+----+----+</span></span><br><span class="line"><span class="comment">// |   1|   2|   3|</span></span><br><span class="line"><span class="comment">// |   4|   5|   6|</span></span><br><span class="line"><span class="comment">// +----+----+----+</span></span><br><span class="line"></span><br><span class="line">unionByName(other: <span class="type">Dataset</span>[<span class="type">T</span>]): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">同union方法，但是按列名合并：</span><br><span class="line"><span class="keyword">val</span> df1 = <span class="type">Seq</span>((<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)).toDF(<span class="string">&quot;col0&quot;</span>, <span class="string">&quot;col1&quot;</span>, <span class="string">&quot;col2&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> df2 = <span class="type">Seq</span>((<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)).toDF(<span class="string">&quot;col1&quot;</span>, <span class="string">&quot;col2&quot;</span>, <span class="string">&quot;col0&quot;</span>)</span><br><span class="line">df1.unionByName(df2).show</span><br><span class="line"><span class="comment">// output:</span></span><br><span class="line"><span class="comment">// +----+----+----+</span></span><br><span class="line"><span class="comment">// |col0|col1|col2|</span></span><br><span class="line"><span class="comment">// +----+----+----+</span></span><br><span class="line"><span class="comment">// |   1|   2|   3|</span></span><br><span class="line"><span class="comment">// |   6|   4|   5|</span></span><br><span class="line"><span class="comment">// +----+----+----+</span></span><br><span class="line"></span><br><span class="line">where(conditionExpr: <span class="type">String</span>): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">where(condition: <span class="type">Column</span>): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br><span class="line">filter的别名</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="弱类型转换（Untyped-transformations）"><a href="#弱类型转换（Untyped-transformations）" class="headerlink" title="弱类型转换（Untyped transformations）"></a>弱类型转换（Untyped transformations）</h2><p>返回类型为DataFrame而不是Dataset。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">agg(expr: <span class="type">Column</span>, exprs: <span class="type">Column</span>*): <span class="type">DataFrame</span></span><br><span class="line">agg(exprs: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">DataFrame</span></span><br><span class="line">agg(aggExpr: (<span class="type">String</span>, <span class="type">String</span>), aggExprs: (<span class="type">String</span>, <span class="type">String</span>)*): <span class="type">DataFrame</span></span><br><span class="line">在整个dataset进行聚合。</span><br><span class="line">ds.agg(...) 是 ds.groupBy().agg(...) 的简写。</span><br><span class="line">e.g.</span><br><span class="line">ds.agg(max($<span class="string">&quot;age&quot;</span>), avg($<span class="string">&quot;salary&quot;</span>))</span><br><span class="line">ds.agg(<span class="type">Map</span>(<span class="string">&quot;age&quot;</span> -&gt; <span class="string">&quot;max&quot;</span>, <span class="string">&quot;salary&quot;</span> -&gt; <span class="string">&quot;avg&quot;</span>))</span><br><span class="line">ds.agg(<span class="string">&quot;age&quot;</span> -&gt; <span class="string">&quot;max&quot;</span>, <span class="string">&quot;salary&quot;</span> -&gt; <span class="string">&quot;avg&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">apply(colName: <span class="type">String</span>): <span class="type">Column</span></span><br><span class="line">col(colName: <span class="type">String</span>): <span class="type">Column</span></span><br><span class="line">colRegex(colName: <span class="type">String</span>): <span class="type">Column</span></span><br><span class="line">返回指定列。</span><br><span class="line"></span><br><span class="line">crossJoin(right: <span class="type">Dataset</span>[_]): <span class="type">DataFrame</span></span><br><span class="line">cross join。</span><br><span class="line"></span><br><span class="line">cube(col1: <span class="type">String</span>, cols: <span class="type">String</span>*): <span class="type">RelationalGroupedDataset</span></span><br><span class="line">cube(cols: <span class="type">Column</span>*): <span class="type">RelationalGroupedDataset</span></span><br><span class="line">使用指定列创建多维cube。</span><br><span class="line"><span class="comment">//TODO</span></span><br><span class="line"></span><br><span class="line">drop(col: <span class="type">Column</span>): <span class="type">DataFrame</span></span><br><span class="line">drop(colNames: <span class="type">String</span>*): <span class="type">DataFrame</span></span><br><span class="line">drop(colName: <span class="type">String</span>): <span class="type">DataFrame</span></span><br><span class="line">剪掉指定字段。</span><br><span class="line"></span><br><span class="line">groupBy(col1: <span class="type">String</span>, cols: <span class="type">String</span>*): <span class="type">RelationalGroupedDataset</span></span><br><span class="line">groupBy(cols: <span class="type">Column</span>*): <span class="type">RelationalGroupedDataset</span></span><br><span class="line">按指定列分组</span><br><span class="line"></span><br><span class="line">join(right: <span class="type">Dataset</span>[_], joinExprs: <span class="type">Column</span>, joinType: <span class="type">String</span>): <span class="type">DataFrame</span></span><br><span class="line">join(right: <span class="type">Dataset</span>[_], joinExprs: <span class="type">Column</span>): <span class="type">DataFrame</span></span><br><span class="line">join(right: <span class="type">Dataset</span>[_], usingColumns: <span class="type">Seq</span>[<span class="type">String</span>], joinType: <span class="type">String</span>): <span class="type">DataFrame</span></span><br><span class="line">join(right: <span class="type">Dataset</span>[_], usingColumns: <span class="type">Seq</span>[<span class="type">String</span>]): <span class="type">DataFrame</span></span><br><span class="line">join(right: <span class="type">Dataset</span>[_], usingColumn: <span class="type">String</span>): <span class="type">DataFrame</span></span><br><span class="line">join(right: <span class="type">Dataset</span>[_]): <span class="type">DataFrame</span></span><br><span class="line">与另一个<span class="type">DataFrame</span> join。</span><br><span class="line">joinExprs：$<span class="string">&quot;df1Key&quot;</span> === $<span class="string">&quot;df2Key&quot;</span></span><br><span class="line">usingColumn：<span class="type">Seq</span>(<span class="string">&quot;user_id&quot;</span>, <span class="string">&quot;user_name&quot;</span>)</span><br><span class="line">joinType：<span class="type">Default</span> inner. <span class="type">Must</span> be one of: inner, cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi, left_anti.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">na: <span class="type">DataFrameNaFunctions</span></span><br><span class="line">见<span class="type">DataFrameNaFunctions</span></span><br><span class="line"></span><br><span class="line">stat: <span class="type">DataFrameStatFunctions</span></span><br><span class="line">见<span class="type">DataFrameStatFunctions</span></span><br><span class="line"></span><br><span class="line">rollup(col1: <span class="type">String</span>, cols: <span class="type">String</span>*): <span class="type">RelationalGroupedDataset</span></span><br><span class="line">rollup(cols: <span class="type">Column</span>*): <span class="type">RelationalGroupedDataset</span></span><br><span class="line">使用指定列进行rollup聚合。<span class="comment">//TODO</span></span><br><span class="line"></span><br><span class="line">select(col: <span class="type">String</span>, cols: <span class="type">String</span>*): <span class="type">DataFrame</span></span><br><span class="line">select(cols: <span class="type">Column</span>*): <span class="type">DataFrame</span></span><br><span class="line">selectExpr(exprs: <span class="type">String</span>*): <span class="type">DataFrame</span></span><br><span class="line">选取指定列、<span class="type">SQL</span>表达式。</span><br><span class="line"></span><br><span class="line">withColumn(colName: <span class="type">String</span>, col: <span class="type">Column</span>): <span class="type">DataFrame</span></span><br><span class="line">新增或替换一列。</span><br><span class="line"></span><br><span class="line">withColumnRenamed(existingName: <span class="type">String</span>, newName: <span class="type">String</span>): <span class="type">DataFrame</span></span><br><span class="line">将指定列更名。</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="未分组（Ungrouped）"><a href="#未分组（Ungrouped）" class="headerlink" title="未分组（Ungrouped）"></a>未分组（Ungrouped）</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">queryExecution: <span class="type">QueryExecution</span></span><br><span class="line">执行计划</span><br><span class="line"></span><br><span class="line">sparkSession: <span class="type">SparkSession</span></span><br><span class="line">创建该dataset的<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line">sqlContext: <span class="type">SQLContext</span></span><br><span class="line">dataset的<span class="type">SQLContext</span></span><br><span class="line"></span><br><span class="line">toJSON: <span class="type">Dataset</span>[<span class="type">String</span>]</span><br><span class="line">每行数据转成<span class="type">JSON</span>字符串。</span><br><span class="line"></span><br><span class="line">toString(): <span class="type">String</span></span><br><span class="line"><span class="type">Any</span>的toString</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset">Spark API</a></li>
</ul>
]]></content>
      <tags>
        <tag>Spark</tag>
        <tag>API</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark API 全集(2):Spark SQL 函数全集</title>
    <url>/2018/03/23/spark-sql-functions-api/</url>
    <content><![CDATA[<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>org.apache.spark.sql.functions是一个Object，提供了约两百多个函数。</p>
<p>大部分函数与Hive的差不多。</p>
<p>除UDF函数，均可在spark-sql中直接使用。</p>
<p>经过import org.apache.spark.sql.functions._ ，也可以用于Dataframe，Dataset。</p>
<p>version<br>2.3.0</p>
<p>大部分支持Column的函数也支持String类型的列名。这些函数的返回类型基本都是Column。</p>
<p>函数很多，都在下面了。</p>
<span id="more"></span>

<h1 id="聚合函数"><a href="#聚合函数" class="headerlink" title="聚合函数"></a>聚合函数</h1><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">approx_count_distinct</span><br><span class="line">count_distinct近似值</span><br><span class="line"></span><br><span class="line">avg</span><br><span class="line">平均值</span><br><span class="line"></span><br><span class="line">collect_list</span><br><span class="line">聚合指定字段的值到list</span><br><span class="line"></span><br><span class="line">collect_set</span><br><span class="line">聚合指定字段的值到set</span><br><span class="line"></span><br><span class="line">corr</span><br><span class="line">计算两列的<span class="type">Pearson</span>相关系数</span><br><span class="line"></span><br><span class="line">count</span><br><span class="line">计数</span><br><span class="line"></span><br><span class="line">countDistinct</span><br><span class="line">去重计数 <span class="type">SQL</span>中用法</span><br><span class="line">select count(distinct <span class="class"><span class="keyword">class</span>)</span></span><br><span class="line"></span><br><span class="line">covar_pop</span><br><span class="line">总体协方差（population covariance）</span><br><span class="line"></span><br><span class="line">covar_samp</span><br><span class="line">样本协方差（sample covariance）</span><br><span class="line"></span><br><span class="line">first</span><br><span class="line">分组第一个元素</span><br><span class="line"></span><br><span class="line">last</span><br><span class="line">分组最后一个元素</span><br><span class="line"></span><br><span class="line">grouping</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">grouping_id</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">kurtosis</span><br><span class="line"> 计算峰态(kurtosis)值</span><br><span class="line"></span><br><span class="line">skewness</span><br><span class="line"> 计算偏度(skewness)</span><br><span class="line"></span><br><span class="line">max</span><br><span class="line">最大值</span><br><span class="line"></span><br><span class="line">min</span><br><span class="line">最小值</span><br><span class="line"></span><br><span class="line">mean</span><br><span class="line">平均值</span><br><span class="line"></span><br><span class="line">stddev</span><br><span class="line"> 即stddev_samp</span><br><span class="line"></span><br><span class="line">stddev_samp</span><br><span class="line"> 样本标准偏差（sample standard deviation）</span><br><span class="line"></span><br><span class="line">stddev_pop</span><br><span class="line">总体标准偏差（population standard deviation）</span><br><span class="line"></span><br><span class="line">sum</span><br><span class="line">求和</span><br><span class="line"></span><br><span class="line">sumDistinct</span><br><span class="line">非重复值求和 <span class="type">SQL</span>中用法</span><br><span class="line">select sum(distinct <span class="class"><span class="keyword">class</span>)</span></span><br><span class="line"></span><br><span class="line">var_pop</span><br><span class="line">总体方差（population variance）</span><br><span class="line"></span><br><span class="line">var_samp</span><br><span class="line">样本无偏方差（unbiased variance）</span><br><span class="line"></span><br><span class="line">variance</span><br><span class="line">即var_samp</span><br></pre></td></tr></table></figure>
<h1 id="集合函数"><a href="#集合函数" class="headerlink" title="集合函数"></a>集合函数</h1><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">array_contains(column,value)</span><br><span class="line">检查array类型字段是否包含指定元素</span><br><span class="line"></span><br><span class="line">explode</span><br><span class="line"> 展开array或map为多行</span><br><span class="line"></span><br><span class="line">explode_outer</span><br><span class="line">同explode，但当array或map为空或<span class="literal">null</span>时，会展开为<span class="literal">null</span>。</span><br><span class="line"></span><br><span class="line">posexplode</span><br><span class="line">同explode，带位置索引。</span><br><span class="line"></span><br><span class="line">posexplode_outer</span><br><span class="line">同explode_outer，带位置索引。</span><br><span class="line"></span><br><span class="line">from_json</span><br><span class="line">解析<span class="type">JSON</span>字符串为<span class="type">StructType</span> or <span class="type">ArrayType</span>，有多种参数形式，详见文档。</span><br><span class="line"></span><br><span class="line">to_json</span><br><span class="line">转为json字符串，支持<span class="type">StructType</span>, <span class="type">ArrayType</span> of <span class="type">StructTypes</span>, a <span class="type">MapType</span> or <span class="type">ArrayType</span> of <span class="type">MapTypes</span>。</span><br><span class="line"></span><br><span class="line">get_json_object(column,path)</span><br><span class="line">获取指定json路径的json对象字符串。</span><br><span class="line">select get_json_object(&#x27;&#123;<span class="string">&quot;a&quot;</span><span class="number">1</span>,<span class="string">&quot;b&quot;</span>:<span class="number">2</span>&#125;&#x27;,&#x27;$.a&#x27;);</span><br><span class="line">[<span class="type">JSON</span> <span class="type">Path</span>介绍](http:<span class="comment">//blog.csdn.net/koflance/article/details/63262484)</span></span><br><span class="line"></span><br><span class="line">json_tuple(column,fields)</span><br><span class="line">获取json中指定字段值。select json_tuple(&#x27;&#123;<span class="string">&quot;a&quot;</span>:<span class="number">1</span>,<span class="string">&quot;b&quot;</span>:<span class="number">2</span>&#125;&#x27;,&#x27;a&#x27;,&#x27;b&#x27;);</span><br><span class="line"></span><br><span class="line">map_keys</span><br><span class="line">返回map的键组成的array</span><br><span class="line"></span><br><span class="line">map_values</span><br><span class="line">返回map的值组成的array</span><br><span class="line"></span><br><span class="line">size</span><br><span class="line">array or map的长度</span><br><span class="line"></span><br><span class="line">sort_array(e: <span class="type">Column</span>, asc: <span class="type">Boolean</span>)</span><br><span class="line">将array中元素排序（自然排序），默认asc。</span><br></pre></td></tr></table></figure>
<h1 id="时间函数"><a href="#时间函数" class="headerlink" title="时间函数"></a>时间函数</h1><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">add_months(startDate: <span class="type">Column</span>, numMonths: <span class="type">Int</span>)</span><br><span class="line">指定日期添加n月</span><br><span class="line"></span><br><span class="line">date_add(start: <span class="type">Column</span>, days: <span class="type">Int</span>)</span><br><span class="line">指定日期之后n天 e.g. select date_add(&#x27;<span class="number">2018</span><span class="number">-01</span><span class="number">-01</span>&#x27;,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">date_sub(start: <span class="type">Column</span>, days: <span class="type">Int</span>)</span><br><span class="line">指定日期之前n天</span><br><span class="line"></span><br><span class="line">datediff(end: <span class="type">Column</span>, start: <span class="type">Column</span>)</span><br><span class="line">两日期间隔天数</span><br><span class="line"></span><br><span class="line">current_date()</span><br><span class="line">当前日期</span><br><span class="line"></span><br><span class="line">current_timestamp()</span><br><span class="line">当前时间戳，<span class="type">TimestampType</span>类型</span><br><span class="line"></span><br><span class="line">date_format(dateExpr: <span class="type">Column</span>, format: <span class="type">String</span>)</span><br><span class="line">日期格式化</span><br><span class="line"></span><br><span class="line">dayofmonth(e: <span class="type">Column</span>)</span><br><span class="line">日期在一月中的天数，支持 date/timestamp/string</span><br><span class="line"></span><br><span class="line">dayofyear(e: <span class="type">Column</span>)</span><br><span class="line">日期在一年中的天数， 支持 date/timestamp/string</span><br><span class="line"></span><br><span class="line">weekofyear(e: <span class="type">Column</span>)</span><br><span class="line">日期在一年中的周数， 支持 date/timestamp/string</span><br><span class="line"></span><br><span class="line">from_unixtime(ut: <span class="type">Column</span>, f: <span class="type">String</span>)</span><br><span class="line">时间戳转字符串格式</span><br><span class="line"></span><br><span class="line">from_utc_timestamp(ts: <span class="type">Column</span>, tz: <span class="type">String</span>)</span><br><span class="line">时间戳转指定时区时间戳</span><br><span class="line"></span><br><span class="line">to_utc_timestamp(ts: <span class="type">Column</span>, tz: <span class="type">String</span>)</span><br><span class="line">指定时区时间戳转<span class="type">UTF</span>时间戳</span><br><span class="line"></span><br><span class="line">hour(e: <span class="type">Column</span>)</span><br><span class="line">提取小时值</span><br><span class="line"></span><br><span class="line">minute(e: <span class="type">Column</span>)</span><br><span class="line">提取分钟值</span><br><span class="line"></span><br><span class="line">month(e: <span class="type">Column</span>)</span><br><span class="line">提取月份值</span><br><span class="line"></span><br><span class="line">quarter(e: <span class="type">Column</span>)</span><br><span class="line">提取季度</span><br><span class="line"></span><br><span class="line">second(e: <span class="type">Column</span>)</span><br><span class="line">提取秒</span><br><span class="line"></span><br><span class="line">year(e: <span class="type">Column</span>):提取年</span><br><span class="line"></span><br><span class="line">last_day(e: <span class="type">Column</span>)</span><br><span class="line">指定日期的月末日期</span><br><span class="line"></span><br><span class="line">months_between(date1: <span class="type">Column</span>, date2: <span class="type">Column</span>)</span><br><span class="line">计算两日期差几个月</span><br><span class="line"></span><br><span class="line">next_day(date: <span class="type">Column</span>, dayOfWeek: <span class="type">String</span>)</span><br><span class="line">计算指定日期之后的下一个周一、二...，dayOfWeek区分大小写，只接受 <span class="string">&quot;Mon&quot;</span>, <span class="string">&quot;Tue&quot;</span>, <span class="string">&quot;Wed&quot;</span>, <span class="string">&quot;Thu&quot;</span>, <span class="string">&quot;Fri&quot;</span>, <span class="string">&quot;Sat&quot;</span>, <span class="string">&quot;Sun&quot;</span>。</span><br><span class="line"></span><br><span class="line">to_date(e: <span class="type">Column</span>)</span><br><span class="line">字段类型转为<span class="type">DateType</span></span><br><span class="line"></span><br><span class="line">trunc(date: <span class="type">Column</span>, format: <span class="type">String</span>)</span><br><span class="line">日期截断</span><br><span class="line"></span><br><span class="line">unix_timestamp(s: <span class="type">Column</span>, p: <span class="type">String</span>)</span><br><span class="line">指定格式的时间字符串转时间戳</span><br><span class="line"></span><br><span class="line">unix_timestamp(s: <span class="type">Column</span>)</span><br><span class="line">同上，默认格式为 yyyy-<span class="type">MM</span>-dd <span class="type">HH</span>:mm:ss</span><br><span class="line"></span><br><span class="line">unix_timestamp():当前时间戳(秒),底层实现为unix_timestamp(current_timestamp(), yyyy-<span class="type">MM</span>-dd <span class="type">HH</span>:mm:ss)</span><br><span class="line"></span><br><span class="line">window(timeColumn: <span class="type">Column</span>, windowDuration: <span class="type">String</span>, slideDuration: <span class="type">String</span>, startTime: <span class="type">String</span>)</span><br><span class="line">时间窗口函数，将指定时间(<span class="type">TimestampType</span>)划分到窗口</span><br></pre></td></tr></table></figure>

<h1 id="数学函数"><a href="#数学函数" class="headerlink" title="数学函数"></a>数学函数</h1><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">cos,sin,tan</span><br><span class="line">计算角度的余弦，正弦。。。</span><br><span class="line"></span><br><span class="line">sinh,tanh,cosh</span><br><span class="line">计算双曲正弦，正切，。。</span><br><span class="line"></span><br><span class="line">acos,asin,atan,atan2</span><br><span class="line">计算余弦/正弦值对应的角度</span><br><span class="line"></span><br><span class="line">bin</span><br><span class="line">将long类型转为对应二进制数值的字符串<span class="type">For</span> example, bin(<span class="string">&quot;12&quot;</span>) returns <span class="string">&quot;1100&quot;</span>.</span><br><span class="line"></span><br><span class="line">bround</span><br><span class="line">舍入，使用<span class="type">Decimal</span>的<span class="type">HALF_EVEN</span>模式，v&gt;<span class="number">0.5</span>向上舍入，v&lt; <span class="number">0.5</span>向下舍入，v0<span class="number">.5</span>向最近的偶数舍入。</span><br><span class="line"></span><br><span class="line">round(e: <span class="type">Column</span>, scale: <span class="type">Int</span>)</span><br><span class="line"><span class="type">HALF_UP</span>模式舍入到scale为小数点。v&gt;=<span class="number">0.5</span>向上舍入，v&lt; <span class="number">0.5</span>向下舍入,即四舍五入。</span><br><span class="line"></span><br><span class="line">ceil</span><br><span class="line">向上舍入</span><br><span class="line"></span><br><span class="line">floor</span><br><span class="line">向下舍入</span><br><span class="line"></span><br><span class="line">cbrt</span><br><span class="line"><span class="type">Computes</span> the cube-root of the <span class="keyword">given</span> value.</span><br><span class="line"></span><br><span class="line">conv(num:<span class="type">Column</span>, fromBase: <span class="type">Int</span>, toBase: <span class="type">Int</span>)</span><br><span class="line"> 转换数值（字符串）的进制</span><br><span class="line"></span><br><span class="line">log(base: <span class="type">Double</span>, a: <span class="type">Column</span>):$log_&#123;base&#125;(a)$</span><br><span class="line"></span><br><span class="line">log(a: <span class="type">Column</span>):$log_e(a)$</span><br><span class="line"></span><br><span class="line">log10(a: <span class="type">Column</span>):$log_&#123;<span class="number">10</span>&#125;(a)$</span><br><span class="line"></span><br><span class="line">log2(a: <span class="type">Column</span>):$log_&#123;<span class="number">2</span>&#125;(a)$</span><br><span class="line"></span><br><span class="line">log1p(a: <span class="type">Column</span>):$log_&#123;e&#125;(a+<span class="number">1</span>)$</span><br><span class="line"></span><br><span class="line">pmod(dividend: <span class="type">Column</span>, divisor: <span class="type">Column</span>):<span class="type">Returns</span> the positive value of dividend mod divisor.</span><br><span class="line"></span><br><span class="line">pow(l: <span class="type">Double</span>, r: <span class="type">Column</span>):$r^l$ 注意r是列</span><br><span class="line"></span><br><span class="line">pow(l: <span class="type">Column</span>, r: <span class="type">Double</span>):$r^l$ 注意l是列</span><br><span class="line"></span><br><span class="line">pow(l: <span class="type">Column</span>, r: <span class="type">Column</span>):$r^l$ 注意r,l都是列</span><br><span class="line"></span><br><span class="line">radians(e: <span class="type">Column</span>):角度转弧度</span><br><span class="line"></span><br><span class="line">rint(e: <span class="type">Column</span>):<span class="type">Returns</span> the double value that is closest in value to the argument and is equal to a mathematical integer.</span><br><span class="line"></span><br><span class="line">shiftLeft(e: <span class="type">Column</span>, numBits: <span class="type">Int</span>):向左位移</span><br><span class="line"></span><br><span class="line">shiftRight(e: <span class="type">Column</span>, numBits: <span class="type">Int</span>):向右位移</span><br><span class="line"></span><br><span class="line">shiftRightUnsigned(e: <span class="type">Column</span>, numBits: <span class="type">Int</span>):向右位移（无符号位）</span><br><span class="line"></span><br><span class="line">signum(e: <span class="type">Column</span>):返回数值正负符号</span><br><span class="line"></span><br><span class="line">sqrt(e: <span class="type">Column</span>):平方根</span><br><span class="line"></span><br><span class="line">hex(column: <span class="type">Column</span>):转十六进制</span><br><span class="line"></span><br><span class="line">unhex(column: <span class="type">Column</span>):逆转十六进制</span><br></pre></td></tr></table></figure>

<h1 id="混杂-misc-函数"><a href="#混杂-misc-函数" class="headerlink" title="混杂(misc)函数"></a>混杂(misc)函数</h1><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">crc32(e: <span class="type">Column</span>):计算<span class="type">CRC32</span>,返回bigint</span><br><span class="line"></span><br><span class="line">hash(cols: <span class="type">Column</span>*):计算 hash code，返回int</span><br><span class="line"></span><br><span class="line">md5(e: <span class="type">Column</span>):计算<span class="type">MD5</span>摘要，返回<span class="number">32</span>位，<span class="number">16</span>进制字符串</span><br><span class="line"></span><br><span class="line">sha1(e: <span class="type">Column</span>):计算<span class="type">SHA</span><span class="number">-1</span>摘要，返回<span class="number">40</span>位，<span class="number">16</span>进制字符串</span><br><span class="line"></span><br><span class="line">sha2(e: <span class="type">Column</span>, numBits: <span class="type">Int</span>):计算<span class="type">SHA</span><span class="number">-1</span>摘要，返回numBits位，<span class="number">16</span>进制字符串。numBits支持<span class="number">224</span>, <span class="number">256</span>, <span class="number">384</span>, or <span class="number">512.</span></span><br></pre></td></tr></table></figure>

<h1 id="其他非聚合函数"><a href="#其他非聚合函数" class="headerlink" title="其他非聚合函数"></a>其他非聚合函数</h1><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">abs(e: <span class="type">Column</span>)</span><br><span class="line">绝对值</span><br><span class="line"></span><br><span class="line">array(cols: <span class="type">Column</span>*)</span><br><span class="line">多列合并为array，cols必须为同类型</span><br><span class="line"></span><br><span class="line">map(cols: <span class="type">Column</span>*):</span><br><span class="line">将多列组织为map，输入列必须为（key,value)形式，各列的key/value分别为同一类型。</span><br><span class="line"></span><br><span class="line">bitwiseNOT(e: <span class="type">Column</span>):</span><br><span class="line"><span class="type">Computes</span> bitwise <span class="type">NOT</span>.</span><br><span class="line"></span><br><span class="line">broadcast[<span class="type">T</span>](df: <span class="type">Dataset</span>[<span class="type">T</span>]): <span class="type">Dataset</span>[<span class="type">T</span>]:</span><br><span class="line">将df变量广播，用于实现broadcast join。如left.join(broadcast(right), <span class="string">&quot;joinKey&quot;</span>)</span><br><span class="line"></span><br><span class="line">coalesce(e: <span class="type">Column</span>*):</span><br><span class="line">返回第一个非空值</span><br><span class="line"></span><br><span class="line">col(colName: <span class="type">String</span>):</span><br><span class="line">返回colName对应的<span class="type">Column</span></span><br><span class="line"></span><br><span class="line">column(colName: <span class="type">String</span>):</span><br><span class="line">col函数的别名</span><br><span class="line"></span><br><span class="line">expr(expr: <span class="type">String</span>):</span><br><span class="line">解析expr表达式，将返回值存于<span class="type">Column</span>，并返回这个<span class="type">Column</span>。</span><br><span class="line"></span><br><span class="line">greatest(exprs: <span class="type">Column</span>*):</span><br><span class="line">返回多列中的最大值，跳过<span class="type">Null</span></span><br><span class="line"></span><br><span class="line">least(exprs: <span class="type">Column</span>*):</span><br><span class="line">返回多列中的最小值，跳过<span class="type">Null</span></span><br><span class="line"></span><br><span class="line">input_file_name():返</span><br><span class="line">回当前任务的文件名 ？？</span><br><span class="line"></span><br><span class="line">isnan(e: <span class="type">Column</span>):</span><br><span class="line">检查是否<span class="type">NaN</span>（非数值）</span><br><span class="line"></span><br><span class="line">isnull(e: <span class="type">Column</span>):</span><br><span class="line">检查是否为<span class="type">Null</span></span><br><span class="line"></span><br><span class="line">lit(literal: <span class="type">Any</span>):</span><br><span class="line">将字面量(literal)创建一个<span class="type">Column</span></span><br><span class="line"></span><br><span class="line">typedLit[<span class="type">T</span>](literal: <span class="type">T</span>)(<span class="keyword">implicit</span> arg0: scala.reflect.api.<span class="type">JavaUniverse</span>.<span class="type">TypeTag</span>[<span class="type">T</span>]):</span><br><span class="line">将字面量(literal)创建一个<span class="type">Column</span>，literal支持 scala types e.g.: <span class="type">List</span>, <span class="type">Seq</span> and <span class="type">Map</span>.</span><br><span class="line"></span><br><span class="line">monotonically_increasing_id():</span><br><span class="line">返回单调递增唯一<span class="type">ID</span>，但不同分区的<span class="type">ID</span>不连续。<span class="type">ID</span>为<span class="number">64</span>位整型。</span><br><span class="line"></span><br><span class="line">nanvl(col1: <span class="type">Column</span>, col2: <span class="type">Column</span>):</span><br><span class="line">col1为<span class="type">NaN</span>则返回col2</span><br><span class="line"></span><br><span class="line">negate(e: <span class="type">Column</span>):</span><br><span class="line">负数，同df.select( -df(<span class="string">&quot;amount&quot;</span>) )</span><br><span class="line"></span><br><span class="line">not(e: <span class="type">Column</span>):</span><br><span class="line">取反，同df.filter( !df(<span class="string">&quot;isActive&quot;</span>) )</span><br><span class="line"></span><br><span class="line">rand():</span><br><span class="line">随机数[<span class="number">0.0</span>, <span class="number">1.0</span>]</span><br><span class="line"></span><br><span class="line">rand(seed: <span class="type">Long</span>):</span><br><span class="line">随机数[<span class="number">0.0</span>, <span class="number">1.0</span>]，使用seed种子</span><br><span class="line"></span><br><span class="line">randn():</span><br><span class="line">随机数，从正态分布取</span><br><span class="line"></span><br><span class="line">randn(seed: <span class="type">Long</span>):</span><br><span class="line">同上</span><br><span class="line"></span><br><span class="line">spark_partition_id():</span><br><span class="line">返回partition <span class="type">ID</span></span><br><span class="line"></span><br><span class="line">struct(cols: <span class="type">Column</span>*):</span><br><span class="line">多列组合成新的struct column ？？</span><br><span class="line"></span><br><span class="line">when(condition: <span class="type">Column</span>, value: <span class="type">Any</span>):</span><br><span class="line">当condition为<span class="literal">true</span>返回value，如</span><br><span class="line">people.select(when(people(<span class="string">&quot;gender&quot;</span>) === <span class="string">&quot;male&quot;</span>, <span class="number">0</span>)</span><br><span class="line">  .when(people(<span class="string">&quot;gender&quot;</span>) === <span class="string">&quot;female&quot;</span>, <span class="number">1</span>)</span><br><span class="line">  .otherwise(<span class="number">2</span>))</span><br><span class="line">如果没有otherwise且condition全部没命中，则返回<span class="literal">null</span>.</span><br></pre></td></tr></table></figure>
<h1 id="排序函数"><a href="#排序函数" class="headerlink" title="排序函数"></a>排序函数</h1><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">asc(columnName: <span class="type">String</span>):正序</span><br><span class="line"></span><br><span class="line">asc_nulls_first(columnName: <span class="type">String</span>):正序，<span class="literal">null</span>排最前</span><br><span class="line"></span><br><span class="line">asc_nulls_last(columnName: <span class="type">String</span>):正序，<span class="literal">null</span>排最后</span><br><span class="line"></span><br><span class="line">e.g.</span><br><span class="line">df.sort(asc(<span class="string">&quot;dept&quot;</span>), desc(<span class="string">&quot;age&quot;</span>))</span><br><span class="line"></span><br><span class="line">对应有desc函数</span><br><span class="line"> desc,desc_nulls_first,desc_nulls_last</span><br></pre></td></tr></table></figure>
<h1 id="字符串函数"><a href="#字符串函数" class="headerlink" title="字符串函数"></a>字符串函数</h1><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">ascii(e: <span class="type">Column</span>): 计算第一个字符的ascii码</span><br><span class="line"></span><br><span class="line">base64(e: <span class="type">Column</span>): base64转码</span><br><span class="line"></span><br><span class="line">unbase64(e: <span class="type">Column</span>): base64解码</span><br><span class="line"></span><br><span class="line">concat(exprs: <span class="type">Column</span>*):连接多列字符串</span><br><span class="line"></span><br><span class="line">concat_ws(sep: <span class="type">String</span>, exprs: <span class="type">Column</span>*):使用sep作为分隔符连接多列字符串</span><br><span class="line"></span><br><span class="line">decode(value: <span class="type">Column</span>, charset: <span class="type">String</span>): 解码</span><br><span class="line"></span><br><span class="line">encode(value: <span class="type">Column</span>, charset: <span class="type">String</span>): 转码，charset支持 &#x27;<span class="type">US</span>-<span class="type">ASCII</span>&#x27;, &#x27;<span class="type">ISO</span><span class="number">-8859</span><span class="number">-1</span>&#x27;, &#x27;<span class="type">UTF</span><span class="number">-8</span>&#x27;, &#x27;<span class="type">UTF</span><span class="number">-16</span>BE&#x27;, &#x27;<span class="type">UTF</span><span class="number">-16</span>LE&#x27;, &#x27;<span class="type">UTF</span><span class="number">-16</span>&#x27;。</span><br><span class="line"></span><br><span class="line">format_number(x: <span class="type">Column</span>, d: <span class="type">Int</span>):格式化&#x27;#,###,###.##&#x27;形式的字符串</span><br><span class="line"></span><br><span class="line">format_string(format: <span class="type">String</span>, arguments: <span class="type">Column</span>*): 将arguments按format格式化，格式为printf-style。</span><br><span class="line"></span><br><span class="line">initcap(e: <span class="type">Column</span>): 单词首字母大写</span><br><span class="line"></span><br><span class="line">lower(e: <span class="type">Column</span>): 转小写</span><br><span class="line"></span><br><span class="line">upper(e: <span class="type">Column</span>): 转大写</span><br><span class="line"></span><br><span class="line">instr(str: <span class="type">Column</span>, substring: <span class="type">String</span>): substring在str中第一次出现的位置</span><br><span class="line"></span><br><span class="line">length(e: <span class="type">Column</span>): 字符串长度</span><br><span class="line"></span><br><span class="line">levenshtein(l: <span class="type">Column</span>, r: <span class="type">Column</span>): 计算两个字符串之间的编辑距离（<span class="type">Levenshtein</span> distance）</span><br><span class="line"></span><br><span class="line">locate(substr: <span class="type">String</span>, str: <span class="type">Column</span>): substring在str中第一次出现的位置，位置编号从<span class="number">1</span>开始，<span class="number">0</span>表示未找到。</span><br><span class="line"></span><br><span class="line">locate(substr: <span class="type">String</span>, str: <span class="type">Column</span>, pos: <span class="type">Int</span>): 同上，但从pos位置后查找。</span><br><span class="line"></span><br><span class="line">lpad(str: <span class="type">Column</span>, len: <span class="type">Int</span>, pad: <span class="type">String</span>):字符串左填充。用pad字符填充str的字符串至len长度。有对应的rpad，右填充。</span><br><span class="line"></span><br><span class="line">ltrim(e: <span class="type">Column</span>):剪掉左边的空格、空白字符，对应有rtrim.</span><br><span class="line"></span><br><span class="line">ltrim(e: <span class="type">Column</span>, trimString: <span class="type">String</span>):剪掉左边的指定字符,对应有rtrim.</span><br><span class="line"></span><br><span class="line">trim(e: <span class="type">Column</span>, trimString: <span class="type">String</span>):剪掉左右两边的指定字符</span><br><span class="line"></span><br><span class="line">trim(e: <span class="type">Column</span>):剪掉左右两边的空格、空白字符</span><br><span class="line"></span><br><span class="line">regexp_extract(e: <span class="type">Column</span>, exp: <span class="type">String</span>, groupIdx: <span class="type">Int</span>): 正则提取匹配的组</span><br><span class="line"></span><br><span class="line">regexp_replace(e: <span class="type">Column</span>, pattern: <span class="type">Column</span>, replacement: <span class="type">Column</span>): 正则替换匹配的部分，这里参数为列。</span><br><span class="line"></span><br><span class="line">regexp_replace(e: <span class="type">Column</span>, pattern: <span class="type">String</span>, replacement: <span class="type">String</span>): 正则替换匹配的部分</span><br><span class="line"></span><br><span class="line">repeat(str: <span class="type">Column</span>, n: <span class="type">Int</span>):将str重复n次返回</span><br><span class="line"></span><br><span class="line">reverse(str: <span class="type">Column</span>): 将str反转</span><br><span class="line"></span><br><span class="line">soundex(e: <span class="type">Column</span>): 计算桑迪克斯代码（soundex code）<span class="type">PS</span>:用于按英语发音来索引姓名,发音相同但拼写不同的单词，会映射成同一个码。</span><br><span class="line"></span><br><span class="line">split(str: <span class="type">Column</span>, pattern: <span class="type">String</span>): 用pattern分割str</span><br><span class="line"></span><br><span class="line">substring(str: <span class="type">Column</span>, pos: <span class="type">Int</span>, len: <span class="type">Int</span>): 在str上截取从pos位置开始长度为len的子字符串。</span><br><span class="line"></span><br><span class="line">substring_index(str: <span class="type">Column</span>, delim: <span class="type">String</span>, count: <span class="type">Int</span>):<span class="type">Returns</span> the substring from string str before count occurrences of the delimiter delim. <span class="type">If</span> count is positive, everything the left of the <span class="keyword">final</span> delimiter (counting from left) is returned. <span class="type">If</span> count is negative, every to the right of the <span class="keyword">final</span> delimiter (counting from the right) is returned. substring_index performs a <span class="keyword">case</span>-sensitive <span class="keyword">match</span> when searching <span class="keyword">for</span> delim.</span><br><span class="line"></span><br><span class="line">translate(src: <span class="type">Column</span>, matchingString: <span class="type">String</span>, replaceString: <span class="type">String</span>):把src中的matchingString全换成replaceString。</span><br></pre></td></tr></table></figure>

<h1 id="UDF函数"><a href="#UDF函数" class="headerlink" title="UDF函数"></a>UDF函数</h1><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">user-defined function.</span><br><span class="line"></span><br><span class="line">callUDF(udfName: <span class="type">String</span>, cols: <span class="type">Column</span>*): 调用<span class="type">UDF</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df = <span class="type">Seq</span>((<span class="string">&quot;id1&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;id2&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;id3&quot;</span>, <span class="number">5</span>)).toDF(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;value&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> spark = df.sparkSession</span><br><span class="line">spark.udf.register(<span class="string">&quot;simpleUDF&quot;</span>, (v: <span class="type">Int</span>) =&gt; v * v)</span><br><span class="line">df.select($<span class="string">&quot;id&quot;</span>, callUDF(<span class="string">&quot;simpleUDF&quot;</span>, $<span class="string">&quot;value&quot;</span>))</span><br><span class="line"></span><br><span class="line">udf: 定义<span class="type">UDF</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h1><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">cume_dist(): cumulative distribution of values within a window partition</span><br><span class="line"></span><br><span class="line">currentRow(): returns the special frame boundary that represents the current row in the window partition.</span><br><span class="line"></span><br><span class="line">rank():排名，返回数据项在分组中的排名，排名相等会在名次中留下空位 <span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>。</span><br><span class="line"></span><br><span class="line">dense_rank(): 排名，返回数据项在分组中的排名，排名相等会在名次中不会留下空位 <span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>。</span><br><span class="line"></span><br><span class="line">row_number():行号，为每条记录返回一个数字 <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span></span><br><span class="line"></span><br><span class="line">percent_rank():returns the relative rank (i.e. percentile) of rows within a window partition.</span><br><span class="line"></span><br><span class="line">lag(e: <span class="type">Column</span>, offset: <span class="type">Int</span>, defaultValue: <span class="type">Any</span>): offset rows before the current row</span><br><span class="line"></span><br><span class="line">lead(e: <span class="type">Column</span>, offset: <span class="type">Int</span>, defaultValue: <span class="type">Any</span>): returns the value that is offset rows after the current row</span><br><span class="line"></span><br><span class="line">ntile(n: <span class="type">Int</span>): returns the ntile group id (from <span class="number">1</span> to n inclusive) in an ordered window partition.</span><br><span class="line"></span><br><span class="line">unboundedFollowing():returns the special frame boundary that represents the last row in the window partition.</span><br></pre></td></tr></table></figure>

<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$">Spark API</a></li>
</ul>
]]></content>
      <tags>
        <tag>Spark</tag>
        <tag>API</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Spark实现推荐算法-1:推荐算法简介</title>
    <url>/2018/05/02/spark-recommendation-1/</url>
    <content><![CDATA[<h1 id="个性化推荐系统简介"><a href="#个性化推荐系统简介" class="headerlink" title="个性化推荐系统简介"></a>个性化推荐系统简介</h1><p>个性化推荐系统的定义在 1997 年由 Resnick 和 Varian 提出:利用互联网向用户提供信 息和建议，帮助用户选择产品，或模拟售货员帮助用户完成购买行为的系统 。通常推荐 由三个要素组成:推荐算法、用户、候选推荐项目。简单来说，一次推荐过程就是推荐算 法从候选推荐项目中挑出某些项目给用户。</p>
<span id="more"></span>

<p>目前个性化推荐系统已经在电子商务、视频、音乐、新闻、博客等领域得到了广泛应 用。通常这些领域的网站和应用会推荐若干商品或者作品给用户，这些推荐项目通常以“猜 你喜欢”、“购买此商品的顾客也同时购买”、“相似的商品”等形式出现，所推荐的物品便 是推荐系统通过推荐算法从海量物品中挑选出的。由于推荐系统会根据用户的历史行为数 据给出因人而异的推荐结果，所以称之为个性化推荐。</p>
<p>一套完整的个性化推荐系统通常包括用户信息收集、推荐模型计算、推荐结果展示三 个部分。个性化推荐系统首先收集用户的网络操作行为——如浏览、评分、购买等，将这 些行为数据存储到数据仓库中。然后通过机器学习、数据挖掘等相关技术来对这些数据进 行分析，更进一步可以从这些历史数据中学习用户的兴趣爱好，运行推荐算法生成推荐模 型。有了推荐模型，便可以为用户提供个性化的推荐服务，实现主动推荐的目的。个性化 推荐技术可以充分提高信息系统或者站点的服务质量和使用效率，从而吸引更多的用户 。</p>
<p>个性化推荐系统的输入数据可以有多种来源途径，通常分为显示输入和隐式输入两种 类型。显示输入是指用户明确表达喜好的行为，例如给电影评分、给微博点赞、购物后给 予好评或差评等。隐式输入则一般是非特意的行为，如浏览商品详情页面、查看电影评价、 搜索关键词等，这些行为并不代表用户喜欢或讨厌某个物品，但是推荐系统能够从中挖掘 出用户的兴趣信息。</p>
<p>个性化推荐系统的输出也是多样化的，有各种各样的形式。最常见的是推荐列表形式，<br>如亚马逊等电子商务网站的推荐商品列表、YouTube 等视频网站的推荐影片列表、微博等 社交网站的推荐关注用户等。这类是最直接的推荐形式，明确告诉了用户这些是推荐结果， 属于显示的推荐。另一类形式是比较隐式的推荐:购物网站在关键词搜索结果列表中加入 推荐的结果;新闻网站根据推荐算法优化文章的排序;网络问答社区把用户可能感兴趣的 话题优先展示。这些推荐系统融入到了传统的系统模块中，起到提升原有系统功能的效果。</p>
<h1 id="推荐算法分类"><a href="#推荐算法分类" class="headerlink" title="推荐算法分类"></a>推荐算法分类</h1><p>推荐算法是推荐系统的核心，直接决定着推荐系统的性能和效果。依据推荐方法的不 同，通常推荐算法可以分为如下几大类:基于内容的推荐(Content-Based Recommendation)， 协同过滤推荐 (Collaborative Filtering Recommendation) ， 混 合 型 推 荐 (Hybrid Recommendation) 。</p>
<p><img src="http://orfd3hw7v.bkt.clouddn.com/601d55beb52c4fcb9e1d1549cf0a8438.png-liam" alt="推荐算法分类"></p>
<h2 id="基于内容的推荐"><a href="#基于内容的推荐" class="headerlink" title="基于内容的推荐"></a>基于内容的推荐</h2><p>  基于内容的推荐是用项目内容或特征来定义所要推荐的项目或对象，然后系统基于用户评价对象的特征学习用户的兴趣，依据用户资料与待预测项目的匹配程度进行推荐。项目或对象的特征通常是文本内容，比如标题、名称、标签及该物品的其他元数据。基于内容的推荐系统不可避免地受到有限的特征信息获取技术的约束，根据推荐对象的不同，可能会要用到文本挖掘技术、图片识别技术、视频挖掘技术等。特别是自动提取多媒体数据内容特征的相关技术还不是很成熟，导致基于内容的推荐算法在这方面的相关应用受到了很大限制。</p>
<h2 id="协同过滤推荐"><a href="#协同过滤推荐" class="headerlink" title="协同过滤推荐"></a>协同过滤推荐</h2><p>协同过滤(Collaborative Filtering，CF)是一种根据用户对各种产品的交互与评分来推荐 新产品的推荐系统技术。协同过滤最突出的优点就在于它的鲁棒性，适用于多种输入数据: 无论是“显式”的输入数据(例如在电影网站上进行评分)还是“隐式”的(例如用户访 问了一个产品的页面但是没有对产品评分)都可以用来做协同过滤。仅仅根据这些交互， 协同过滤算法就能够知道哪些产品之间比较相似以及哪些用户之间比较相似，然后就可以 做出新的推荐 。相比与基于内容的推荐算法，协同过滤推荐算法不需要分析项目的内 容和特征，所以可以不依赖于特征信息获取技术，从而适用于各种类别的项目。</p>
<p>总体上来说，协同过滤是一种利用集体智慧的方法，使用部分或所有用户的行为数据来进行推荐，不同用户通过推荐系统间接地相互协作，来获取各自感兴趣的信息。协同过 滤的核心是找出用户或项目的邻居，即相似的用户或项目，然后取若干个最近邻居推荐给 用户。根据具体的实现，协同过滤又可以被分为多个子类别，分别是基于记忆的协同过滤 (Memory-Based Collaborative Filtering)，基于模型的协同过滤(Model-Based Collaborative Filtering)，基于社交网络关系的协同过滤(Social-Based Collaborative Filtering)等。</p>
<h3 id="基于记忆的协同过滤"><a href="#基于记忆的协同过滤" class="headerlink" title="基于记忆的协同过滤"></a>基于记忆的协同过滤</h3><p>基于记忆的协同过滤算法使用全部或大部分用户—项目数据来生成预测。基于记忆的 协同过滤常用的有两种 :基于物品的协同过滤(Item-Based Collaborative Filtering， Item-Based CF) 和基于用户的协同过滤 (User-Based Collaborative Filtering，User-Based CF)。</p>
<p>对于类似微博、新闻、论坛、社交和知识问答等内容极多、候选推荐项目庞大的系统， User-Based CF 方法通常优于 Item-Based CF。因为这种情况下，物品相似度的计算量巨大， 而且需要频繁更新，而使用 User-Based CF 能够避免计算物品间的相似度。相反，对于一 个用户数量远超过物品数量的应用来说，此时 Item-Based CF 的性能可能比 User-Based CF 更优，这是由于此种情况的物品间相似度要比用户间相似度更容易计算。由此可见，推荐 系统的设计者必须根据应用的特点来选择合适的算法 ，不能一概而论。</p>
<p>与基于内容的推荐算法相比，Item-Based CF 和 User-Based CF 有下列优点:</p>
<ul>
<li><ol>
<li>能够过滤难以进行内容分析的信息，如艺术品、音乐、文本、视频;</li>
</ol>
</li>
<li><ol start="2">
<li>可以基于一些复杂的，难以描述的概念进行过滤，如品质、风格、流行度等。</li>
</ol>
</li>
</ul>
<p>然而，Item-Based CF 和 User-Based CF 也存在着以下的缺点:</p>
<ul>
<li><ol>
<li>稀疏性问题。如果用户对物品的评价非常稀疏，没有充足的数据用于计算相似性，<br> 那么基于评价所得到的用户或物品间的相似性可能不准确;</li>
</ol>
</li>
<li><ol start="2">
<li>可扩展性问题。随着用户和物品的增多，数据量将会剧增，系统的性能会越来越<br>低;</li>
</ol>
</li>
<li><ol start="3">
<li>冷启动问题。如果某一物品从来没有被评价过，则该物品不会被列为相似物品，<br>那么这个物品就不可能被推荐。类似的，对于一个从未评价过物品的用户，无法 获得推荐。</li>
</ol>
</li>
</ul>
<h3 id="基于模型的协同过滤"><a href="#基于模型的协同过滤" class="headerlink" title="基于模型的协同过滤"></a>基于模型的协同过滤</h3><p>Model-Based CF 简单来说是先用历史数据训练得到推荐模型，再用此模型进行预测的推荐算法。常见的模型算法包括基于概率的朴素贝叶斯算法、聚类算法和基于矩阵分解的算法等 。</p>
<p>常用的矩阵分解算法有:正则化矩阵分解(Regularized Matrix Factorization)，带偏置 的矩阵分解(Biased Matrix Factorization)，交替最小二乘法(Alternating Least Squares，ALS) 和奇异值分解(Singular Value Decomposition，SVD)。这些算法的基本思想都是将稀疏且 高维的用户—物品评分矩阵分解为两个低维矩阵，分别表示用户的特征和物品的特征。用 户的特征矩阵可以代表用户的兴趣，同样的，物品的特征矩阵暗含了物品的特点，这两个 矩阵的乘积能反应出用户对物品的喜好程度 。所以该类方法的主要工作就是计算出用户 的特征矩阵和物品的特征矩阵。</p>
<h3 id="基于社交网络关系的协同过滤"><a href="#基于社交网络关系的协同过滤" class="headerlink" title="基于社交网络关系的协同过滤"></a>基于社交网络关系的协同过滤</h3><p>简单来说，是一种使用用户的社交关系数据、用户好友的喜好数据等社交类数据来进行推荐的算法。该类算法适合于微博、微信等拥有用户社交关系数据的系统。</p>
<h2 id="混合型推荐"><a href="#混合型推荐" class="headerlink" title="混合型推荐"></a>混合型推荐</h2><p>  混合推荐是指将多种推荐技术综合，取长补短，以此来获得更好的推荐效果。常用的<br>方案是将协同过滤技术与另一种算法结合，然后将不同方法得出的推荐结果进行筛选和融<br>合，从而提高推荐效果。</p>
]]></content>
      <tags>
        <tag>Spark</tag>
        <tag>algorithm</tag>
        <tag>recommendation</tag>
      </tags>
  </entry>
  <entry>
    <title>用Spark Streaming实时计算海量用户UV</title>
    <url>/2018/05/01/spark-streaming-uv/</url>
    <content><![CDATA[<h1 id="提出需求"><a href="#提出需求" class="headerlink" title="提出需求"></a>提出需求</h1><p>实时统计业务系统(web,APP之类)的访问人数,即所谓UV,或者DAU指标.</p>
<p>这个需求怕是流计算最最最常见的需求了.</p>
<p>计算UV的关键点就在于去重,即同一个人访问两次是只计一个UV的.在离线计算中统计UV比较容易想到的方法就是用group或distinct机制来去重.但是在实时计算场景,还用group就不太科学了,一个是全量数据的group是比较费时的,第二个是全量数据的group是很费内存和CPU的.特别是当用户量巨大的时候,还要做到秒级更新就更难了.</p>
<p>总结起来,需求就是:海量用户场景UV实时计算.</p>
<span id="more"></span>

<h1 id="接受挑战"><a href="#接受挑战" class="headerlink" title="接受挑战"></a>接受挑战</h1><p>不难发现,问题的主要难点就是<strong>去重</strong>.</p>
<p>Spark Streaming目前没有给出内置方案(这个其实可以有),但是海量数据去重问题早就有解决办法了.<br>所以Spark Streaming程序完全可以利用其他系统的现有方案解决去重问题,比如Redis.</p>
<h2 id="Redis的海量去重计数方案"><a href="#Redis的海量去重计数方案" class="headerlink" title="Redis的海量去重计数方案"></a>Redis的海量去重计数方案</h2><h3 id="Bitmap方案"><a href="#Bitmap方案" class="headerlink" title="Bitmap方案"></a>Bitmap方案</h3><p>所谓的Bitmap就是用一个bit位来标记某个元素对应的Value,比如ID为2的用户,就用第2个bit位来表示,然后用该位的值来表示该用户是否访问过.如果要计算UV,那就只要数一下有多少个1就行啦.</p>
<p>假设我们有40亿用户,使用Bitmap需要2^32个bit位,算下来也就500M左右.</p>
<p>你可能没想到的是,Redis中最常用的数据结构string,就可以实现bitmap算法.</p>
<p>Redis提供了如下命令</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 插入</span><br><span class="line">setbit key offset value</span><br><span class="line">//获取</span><br><span class="line">getbit key offset</span><br><span class="line">//计数</span><br><span class="line">BITCOUNT key [start] [end]</span><br></pre></td></tr></table></figure>
<p>这里offset最大值就是2^32.<br>比如ID为2的用户,可以setbit uv 2 1,来记录.<br>最后要计算UV,就直接 BITCOUNT uv. 这步计数非常快,复杂度是O(1).</p>
<h3 id="HyperLogLog方案"><a href="#HyperLogLog方案" class="headerlink" title="HyperLogLog方案"></a>HyperLogLog方案</h3><p>若要计算很多页面的UV,用bitmap还是比较费空间的,N个页面就得有N个500M.这时候HyperLogLog结构就是一个比较好的选择.</p>
<blockquote>
<p>Redis 在 2.8.9 版本添加了 HyperLogLog 结构。<br>Redis HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定 的、并且是很小的。<br>在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基 数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。<br>但是，因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。</p>
</blockquote>
<p>也就是说HyperLogLog是一种基数统计算法,计算结果是近似值, 12 KB 内存就可以计算2^64 个不同元素的基数.</p>
<p>Redis命令如下:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">redis 127.0.0.1:6379&gt; PFADD runoobkey &quot;redis&quot;</span><br><span class="line"></span><br><span class="line">1) (integer) 1</span><br><span class="line"></span><br><span class="line">redis 127.0.0.1:6379&gt; PFADD runoobkey &quot;mongodb&quot;</span><br><span class="line"></span><br><span class="line">1) (integer) 1</span><br><span class="line"></span><br><span class="line">redis 127.0.0.1:6379&gt; PFADD runoobkey &quot;mysql&quot;</span><br><span class="line"></span><br><span class="line">1) (integer) 1</span><br><span class="line"></span><br><span class="line">redis 127.0.0.1:6379&gt; PFCOUNT runoobkey</span><br><span class="line"></span><br><span class="line">(integer) 3</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><p>下面给出HyperLogLog方案的参考实现:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">stream.foreachRDD &#123; rdd =&gt;</span><br><span class="line">	<span class="comment">//统计人数</span></span><br><span class="line">	rdd.foreachPartition &#123; partition =&gt;</span><br><span class="line">		<span class="comment">//从分区所属executor的redis线程池获取一个连接.</span></span><br><span class="line">		<span class="keyword">val</span> redis = <span class="type">RedisUtil</span>.getRedis</span><br><span class="line">		partition.foreach &#123; <span class="keyword">case</span> (date, userId) =&gt;</span><br><span class="line">			<span class="comment">//统计当前userId</span></span><br><span class="line">			redis.pfadd(<span class="string">s&quot;uv:<span class="subst">$date</span>&quot;</span>, userId)</span><br><span class="line">		&#125;</span><br><span class="line">		redis.close()</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>关于Redis的连接,如果是用java或scala可以使用JedisPool,注意处理序列化即可.</p>
]]></content>
      <tags>
        <tag>Spark</tag>
        <tag>streaming</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Spark实现推荐算法-2:基于用户的协同过滤(理论篇)</title>
    <url>/2018/05/03/spark-recommendation-2/</url>
    <content><![CDATA[<link href="https://cdn.bootcss.com/KaTeX/0.7.1/katex.min.css" rel="stylesheet">

<h1 id="基于用户的协同过滤"><a href="#基于用户的协同过滤" class="headerlink" title="基于用户的协同过滤"></a>基于用户的协同过滤</h1><p>基于用户的协同过滤，即User-Based CF (User-Based Collaborative Filtering)，是基于一个这样的假设“跟你爱好相同的人喜欢的物品，你很可能也喜欢”，所以User-Based CF主要的任务就是找出用户的最近邻居，从而根据最近邻居的喜好做出未知项的评分预测。<br>User-Based CF算法可以分为4个步骤：数据表示、最近邻查询、评分预测、推荐结果产生。</p>
<span id="more"></span>

<h2 id="1-数据表示"><a href="#1-数据表示" class="headerlink" title="1.数据表示"></a>1.数据表示</h2><p>User-Based CF使用的用户—项目数据可以表示为二维矩阵。以用户对项目的评分数据为例，评分数据可以用如下方法表示，</p>
<ul>
<li>用户评分数据表</li>
</ul>
<table>
<thead>
<tr>
<th>Item&#x2F;User</th>
<th>User 1</th>
<th>…</th>
<th>User k</th>
<th>…</th>
<th>User n</th>
</tr>
</thead>
<tbody><tr>
<td>Item 1</td>
<td>R1,1</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>…</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Item j</td>
<td></td>
<td></td>
<td>Rj,k</td>
<td></td>
<td></td>
</tr>
<tr>
<td>…</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Item m</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Rm,n</td>
</tr>
</tbody></table>
<p>其中每一行代表物品所获得的评分。以Item j为例，Rj,k代表用户k给物品j的评分，但并不是所有用户都会对Item j进行评分，所以Rj,k可能存在也可能不存在。这里设用户数为n，物品数目为m，可以将数据转换为评分值组成的二维矩阵R，则R是维度为(m×n)的评分矩阵。</p>
<h2 id="2-最近邻查询"><a href="#2-最近邻查询" class="headerlink" title="2.最近邻查询"></a>2.最近邻查询</h2><p>所谓“最近邻居”是指与当前用户最相似的其他若干用户，为了防止单个用户评分对推荐结果影响过大，导致偏差，通常取N（N为大于1的整数）个相似用户的数据。没有度量就没有比较，所以需要某种方法度量用户间的相似程度，这里相似程度简称为相似度。<br>相似度的计算方法较多，以余弦相似度计算为例说明。余弦相似度(Cosine-based Similarity)是用两个向量间夹角的余弦值来衡量相似性。若要计算用户间的相似度，也就是要计算上一步中矩阵R的列向量间的相似度，取User X和User Y的向量分别为如下公式所示，<br>$$<br>    x ⃗&#x3D;(R_{1,x},…,R_{m,x} )<br>$$<br>$$<br>    y ⃗&#x3D;(R_{1,y},…,R_{m,y} )<br>$$</p>
<p>那么User X与User Y间的余弦相似度为，<br>$$<br>    sim(X,Y)&#x3D;cos⁡(x ⃗,y ⃗ )&#x3D;\frac{x ⃗∙y ⃗}{‖x ⃗ ‖*‖y ⃗ ‖}<br>$$</p>
<p>其中$x ⃗∙y ⃗$表示向量间的内积，$‖x ⃗ ‖$是向量 x ⃗$的模，sim(X,Y)就是相似度的值。依次计算当前用户与其他所有用户的相似度，选取相似度最大的前K个用户作为该用户的最近邻居集合。</p>
<h2 id="3-评分预测"><a href="#3-评分预测" class="headerlink" title="3.评分预测"></a>3.评分预测</h2><p>评分预测的方法也有多种，常用的是加权求和的方法计算预测值。将用户u的最近邻居用户对物品i的打分进行加权求和，权值为各个最近邻居用户与用户u的相似度，然后对加权求和的结果求平均，计算得到用户u对物品i的预测打分，公式如下：</p>
<p>$$<br>    P_{u,i}&#x3D;\frac{\sum_{n∈N}S_{u,n}*R_{n,i}}<br>                                {\sum_{n∈N}(|S_{u,n}|)}<br>$$</p>
<p>N代表用户u的最近邻居用户集合，n是属于N中的一个用户， $S_{u,n}$ 表示用户u和用户n的相似度，$R_{n,i}$ 表示用户n对物品i的评分。根据该公式逐个计算出用户u未评分的所有物品的预测评分，得到预测评分集合。</p>
<h2 id="4-推荐结果产生"><a href="#4-推荐结果产生" class="headerlink" title="4.推荐结果产生"></a>4.推荐结果产生</h2><p>从上一步得到的预测评分集合中，选出评分最高的若干个物品作为推荐物品集，也就是最终的推荐结果。</p>
]]></content>
      <tags>
        <tag>Spark</tag>
        <tag>algorithm</tag>
        <tag>recommendation</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Spark实现推荐算法-4:基于物品的协同过滤(实现篇)</title>
    <url>/2018/05/04/spark-recommendation-item-based-cf/</url>
    <content><![CDATA[<h1 id="算法设计与实现"><a href="#算法设计与实现" class="headerlink" title="算法设计与实现"></a>算法设计与实现</h1><p>基于物品的协同过滤又称Item-Based CF.<br>基于Spark的Item-Based CF算法其实现原理和步骤与经典方法基本一致，不同的地方主要在于具体步骤内的并行化计算。</p>
<span id="more"></span>

<h2 id="相似度算法"><a href="#相似度算法" class="headerlink" title="相似度算法"></a>相似度算法</h2><p>在Spark MLlib中提供了余弦相似度的分布式实现，org.apache.spark.mllib.linalg.distributed包中的IndexedRowMatrix是一个分布式矩阵类，其中提供了一个columnSimilarities方法用于计算该矩阵各列之间的余弦相似度。</p>
<h2 id="预测值计算"><a href="#预测值计算" class="headerlink" title="预测值计算"></a>预测值计算</h2><p>采用加权求和的方法计算预测值.</p>
<h2 id="实现步骤："><a href="#实现步骤：" class="headerlink" title="实现步骤："></a>实现步骤：</h2><h3 id="步骤分解"><a href="#步骤分解" class="headerlink" title="步骤分解"></a>步骤分解</h3><p>Step 1：读取用户评分数据，设用户数为U，物品数目为I，将数据转换为以用户为行，物品为列的二维矩阵R，R维度为（U×I）。矩阵的每一个数据表示某个用户对特定物品的评分。</p>
<p>Step 2：计算R每列之间的相似度，可以得到维度(I×I)的矩阵S。S(i,j)表示物品i和物品j之间的相似度。</p>
<p>Step 3：使用预测值计算公式计算用户对未评分物品的预测评分。得到预测评分矩阵P，P维度为（U×I），P(i,j) 表示用户i对物品j的预测评分。</p>
<p>Step 4：对用户i进行推荐。找出P的第i行中评分最高的前K个物品推荐给用户。K是需要推荐的物品数量。</p>
<h3 id="基于Spark的实现"><a href="#基于Spark的实现" class="headerlink" title="基于Spark的实现"></a>基于Spark的实现</h3><p>鉴于从Spark 2.0.0开始基于Dataset的API成为了主要编程API，本文采用Dataset的API进行实现，使用的语言为Scala。基于Spark平台Item-Based CF可以分为4步实现：</p>
<p>Step 1：读取评分数据集，这里是从Hive数据仓库读取数据，udata是表名，userId、itemId、rating是3个字段，分别是[用户ID，物品ID，评分]。这里按照8:2的比例将数据集分为训练集和测试集。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//读取评分数据集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dataSet</span></span>(): (<span class="type">DataFrame</span>, <span class="type">DataFrame</span>) = &#123;</span><br><span class="line">    <span class="keyword">val</span> table = <span class="string">&quot; udata&quot;</span></span><br><span class="line">    <span class="keyword">val</span> df = spark.sql(<span class="string">s&quot;select userId,itemId,rating from test.<span class="subst">$table</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> <span class="type">Array</span>(training, test) = df.randomSplit(<span class="type">Array</span>(<span class="number">0.8</span>, <span class="number">0.2</span>))</span><br><span class="line">    training.cache()</span><br><span class="line">    test.cache()</span><br><span class="line">    (training, test)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>Step 2：将数据集转换成以用户为行、物品为列的二维评分矩阵，矩阵的每一个行是一个用户对所有物品的评分。然后求出评分矩阵每列之间的相似度，得到一个以物品ID为行和列，以相似度为数据的矩阵，这个矩阵就是物品相似度矩阵。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//计算相似度矩阵</span></span><br><span class="line"><span class="comment">//评分数据转换成矩阵</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">parseToMatrix</span></span>(data: <span class="type">DataFrame</span>): <span class="type">CoordinateMatrix</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> parsedData = data.rdd.map &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Row</span>(user: <span class="type">Int</span>, item: <span class="type">Int</span>, rate: <span class="type">Int</span>) =&gt;</span><br><span class="line">        <span class="type">MatrixEntry</span>(user, item, rate.toDouble)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">CoordinateMatrix</span>(parsedData)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//计算相似度矩阵</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">standardCosine</span></span>(matrix: <span class="type">CoordinateMatrix</span>): <span class="type">RDD</span>[<span class="type">MatrixEntry</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> similarity = matrix.toIndexedRowMatrix().columnSimilarities()</span><br><span class="line">    <span class="keyword">val</span> sim = similarity.entries</span><br><span class="line">    sim</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>Step 3：计算测试集相似物品表。将测试集与训练集、物品相似度表进行左联接，得到测试集相似物品表，字段为[用户ID,物品ID,实际评分,相似物品评分,相似度]。然后将该表注册成临时表testAndSim，并且缓存起来，供下一步使用。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//计算测试集相似物品表</span></span><br><span class="line"><span class="keyword">val</span> testItemSim = spark.sql(</span><br><span class="line">      <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        |select test.userId,test.itemId,test.rating testRating,training.rating,sim.sim</span></span><br><span class="line"><span class="string">        |from test</span></span><br><span class="line"><span class="string">        | left join training on test.userId=training.userId</span></span><br><span class="line"><span class="string">        | left join itemSim sim on test.itemId=sim.itemX and training.itemId=sim.itemY</span></span><br><span class="line"><span class="string">      &quot;&quot;&quot;</span>.stripMargin</span><br><span class="line">    )</span><br><span class="line">    testItemSim.cache()</span><br><span class="line">    testItemSim.createOrReplaceTempView(<span class="string">&quot;testAndSim&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>Step 4：预测测试集中用户对物品的评分。对上一步得到的测试集相似物品表testAndSim进行计算，将该表数据按照(userId,itemId)进行分组，取每组相似度前K个物品的评分。最后依照预测值计算公式求出预测值，得到预测评分表testAndPre，其中pre字段就是对应的预测评分值。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//预测评分</span></span><br><span class="line"><span class="keyword">val</span> sqlRank = <span class="string">&quot;select userId,itemId,testRating,rating,sim,&quot;</span> +</span><br><span class="line">        <span class="string">&quot;rank() over (partition by userId,itemId order by sim desc) rank\n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;from testAndSim&quot;</span></span><br><span class="line"><span class="keyword">val</span> testAndPre = spark.sql(</span><br><span class="line">        <span class="string">&quot;select userId,itemId,first(testRating) rate,nvl(sum(rating*sim)/sum(abs(sim)),0) pre\n&quot;</span> +</span><br><span class="line">          <span class="string">&quot;from( &quot;</span> +</span><br><span class="line">          <span class="string">&quot;  select *&quot;</span> +</span><br><span class="line">          <span class="string">&quot;  from  (&quot;</span> + sqlRank + <span class="string">&quot;) t &quot;</span> +</span><br><span class="line">          <span class="string">s&quot; where rank &lt;= <span class="subst">$k</span> &quot;</span> +</span><br><span class="line">          <span class="string">&quot;) w &quot;</span> +</span><br><span class="line">          <span class="string">&quot;group by userId,itemId&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Spark</tag>
        <tag>algorithm</tag>
        <tag>recommendation</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark 2.x读写MySQL</title>
    <url>/2018/12/25/spark-work-with-mysql-cn/</url>
    <content><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>从 spark 2.0 开始，我们可以使用DataFrameReader 和 DataFrameWriter来读写MySQL。</p>
<span id="more"></span>

<p>SparkSession.read 返回 DataFrameReader.<br>Dataset.write 返回 DataFrameWriter.</p>
<h1 id="煮几个栗子"><a href="#煮几个栗子" class="headerlink" title="煮几个栗子"></a>煮几个栗子</h1><p>其中spark是SparkSession</p>
<h2 id="read-example"><a href="#read-example" class="headerlink" title="read example"></a>read example</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> prop=<span class="keyword">new</span> java.util.<span class="type">Properties</span>()</span><br><span class="line">prop.put(<span class="string">&quot;user&quot;</span>,<span class="string">&quot;username&quot;</span>)</span><br><span class="line">prop.put(<span class="string">&quot;password&quot;</span>,<span class="string">&quot;yourpassword&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> url=<span class="string">&quot;jdbc:mysql://host:3306/db_name&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df=spark.read.jdbc(url,<span class="string">&quot;table_name&quot;</span>,prop)</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure>

<h2 id="read-example2"><a href="#read-example2" class="headerlink" title="read example2"></a>read example2</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> jdbcDF = spark.read</span><br><span class="line">  .format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql:dbserver&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;schema.tablename&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;username&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;password&quot;</span>)</span><br><span class="line">  .load()</span><br></pre></td></tr></table></figure>

<h2 id="read-example3"><a href="#read-example3" class="headerlink" title="read example3"></a>read example3</h2><p>如果你想读取的不是一张表的数据，而是一句SQL的查询结果，那么可以这么操作：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> sql=<span class="string">&quot;&quot;&quot;select * from db.your_table where id&gt;1&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">val</span> jdbcDF = spark.read</span><br><span class="line">  .format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql:dbserver&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;dbtable&quot;</span>,  <span class="string">s&quot;( <span class="subst">$sql</span> ) t&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;username&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;password&quot;</span>)</span><br><span class="line">  .load()</span><br></pre></td></tr></table></figure>


<h2 id="write-example"><a href="#write-example" class="headerlink" title="write example"></a>write example</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SaveMode</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> prop=<span class="keyword">new</span> java.util.<span class="type">Properties</span>()</span><br><span class="line">prop.put(<span class="string">&quot;user&quot;</span>,<span class="string">&quot;username&quot;</span>)</span><br><span class="line">prop.put(<span class="string">&quot;password&quot;</span>,<span class="string">&quot;yourpassword&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> url=<span class="string">&quot;jdbc:mysql://host:3306/db_name&quot;</span></span><br><span class="line"><span class="comment">//df是一个dataframe,也就是你要写入的数据</span></span><br><span class="line">df.write.mode(<span class="type">SaveMode</span>.<span class="type">Append</span>).jdbc(url,<span class="string">&quot;table_name&quot;</span>,prop)</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Spark</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive窗口函数实战</title>
    <url>/2019/02/20/hive-window-sql/</url>
    <content><![CDATA[<p>本文将介绍使用Hive强大的窗口函数，解决实际问题的方法，仅供参考。</p>
<span id="more"></span>

<h1 id="连续登陆-x2F-活跃-x2F-访问问题"><a href="#连续登陆-x2F-活跃-x2F-访问问题" class="headerlink" title="连续登陆&#x2F;活跃&#x2F;访问问题"></a>连续登陆&#x2F;活跃&#x2F;访问问题</h1><p>假设有一张登陆日志表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_login (</span><br><span class="line">    uid <span class="type">int</span> comment <span class="string">&#x27;用户ID&#x27;</span>,</span><br><span class="line">    dt <span class="type">int</span> comment <span class="string">&#x27;日期yyyymmdd&#x27;</span></span><br><span class="line">) STORED <span class="keyword">AS</span> PARQUET</span><br></pre></td></tr></table></figure>
<p>PS：如果用户在某天多次登陆，也只会有一条记录。</p>
<h2 id="计算距离上次登录天数"><a href="#计算距离上次登录天数" class="headerlink" title="计算距离上次登录天数"></a>计算距离上次登录天数</h2><p>先来个最简单的案例, 要算出用户每次登陆，距离上次有多少天。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span>,dt <span class="operator">-</span> <span class="built_in">lag</span>(dt,<span class="number">1</span>) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> uid <span class="keyword">ORDER</span> <span class="keyword">BY</span> dt)</span><br><span class="line"><span class="keyword">from</span> user_login</span><br></pre></td></tr></table></figure>
<p>lag 函数是取上N行的指定列的数据，N默认是1，所以也可以写成lag(dt)。</p>
<h2 id="计算连续登录天数"><a href="#计算连续登录天数" class="headerlink" title="计算连续登录天数"></a>计算连续登录天数</h2><p>计算用户连续登陆了多少天，一段时间内可能会有多次连续登陆。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    uid,</span><br><span class="line">    <span class="built_in">max</span>(dt)<span class="operator">-</span><span class="built_in">min</span>(dt) diff,</span><br><span class="line">    collect_set(dt)</span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line">    <span class="keyword">SELECT</span></span><br><span class="line">        a.uid,</span><br><span class="line">        a.dt,</span><br><span class="line">        dt <span class="operator">-</span> rn num</span><br><span class="line">    <span class="keyword">from</span> (</span><br><span class="line">        <span class="keyword">SELECT</span> uid, dt, <span class="built_in">row_number</span>() <span class="keyword">over</span> ( <span class="keyword">PARTITION</span> <span class="keyword">BY</span> uid <span class="keyword">ORDER</span> <span class="keyword">BY</span> dt ) rn</span><br><span class="line">        <span class="keyword">FROM</span> user_login</span><br><span class="line">    ) a</span><br><span class="line">) b</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> uid,num</span><br></pre></td></tr></table></figure>

<p>这里需要理解的关键点是’dt - rn’得到num这个计算，也就是将日期减去行号，<br>这是个取巧的算法，看起来有点奇怪。</p>
<p>因为row_number()是按dt正序的，所以连续的dt减去行号，会得到同一个值，而非连续的则不会。<br>举个例子：</p>
<table>
<thead>
<tr>
<th>uid</th>
<th>dt</th>
<th>row_number</th>
<th>dt - row_number (num)</th>
</tr>
</thead>
<tbody><tr>
<td>u1</td>
<td>20190203</td>
<td>1</td>
<td>20190202</td>
</tr>
<tr>
<td>u1</td>
<td>20190204</td>
<td>2</td>
<td>20190202</td>
</tr>
<tr>
<td>u1</td>
<td>20190206</td>
<td>3</td>
<td>20190203</td>
</tr>
</tbody></table>
<p>这里其实就是子查询b的结果，理解了这个，最外层的语句就很好理解了。</p>
<h2 id="标记用户连续间隔不超过3天的登录"><a href="#标记用户连续间隔不超过3天的登录" class="headerlink" title="标记用户连续间隔不超过3天的登录"></a>标记用户连续间隔不超过3天的登录</h2><p>标记用户连续的活跃记录，只要不超过3天间隔，就算连续活跃。</p>
<p>假设有记录如下：</p>
<table>
<thead>
<tr>
<th>uid</th>
<th>dt</th>
</tr>
</thead>
<tbody><tr>
<td>u1</td>
<td>20190203</td>
</tr>
<tr>
<td>u1</td>
<td>20190204</td>
</tr>
<tr>
<td>u1</td>
<td>20190206</td>
</tr>
<tr>
<td>u1</td>
<td>20190210</td>
</tr>
<tr>
<td>u1</td>
<td>20190211</td>
</tr>
</tbody></table>
<p>这里需求就是要把前3行和后2行区分出来，因为20190206和20190210之间隔了4天。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span>,<span class="built_in">max</span>(flag) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> uid <span class="keyword">ORDER</span> <span class="keyword">BY</span> dt <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> UNBOUNDED PRECEDING <span class="keyword">and</span> <span class="keyword">CURRENT</span> <span class="type">ROW</span>) session_id</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> uid,dt,if(dt<span class="operator">-</span><span class="built_in">lag</span>(dt)<span class="operator">&gt;</span><span class="number">3</span>,dt,<span class="number">0</span>) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> uid <span class="keyword">ORDER</span> <span class="keyword">BY</span> dt) flag</span><br><span class="line">    <span class="keyword">from</span> user_login</span><br><span class="line">) t</span><br></pre></td></tr></table></figure>

<p>这里子查询t就是先标记处间隔大于3的记录。</p>
<table>
<thead>
<tr>
<th>uid</th>
<th>dt</th>
<th>flag</th>
</tr>
</thead>
<tbody><tr>
<td>u1</td>
<td>20190203</td>
<td>0</td>
</tr>
<tr>
<td>u1</td>
<td>20190204</td>
<td>0</td>
</tr>
<tr>
<td>u1</td>
<td>20190206</td>
<td>0</td>
</tr>
<tr>
<td>u1</td>
<td>20190210</td>
<td>20190210</td>
</tr>
<tr>
<td>u1</td>
<td>20190211</td>
<td>0</td>
</tr>
</tbody></table>
<p>最外层的查询，通过窗口函数，从当前行往上找出最大的flag，因为按dt排序，得到的其实就是最近的flag，<br>将该flag设置为自己的session_id，表示所属同一批连续活跃的记录。<br>得到：</p>
<table>
<thead>
<tr>
<th>uid</th>
<th>dt</th>
<th>flag</th>
<th>session_id</th>
</tr>
</thead>
<tbody><tr>
<td>u1</td>
<td>20190203</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>u1</td>
<td>20190204</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>u1</td>
<td>20190206</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>u1</td>
<td>20190210</td>
<td>20190210</td>
<td>20190210</td>
</tr>
<tr>
<td>u1</td>
<td>20190211</td>
<td>0</td>
<td>20190210</td>
</tr>
</tbody></table>
<p>现在前3行和后2行，就被标记为不同的组了。</p>
<p><strong>如果我们有的是精确到分、秒的日志，这个解决方案可以用来给用户的访问记录打上会话标记，然后拿来计算单次访问时长，访问深度等等，只是这里的dt改成时间戳而已。</strong></p>
<p>计算会话标记其实是数仓同事提的需求，也是我想出这个sql方案的初衷。</p>
<p>希望能帮到需要的人。^-^</p>
]]></content>
      <tags>
        <tag>hive</tag>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title>[读书笔记] 黑客与画家</title>
    <url>/2019/01/19/Hackers-and-Painters-note/</url>
    <content><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>《黑客与画家》是硅谷创业之父Paul Graham的文集，这里的黑客指的是优秀的软件工程师，作者将黑客与画家作比较，认为与黑客最接近的职业是画家，这便是这本书书名的由来了。<br>Paul Graham在1995年创办了Viaweb，帮助个人用户方便的开设网上商店，1998年以5千万美元的价格卖给Yahoo公司。<br>2005年他创建了风投公司YC，被称为全世界最牛的创业孵化器，YC帮助了大量的创业公司（到2018年投资了1450家公司），比较成功的案例有：Airbnb，Stripe，Dropbox，Docker，GitLab等等。</p>
<p>这本书中包含了作者对黑客精神，创业，互联网，社会，编程语言的诸多观点，读完让人感觉受益匪浅，甚至会有创业的冲动，哈哈。</p>
<blockquote>
<p>以下是对这本书内容的一些摘抄。</p>
</blockquote>
<span id="more"></span>

<h1 id="序-amp-前言"><a href="#序-amp-前言" class="headerlink" title="序 &amp; 前言"></a>序 &amp; 前言</h1><p>黑客行为必须包含三个特点：好玩、高智商、探索精神。</p>
<h1 id="1-为什么书呆子不受欢迎"><a href="#1-为什么书呆子不受欢迎" class="headerlink" title="1 为什么书呆子不受欢迎"></a>1 为什么书呆子不受欢迎</h1><p>我认为，这就是问题的根源。<strong>“书呆子”的目标具有两重性</strong>。他们毫无疑问想让自己受欢迎，但是他们更愿意让自己聪明。“受欢迎”并不是你在课后时间随便做一做就能实现的，尤其是在美国的中学中，在这里，所有人为了个人魅力都会进行激烈竞争。</p>
<p>表面上，学校的使命是教育儿童。事实上，学校的真正目的是把儿童都关在同一个地方，以便大人们白天可以腾出手来把事情做完。</p>
<p>校园生活的两大恐怖之处——残忍和无聊——也是出于同样的原因。</p>
<p>如果你觉得人生糟透了，那不是因为体内激素分泌失调（你父母相信这种说法），也不是因为人生真的糟透了（你本人相信这种说法）。那是因为你对成年人不再具有经济价值（与工业社会以前的时期相比），所以他们把你扔在学校里，一关就是好几年，根本没有真正的事情可做。任何这种类型的组织都是可怕的生存环境。你根本不需要寻找其他的原因就能解释为什么青少年是不快乐的。</p>
<h1 id="2-黑客与画家"><a href="#2-黑客与画家" class="headerlink" title="2 黑客与画家"></a>2 黑客与画家</h1><p>这个学科的一端是纯粹的数学家，他们自称“计算机科学家”，只是为了得到国防部研究局（DARPA）的项目资助。中间部分是计算机博物学家，研究各种专门性的题目，比如网络数据的路由算法。另一端则是黑客，只想写出有趣的软件，对于他们来说，计算机只是一种表达的媒介，就像建筑师手里的混凝土，或者画家手里的颜料。</p>
<p>创造优美事物的方式往往不是从头做起，而是在现有成果的基础上做一些小小的调整，或者将已有的观点用比较新的方式组合起来。这种类型的工作很难用研究性的论文表达。</p>
<p>黑客搞懂“计算理论”（theory of computation）的必要性，与画家搞懂颜料化学成分的必要性差不多大。</p>
<p><strong>编程语言是用来帮助思考程序的，而不是用来表达你已经想好的程序。它应该是一支铅笔，而不是一支钢笔。</strong></p>
<p>这似乎是大公司的普遍情况。大公司这样安排的原因是为了减少结果的标准差。因为实际上只有很少一部分黑客懂得如何正确设计软件，</p>
<p>真正竞争软件设计的战场是新兴领域的市场，这里还没有人建立过防御工事。只要你能做出大胆的设计，由一个人或一批人同时负责设计和实现产品，你就能在这里战胜大公司</p>
<p>黑客如何才能做自己喜欢的事情？我认为这个问题的解决方法是一个几乎所有创作者都知道的方法：找一份养家糊口的“白天工作”（day job）。这个词是从音乐家身上来的，他们晚上表演音乐，所以白天可以找一份其他工作。更一般地说，“白天工作”的意思是，你有一份为了赚钱的工作，还有一份为了爱好的工作。<br><strong>黑客解决生计问题的方法是找一份“白天工作”，然后在其余时间开发优美的软件，</strong></p>
<p>因为黑客更像创作者，而不是科学家，所以要了解黑客，不应该在科学家身上寻找启示，而是应该观察其他类型的创作者。</p>
<p>也许对于黑客来说，采取像画家这样的做法很有好处：应该定期地从头开始，而不要长年累月地在一个项目上不断工作，并且试图把所有的最新想法都以修订版的形式包括进去。</p>
<p>创作者另一个学习的途径是通过范例。</p>
<p>黑客就像画家，工作起来是有心理周期的。有时候，你有了一个令人兴奋的新项目，你会愿意为它一天工作16个小时。等过了这一阵，你又会觉得百无聊赖，对所有事情都提不起兴趣。</p>
<p>普通黑客与优秀黑客的所有区别之中，会不会“换位思考”可能是最重要的单个因素。有些黑客很聪明，但是完全以自我为中心，根本不会设身处地为用户考虑。这样的人很难设计出优秀软件，因为他们不从用户的角度看待问题。</p>
<h1 id="3-不能说的话"><a href="#3-不能说的话" class="headerlink" title="3 不能说的话"></a>3 不能说的话</h1><p>如果别人告诉你应该相信什么，你就真的相信了，那么你就会和别人一样犯下同样的错误。</p>
<p>，并不是所有不能说出口的话都是我们要找的答案。实际上，只有同时满足两个条件才行。第一个条件是，这些话不能说出口；第二个条件是，它们是正确的，或者看起来很可能正确，值得进一步讨论。</p>
<p>守口如瓶”的真正缺点在于，你从此无法享受讨论带来的好处了。讨论一个观点会产生更多的观点，不讨论就什么观点也没有。</p>
<h1 id="4-良好的坏习惯"><a href="#4-良好的坏习惯" class="headerlink" title="4 良好的坏习惯"></a>4 良好的坏习惯</h1><p>公民自由真的是国家富强的原因，而不是结果吗？我认为是的。在我看来，一个人们拥有言论自由和行动自由的社会，往往最有可能采纳最优方案，而不是采纳最有权势的人提出的方案。</p>
<p>正是那些不服从管教的人们，才是美国财富与力量的源泉。</p>
<h1 id="5-另一条路"><a href="#5-另一条路" class="headerlink" title="5 另一条路"></a>5 另一条路</h1><p>如果你想把钱藏在安全的地方，请问你是选择放在家中床垫下面，还是放在银行？这个比喻对服务器管理的方方面面都适用，不仅是安全性，还包括正常运行时间、带宽、负载管理、备份等，都是我们占优。</p>
<p><strong>不少公司都很想知道，什么事情可以外包，什么事情不可以外包。一个可能的答案是，公司内部所有不直接感受到竞争压力的部门都应该外包出去，让它们暴露在竞争压力之下。</strong></p>
<p>只有懂得设计的黑客，才能设计软件，不能交给对软件一知半解的设计师。如果你不打算自己动手设计和开发，那就不要创业。</p>
<h1 id="6-如何创造财富"><a href="#6-如何创造财富" class="headerlink" title="6 如何创造财富"></a>6 如何创造财富</h1><p>从经济学观点看，你可以把创业想象成一个压缩过程，你的所有工作年份被压缩成了短短几年。你不再是低强度地工作四十年，而是以极限强度工作四年。在高技术领域，这种压缩的回报尤其丰厚，工作效率越高，额外报酬就越高。</p>
<p>真正重要的是做出人们需要的东西，而不是加入某个公司。</p>
<p><strong>要致富，你需要两样东西：可测量性和可放大性</strong>。</p>
<p>乔布斯曾经说过，创业的成败取决于最早加入公司的那十个人。我基本同意这个观点，虽然我觉得真正决定成败的其实只是前五人。小团队的优势不在于它本身的小，而在于你可以选择成员。我们不需要小村庄的那种“小”，而需要全明星第一阵容的那种“小”。</p>
<p>选择公司要解决什么问题应该以问题的难度作为指引，而且此后的各种决策都应该以此为原则。Viaweb的一个经验法则就是“更上一层楼”。<br>总的来说，这也是很好的处事原则。如果你有两个选择，就选较难的那个。如果你要选择是坐在家里看电视，还是外出跑步，那就出去跑步吧。这个方法有效的原因可能是遇到两个一难一易的选择时，往往出于懒惰的缘故，你会选择较易的那个选项。在意识深处，你其实知道不懒惰的做法会带来更好的结果，这个方法只是迫使你接受这一点。</p>
<h1 id="7-关注贫富分化"><a href="#7-关注贫富分化" class="headerlink" title="7 关注贫富分化"></a>7 关注贫富分化</h1><p>无论在物质上，还是在社会地位上，技术好像都缩小了富人与穷人之间的差距，而不是让这种差距扩大了。</p>
<h1 id="9-设计者的品味"><a href="#9-设计者的品味" class="headerlink" title="9 设计者的品味"></a>9 设计者的品味</h1><p>等到你逐渐对一件事产生热情的时候，就不会满足于模仿了。你的品味就进入了第二阶段，开始自觉地进行原创。</p>
<h1 id="10-编程语言解析"><a href="#10-编程语言解析" class="headerlink" title="10 编程语言解析"></a>10 编程语言解析</h1><p>头重脚轻”的语言：它们的内核设计得并非很好，但是却有着无数强大的函数库，可以用来解决特定的问题。</p>
<h1 id="11-百年后的编程语言"><a href="#11-百年后的编程语言" class="headerlink" title="11 百年后的编程语言"></a>11 百年后的编程语言</h1><p>这有点像买房子的时候你应该先考虑地理位置。别的地方将来出问题都有办法弥补，但是地理位置是没法变的。</p>
<p>浪费程序员的时间而不是浪费机器的时间才是真正的无效率。随着计算机速度越来越快，这会变得越来越明显。</p>
<h1 id="13-书呆子的复仇"><a href="#13-书呆子的复仇" class="headerlink" title="13 书呆子的复仇"></a>13 书呆子的复仇</h1><p>在大型组织内部，有一个专门的术语描述这种跟随大多数人的选择的做法，叫做“业界最佳实践”。这个词出现的原因其实就是为了让你的经理可以推卸责任。既然我选择的是“业界最佳实践”，如果不成功，项目失败了，那么你也无法指责我，因为做出选择的人不是我，而是整个“业界”。</p>
<p>第一，不同语言的编程能力不一样。第二，大多数经理故意忽视第一点。你把这两点事实结合起来，其实就得到了赚钱的诀窍</p>
<h1 id="14-梦寐以求的编程语言"><a href="#14-梦寐以求的编程语言" class="headerlink" title="14 梦寐以求的编程语言"></a>14 梦寐以求的编程语言</h1><p>让我们试着描述黑客心目中梦寐以求的语言来为以上内容做个小结。这种语言干净简练，具有最高层次的抽象和互动性，而且很容易装备，可以只用很少的代码就解决常见的问题。不管是什么程序，你真正要写的代码几乎都与你自己的特定设置有关，其他具有普遍性的问题都有现成的函数库可以调用。</p>
<h1 id="15-设计与研究"><a href="#15-设计与研究" class="headerlink" title="15 设计与研究"></a>15 设计与研究</h1><p>其中有一点是正确的，那就是如果你正在设计某种新东西，就应该尽快拿出原型，听取用户的意见。</p>
<p>原型（prototype）并不只是模型（model），不等于将来一定要另起炉灶，你完全能够在原型的基础上直接做出最后的成品。我认为，只要有可能，你就应该这样做。</p>
<p>为了把产品设计好，你必须对自己说：“哇，这个产品太棒了，我一定要设计好！”而不是心想：“这种垃圾玩意，只有傻瓜才会喜欢，随便设计一下就行了。”设计意味着做出符合人类特点和需要的产品。但是，“人类”不仅包括用户，还包括设计师，所以设计工作本身也必须符合设计师的特点和需要。</p>
]]></content>
      <tags>
        <tag>book</tag>
      </tags>
  </entry>
  <entry>
    <title>Maven用户都应该知道的一些事：关于依赖的常见问题</title>
    <url>/2019/06/20/maven-details-about-dependency/</url>
    <content><![CDATA[<h1 id="依赖范围-scope-不同选项的区别"><a href="#依赖范围-scope-不同选项的区别" class="headerlink" title="依赖范围(scope)不同选项的区别"></a>依赖范围(scope)不同选项的区别</h1><p>依赖范围参数的作用是控制依赖在不同阶段与classpath的关系，具体区别如下图所示。</p>
<span id="more"></span>

<p><img src="https://raw.githubusercontent.com/Liam8/img/master/maven/scope.png" alt="依赖范围图"></p>
<p>表中没有列出的值是import，这个选项是用于引入dependencyManagement，下文会有介绍。</p>
<h1 id="依赖调解，调解同一依赖的不同版本"><a href="#依赖调解，调解同一依赖的不同版本" class="headerlink" title="依赖调解，调解同一依赖的不同版本"></a>依赖调解，调解同一依赖的不同版本</h1><p>假设你的项目有如下的依赖树</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">POM</span><br><span class="line">|-- A</span><br><span class="line">|   `-- B 1.0</span><br><span class="line">|-- C</span><br><span class="line">|   `-- D</span><br><span class="line">|       `-- B 2.0</span><br><span class="line">`-- E</span><br><span class="line">    `-- B 3.0</span><br></pre></td></tr></table></figure>

<p>POM同时依赖了B的1.0和2.0版本，可Maven是不会重复引入相同坐标的依赖的，那么究竟哪个版本会生效呢？</p>
<p>Maven对于依赖的调解遵循两个基本原则：</p>
<ul>
<li>1 依赖路径长度短者优先；</li>
<li>2 如果依赖路径长度相同，则后声明优先。</li>
</ul>
<p>所以根据1，2.0版本被排除，根据2，3.0版本被实际引入。</p>
<p>如果你自己明确知道该引入哪个版本的B，那么直接在POM中声明B依赖就好了，因为这时的依赖路径是最短了。</p>
<h1 id="可选依赖-optional-含义"><a href="#可选依赖-optional-含义" class="headerlink" title="可选依赖(optional)含义"></a>可选依赖(optional)含义</h1><p>可选依赖的作用就是声明该依赖不被传递依赖。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">POM</span><br><span class="line">`-- A</span><br><span class="line">   `-- B(optional)</span><br></pre></td></tr></table></figure>
<p>这里B不会被引入。</p>
<h1 id="Super-POM-Project-Object-Model"><a href="#Super-POM-Project-Object-Model" class="headerlink" title="Super POM(Project Object Model)"></a>Super POM(Project Object Model)</h1><p>Super POM是Maven自带的全局POM文件，所有的POM文件都默认继承了Super POM。其中定义了各种默认配置，以简化POM文件的编写。下面是Maven 3.5.4的Super POM的核心部分。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">project</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">  <span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">id</span>&gt;</span>central<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>Central Repository<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repo.maven.apache.org/maven2<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">layout</span>&gt;</span>default<span class="tag">&lt;/<span class="name">layout</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">snapshots</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>false<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">snapshots</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">  <span class="tag">&lt;<span class="name">pluginRepositories</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">pluginRepository</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">id</span>&gt;</span>central<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>Central Repository<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repo.maven.apache.org/maven2<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">layout</span>&gt;</span>default<span class="tag">&lt;/<span class="name">layout</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">snapshots</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>false<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">snapshots</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">releases</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">updatePolicy</span>&gt;</span>never<span class="tag">&lt;/<span class="name">updatePolicy</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">releases</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">pluginRepository</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">pluginRepositories</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">  <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">directory</span>&gt;</span>$&#123;project.basedir&#125;/target<span class="tag">&lt;/<span class="name">directory</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">outputDirectory</span>&gt;</span>$&#123;project.build.directory&#125;/classes<span class="tag">&lt;/<span class="name">outputDirectory</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">finalName</span>&gt;</span>$&#123;project.artifactId&#125;-$&#123;project.version&#125;<span class="tag">&lt;/<span class="name">finalName</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">testOutputDirectory</span>&gt;</span>$&#123;project.build.directory&#125;/test-classes<span class="tag">&lt;/<span class="name">testOutputDirectory</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">sourceDirectory</span>&gt;</span>$&#123;project.basedir&#125;/src/main/java<span class="tag">&lt;/<span class="name">sourceDirectory</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scriptSourceDirectory</span>&gt;</span>$&#123;project.basedir&#125;/src/main/scripts<span class="tag">&lt;/<span class="name">scriptSourceDirectory</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">testSourceDirectory</span>&gt;</span>$&#123;project.basedir&#125;/src/test/java<span class="tag">&lt;/<span class="name">testSourceDirectory</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">resources</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">resource</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">directory</span>&gt;</span>$&#123;project.basedir&#125;/src/main/resources<span class="tag">&lt;/<span class="name">directory</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">resource</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">resources</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">testResources</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">testResource</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">directory</span>&gt;</span>$&#123;project.basedir&#125;/src/test/resources<span class="tag">&lt;/<span class="name">directory</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">testResource</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">testResources</span>&gt;</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>Super POM中定义了中央依赖仓库、中央插件仓库，以及各种文件夹的默认路径。</p>
<h1 id="POM继承"><a href="#POM继承" class="headerlink" title="POM继承"></a>POM继承</h1><p>如果你的项目有多个模块，通常每个模块之间会有一些相同的公共的依赖，你可以把依赖声明在模块各自的POM中，如下所示。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">|-- mod A</span><br><span class="line">|   `-- pom.xml</span><br><span class="line">|       `-- P(1.0)</span><br><span class="line">`-- mod B</span><br><span class="line">    `-- pom.xml</span><br><span class="line">        `-- P(1.0)</span><br></pre></td></tr></table></figure>
<p>这样A模块和B模块都依赖了P，但是如果有天你想要修改P的版本，又希望A,B模块依赖的P版本相同，那就得同时修改POM-A和POM-B，太不优雅了。</p>
<p>所以Maven提供了POM继承功能，让我们可以吧公共的依赖抽取出来。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">|-- pom.xml (父POM)</span><br><span class="line">|   `-- P(1.0)</span><br><span class="line">|-- mod A</span><br><span class="line">|   `-- pom.xml</span><br><span class="line">`-- mod B</span><br><span class="line">    `-- pom.xml</span><br></pre></td></tr></table></figure>

<p>做法如上，在父POM中声明依赖P，在AB模块的POM中声明对父POM的继承，便能实现AB模块对P依赖的引入。</p>
<p>AB模块中加入下面一段，便能实现对父POM的继承。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">parent</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.mycompany.app<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>my-app<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">relativePath</span>&gt;</span>../pom.xml<span class="tag">&lt;/<span class="name">relativePath</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>relativePath的默认值即是‘..&#x2F;pom.xml’，可以省略。</p>
<p>值得一提的是，这里父POM并不需要知道子POM的信息。</p>
<p><strong>总之，有了继承，子模块就能拥有父模块同样的依赖。</strong></p>
<h1 id="POM聚合"><a href="#POM聚合" class="headerlink" title="POM聚合"></a>POM聚合</h1><p>假设你的项目有多个模块，通常每个模块需要各自单独构建，如果想要所有模块能同时构建，则需要使用Maven的聚合功能。</p>
<p>使用聚合同样需要建一个父POM</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">|-- pom.xml (父POM)</span><br><span class="line">|-- A</span><br><span class="line">|   `-- pom.xml</span><br><span class="line">`-- B</span><br><span class="line">    `-- pom.xml</span><br></pre></td></tr></table></figure>
<p>父POM内容如下：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">project</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.mycompany.app<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>my-app<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">packaging</span>&gt;</span>pom<span class="tag">&lt;/<span class="name">packaging</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">  <span class="tag">&lt;<span class="name">modules</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">module</span>&gt;</span>A<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">module</span>&gt;</span>B<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">modules</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>其中packaging必须是pom，modules中声明需要聚合的子模块。<br>与继承相反，子POM中不需要父POM的信息。</p>
<p><strong>总之，有了聚合，所有对父模块执行的Maven命令，同样会对子模块执行。</strong></p>
<h1 id="依赖管理-dependencyManagement-是什么"><a href="#依赖管理-dependencyManagement-是什么" class="headerlink" title="依赖管理(dependencyManagement)是什么"></a>依赖管理(dependencyManagement)是什么</h1><p>在前面讲继承的时候，我们通过父POM使得AB两个模块都拥有了依赖P，但是如果我们现在添加一个模块C，模块C并不需要依赖P，只需要父POM中的其他配置和依赖，该怎么办呢？</p>
<p>这就要用到dependencyManagement这个配置了，dependencyManagement与dependencies元素不同在于并不会真的引入依赖，只是指定依赖的版本。</p>
<p>做法是在父POM中添加dependencyManagement配置</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencyManagement</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>groupabc<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>p<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">dependencyManagement</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>然后在AB模块的POM中添加如下，注意不含版本信息。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>groupabc<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>p<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span>  </span><br></pre></td></tr></table></figure>

<p>如此一来，AB模块都引入了依赖p的v1.0，而模块C没有引入依赖p。</p>
<p>另外还有pluginManagement与此类似，不过是针对插件而已。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://maven.apache.org/guides/introduction/introduction-to-the-pom.html">Introduction to the POM</a></p>
<p><a href="https://maven.apache.org/guides/introduction/introduction-to-dependency-mechanism.html#Dependency_Management">Introduction to the Dependency Mechanism</a></p>
<p><a href="https://book.douban.com/subject/5345682/">《Maven实战》</a></p>
<p>转载请保留原文地址：<a href="https://liam-blog.ml/2019/06/20/maven-details-about-dependency/">Maven用户都应该知道的一些事：关于依赖的常见问题</a></p>
]]></content>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title>Scala并发编程实战 1：Monitor与synchronized</title>
    <url>/2019/07/14/Scala-Concurrency-in-Practice-1/</url>
    <content><![CDATA[<p>Java并发编程最常用和易用的技术莫过于synchronized关键字，而Scala的并发编程之旅也可以从synchronized开始。而synchronized的背后其实是monitor技术。</p>
<span id="more"></span>

<h1 id="什么是Monitor"><a href="#什么是Monitor" class="headerlink" title="什么是Monitor"></a>什么是Monitor</h1><p>Monitor是解决并发编程问题的一种常用技术，可以有效解决互斥和同步两大常见问题，通常翻译为‘监视器’或‘管程’。个人认为‘管程‘更能表达monitor的含义，指的是管理共享变量以及对共享变量的操作过程，让他们支持并发。</p>
<h1 id="Scala的synchronized"><a href="#Scala的synchronized" class="headerlink" title="Scala的synchronized"></a>Scala的synchronized</h1><p>Synchronized是Java对monitor的实现，可以对代码块或方法使用，使得每次只能有一个线程访问，实现了线程互斥。当一个线程获取了锁，其他线程将在队列上等待，实现了线程同步。</p>
<p>Scala延用了这一关键字，但是语法有所不同。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//用于代码块</span></span><br><span class="line">obj.synchronized &#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//用于方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span></span>(): <span class="type">Unit</span> = <span class="keyword">this</span>.synchronized &#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>跟Java一样，这里的this是可以省略的，因为默认加锁的对象就是this，但是不建议省略。</p>
<h1 id="Scala实例"><a href="#Scala实例" class="headerlink" title="Scala实例"></a>Scala实例</h1><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.concurrent.<span class="type">TimeUnit</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SynchronizedDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> inc: <span class="type">Int</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">addOne</span></span>(): <span class="type">Unit</span> = <span class="keyword">this</span>.synchronized &#123;</span><br><span class="line">    <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>.sleep(<span class="number">1</span>)</span><br><span class="line">    inc += <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">10</span>) &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">Thread</span> &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">          println(<span class="string">s&quot;run thread with object method <span class="subst">$i</span>&quot;</span>)</span><br><span class="line">          addOne()</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;.start()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> instance = <span class="keyword">new</span> <span class="type">SynchronizedDemo</span></span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">10</span>) &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">Thread</span> &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">          println(<span class="string">s&quot;run thread with class method <span class="subst">$i</span>&quot;</span>)</span><br><span class="line">          instance.addOne()</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;.start()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">      println(<span class="string">s&quot;object inc=<span class="subst">$inc</span>, class inc=<span class="subst">$&#123;instance.inc&#125;</span>&quot;</span>)</span><br><span class="line">      <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>.sleep(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SynchronizedDemo</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> inc: <span class="type">Int</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">addOne</span></span>(): <span class="type">Unit</span> = <span class="keyword">this</span>.synchronized &#123;</span><br><span class="line">    <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>.sleep(<span class="number">1</span>)</span><br><span class="line">    inc += <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>程序输出</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">run thread with class method 7</span><br><span class="line">run thread with class method 4</span><br><span class="line">run thread with object method 8</span><br><span class="line">run thread with object method 7</span><br><span class="line">run thread with class method 10</span><br><span class="line">run thread with class method 8</span><br><span class="line">run thread with class method 9</span><br><span class="line">run thread with object method 5</span><br><span class="line">run thread with object method 3</span><br><span class="line">run thread with object method 2</span><br><span class="line">run thread with object method 4</span><br><span class="line">run thread with object method 10</span><br><span class="line">run thread with object method 9</span><br><span class="line">run thread with class method 5</span><br><span class="line">run thread with class method 3</span><br><span class="line">object inc=0, class inc=0</span><br><span class="line">run thread with object method 1</span><br><span class="line">run thread with class method 6</span><br><span class="line">run thread with class method 1</span><br><span class="line">run thread with class method 2</span><br><span class="line">run thread with object method 6</span><br><span class="line">object inc=1, class inc=1</span><br><span class="line">object inc=2, class inc=2</span><br><span class="line">object inc=3, class inc=2</span><br><span class="line">object inc=4, class inc=4</span><br><span class="line">object inc=5, class inc=5</span><br><span class="line">object inc=6, class inc=6</span><br><span class="line">object inc=7, class inc=7</span><br><span class="line">object inc=8, class inc=8</span><br><span class="line">object inc=9, class inc=9</span><br><span class="line">object inc=10, class inc=10</span><br></pre></td></tr></table></figure>

<p><strong>解析</strong></p>
<ul>
<li>在object SynchronizedDemo和class SynchronizedDemo中均定义了一个inc变量和一个addOne方法，addOne方法的作用就是将inc加1。</li>
<li>main方法中，分别创建10个线程调用addOne方法，对inc进行10次加1操作。</li>
<li>因为inc变量不是线程安全的，所以对addOne方法加上synchronized关键字，使得修改操作是线程安全的。这样才能保证inc会从1加到10。</li>
<li>object和class中的this并不相同，object中的this指向的是名为SynchronizedDemo的object对象，class中的则是该class实例化后的对象。(Scala中没有静态类和静态方法，object SynchronizedDemo实际上是创建名为SynchronizedDemo的单例对象)</li>
</ul>
<p>如果把class中定义的addOne改成如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addOne</span></span>(): <span class="type">Unit</span> = <span class="type">SynchronizedDemo</span>.synchronized &#123;</span><br><span class="line">  <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>.sleep(<span class="number">1</span>)</span><br><span class="line">  inc += <span class="number">1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>两处定义的addOne方法就会互斥，输出就会变成如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">run thread with object method 2</span><br><span class="line">run thread with object method 1</span><br><span class="line">run thread with object method 3</span><br><span class="line">run thread with object method 4</span><br><span class="line">run thread with object method 5</span><br><span class="line">run thread with object method 6</span><br><span class="line">run thread with object method 7</span><br><span class="line">run thread with object method 8</span><br><span class="line">run thread with object method 9</span><br><span class="line">run thread with object method 10</span><br><span class="line">run thread with class method 1</span><br><span class="line">run thread with class method 2</span><br><span class="line">run thread with class method 3</span><br><span class="line">run thread with class method 4</span><br><span class="line">run thread with class method 5</span><br><span class="line">run thread with class method 6</span><br><span class="line">run thread with class method 7</span><br><span class="line">run thread with class method 8</span><br><span class="line">run thread with class method 9</span><br><span class="line">run thread with class method 10</span><br><span class="line">object inc=0, class inc=0</span><br><span class="line">object inc=1, class inc=0</span><br><span class="line">object inc=1, class inc=1</span><br><span class="line">object inc=1, class inc=2</span><br><span class="line">object inc=1, class inc=3</span><br><span class="line">object inc=1, class inc=4</span><br><span class="line">object inc=1, class inc=5</span><br><span class="line">object inc=1, class inc=6</span><br><span class="line">object inc=1, class inc=7</span><br><span class="line">object inc=1, class inc=8</span><br><span class="line">object inc=1, class inc=9</span><br><span class="line">object inc=1, class inc=10</span><br><span class="line">object inc=2, class inc=10</span><br><span class="line">object inc=3, class inc=10</span><br><span class="line">object inc=4, class inc=10</span><br><span class="line">object inc=5, class inc=10</span><br><span class="line">object inc=6, class inc=10</span><br><span class="line">object inc=7, class inc=10</span><br><span class="line">object inc=8, class inc=10</span><br><span class="line">object inc=9, class inc=10</span><br><span class="line">object inc=10, class inc=10</span><br></pre></td></tr></table></figure>

<h1 id="本文代码"><a href="#本文代码" class="headerlink" title="本文代码"></a>本文代码</h1><p><a href="https://github.com/Liam8/learn-scala">Github仓库</a></p>
<p>转载请注明原文地址：<a href="https://liam-blog.ml/2019/07/14/Scala-Concurrency-in-Practice-1/">https://liam-blog.ml/2019/07/14/Scala-Concurrency-in-Practice-1/</a></p>
]]></content>
      <tags>
        <tag>Scala</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title>[读书笔记] ES权威指南</title>
    <url>/2018/03/26/es-guide-note/</url>
    <content><![CDATA[<h1 id="基础入门"><a href="#基础入门" class="headerlink" title="基础入门"></a>基础入门</h1><span id="more"></span>

<h3 id="轻量搜索"><a href="#轻量搜索" class="headerlink" title="轻量搜索"></a>轻量搜索</h3><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /megacorp/employee/_search?q=last_name<span class="punctuation">:</span>Smith</span><br></pre></td></tr></table></figure>

<h3 id="使用查询表达式搜索"><a href="#使用查询表达式搜索" class="headerlink" title="使用查询表达式搜索"></a>使用查询表达式搜索</h3><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /megacorp/employee/_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;query&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;bool&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;must&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;match&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;last_name&quot;</span> <span class="punctuation">:</span> <span class="string">&quot;smith&quot;</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;filter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;range&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;age&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;gt&quot;</span> <span class="punctuation">:</span> <span class="number">30</span> <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="全文搜索"><a href="#全文搜索" class="headerlink" title="全文搜索"></a>全文搜索</h3><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /megacorp/employee/_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;query&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;match&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;about&quot;</span> <span class="punctuation">:</span> <span class="string">&quot;rock climbing&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="短语搜索"><a href="#短语搜索" class="headerlink" title="短语搜索"></a>短语搜索</h3><p>精确匹配一系列单词或者短语</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /megacorp/employee/_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;query&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;match_phrase&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;about&quot;</span> <span class="punctuation">:</span> <span class="string">&quot;rock climbing&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="高亮搜索"><a href="#高亮搜索" class="headerlink" title="高亮搜索"></a>高亮搜索</h3><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /megacorp/employee/_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;query&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;match_phrase&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;about&quot;</span> <span class="punctuation">:</span> <span class="string">&quot;rock climbing&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;highlight&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;fields&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;about&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span><span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="聚合"><a href="#聚合" class="headerlink" title="聚合"></a>聚合</h3><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /megacorp/employee/_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;aggs&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;all_interests&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;terms&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;field&quot;</span> <span class="punctuation">:</span> <span class="string">&quot;interests&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;aggs&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;avg_age&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;avg&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;field&quot;</span> <span class="punctuation">:</span> <span class="string">&quot;age&quot;</span> <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="分布式特性"><a href="#分布式特性" class="headerlink" title="分布式特性"></a>分布式特性</h3><p>Elasticsearch 尽可能地屏蔽了分布式系统的复杂性。这里列举了一些在后台自动执行的操作：</p>
<p>分配文档到不同的容器 或 分片 中，文档可以储存在一个或多个节点中<br>按集群节点来均衡分配这些分片，从而对索引和搜索过程进行负载均衡<br>复制每个分片以支持数据冗余，从而防止硬件故障导致的数据丢失<br>将集群中任一节点的请求路由到存有相关数据的节点<br>集群扩容时无缝整合新节点，重新分配分片以便从离群节点恢复</p>
<h2 id="集群内的原理"><a href="#集群内的原理" class="headerlink" title="集群内的原理"></a>集群内的原理</h2><p>主分片的数目在索引创建时 就已经确定了下来。实际上，这个数目定义了这个索引能够 存储 的最大数据量。（实际大小取决于你的数据、硬件和使用场景。） 但是，读操作——搜索和返回数据——可以同时被主分片 或 副本分片所处理，所以当你拥有越多的副本分片时，也将拥有越高的吞吐量。</p>
<p>在运行中的集群上是可以动态调整副本分片数目的 ，我们可以按需伸缩集群。</p>
<p>当然，如果只是在相同节点数目的集群上增加更多的副本分片并不能提高性能，因为每个分片从节点上获得的资源会变少。 你需要增加更多的硬件资源来提升吞吐量。</p>
<p>但是更多的副本分片数提高了数据冗余量：按照上面的节点配置，我们可以在失去2个节点的情况下不丢失任何数据。</p>
<p>如果我们重新启动 Node 1 ，集群可以将缺失的副本分片再次进行分配，那么集群的状态也将如图 5 “将参数 number_of_replicas 调大到 2”所示。 如果 Node 1 依然拥有着之前的分片，它将尝试去重用它们，同时仅从主分片复制发生了修改的数据文件。</p>
<h2 id="数据输入和输出"><a href="#数据输入和输出" class="headerlink" title="数据输入和输出"></a>数据输入和输出</h2><p> 一个 键 可以是一个字段或字段的名称，一个 值 可以是一个字符串，一个数字，一个布尔值， 另一个对象，一些数组值，或一些其它特殊类型诸如表示日期的字符串，或代表一个地理位置的对象：</p>
<h3 id="文档元数据"><a href="#文档元数据" class="headerlink" title="文档元数据"></a>文档元数据</h3><p>一个文档不仅仅包含它的数据 ，也包含 元数据 —— 有关 文档的信息。 三个必须的元数据元素如下：</p>
<p>_index<br>文档在哪存放<br>_type<br>文档表示的对象类别<br>_id<br>文档唯一标识</p>
<p>_index<br>一个 索引 应该是因共同的特性被分组到一起的文档集合。 例如，你可能存储所有的产品在索引 products 中，而存储所有销售的交易到索引 sales 中。 虽然也允许存储不相关的数据到一个索引中，但这通常看作是一个反模式的做法。</p>
<p>提示<br>实际上，在 Elasticsearch 中，我们的数据是被存储和索引在 分片 中，而一个索引仅仅是逻辑上的命名空间， 这个命名空间由一个或者多个分片组合在一起。 然而，这是一个内部细节，我们的应用程序根本不应该关心分片，对于应用程序而言，只需知道文档位于一个 索引 内。 Elasticsearch 会处理所有的细节。</p>
<p>我们将在 索引管理 介绍如何自行创建和管理索引，但现在我们将让 Elasticsearch 帮我们创建索引。 所有需要我们做的就是选择一个索引名，这个名字必须小写，不能以下划线开头，不能包含逗号。我们用 website 作为索引名举例。</p>
<p>_type编辑<br>数据可能在索引中只是松散的组合在一起，但是通常明确定义一些数据中的子分区是很有用的。 例如，所有的产品都放在一个索引中，但是你有许多不同的产品类别，比如 “electronics” 、 “kitchen” 和 “lawn-care”。</p>
<p>这些文档共享一种相同的（或非常相似）的模式：他们有一个标题、描述、产品代码和价格。他们只是正好属于“产品”下的一些子类。</p>
<p>Elasticsearch 公开了一个称为 types （类型）的特性，它允许您在索引中对数据进行逻辑分区。不同 types 的文档可能有不同的字段，但最好能够非常相似。 我们将在 类型和映射 中更多的讨论关于 types 的一些应用和限制。</p>
<p>一个 _type 命名可以是大写或者小写，但是不能以下划线或者句号开头，不应该包含逗号， 并且长度限制为256个字符. 我们使用 blog 作为类型名举例。</p>
<p>_id编辑<br>ID 是一个字符串， 当它和 _index 以及 _type 组合就可以唯一确定 Elasticsearch 中的一个文档。 当你创建一个新的文档，要么提供自己的 _id ，要么让 Elasticsearch 帮你生成。</p>
<p>其他元数据编辑<br>还有一些其他的元数据元素，他们在 类型和映射 进行了介绍。通过前面已经列出的元数据元素， 我们已经能存储文档到 Elasticsearch 中并通过 ID 检索它–换句话说，使用 Elasticsearch 作为文档的存储介质。</p>
<h3 id="取回一个文档"><a href="#取回一个文档" class="headerlink" title="取回一个文档"></a>取回一个文档</h3><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /website/blog/<span class="number">123</span>?pretty #调用 Elasticsearch 的 pretty-print 功能，该功能 使得 JSON 响应体更加可读</span><br><span class="line">GET /website/blog/<span class="number">123</span>?_source=title<span class="punctuation">,</span>text #该 _source 字段现在包含的只是我们请求的那些字段</span><br><span class="line">GET /website/blog/<span class="number">123</span>/_source #只想得到 _source 字段，不需要任何元数据</span><br></pre></td></tr></table></figure>
<h3 id="检查文档是否存在"><a href="#检查文档是否存在" class="headerlink" title="检查文档是否存在"></a>检查文档是否存在</h3><p>curl -i -XHEAD <a href="http://localhost:9200/website/blog/123">http://localhost:9200/website/blog/123</a></p>
<h3 id="更新整个文档"><a href="#更新整个文档" class="headerlink" title="更新整个文档"></a>更新整个文档</h3><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">PUT /website/blog/<span class="number">123</span></span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;My first blog entry&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;I am starting to get the hang of this...&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;date&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;2014/01/02&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>ps:不存在时，就是创建</p>
<h3 id="仅创建文档"><a href="#仅创建文档" class="headerlink" title="仅创建文档"></a>仅创建文档</h3><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">PUT /website/blog/<span class="number">123</span>/_create</span><br><span class="line"><span class="punctuation">&#123;</span> ... <span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="删除文档"><a href="#删除文档" class="headerlink" title="删除文档"></a>删除文档</h3><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">DELETE /website/blog/<span class="number">123</span></span><br></pre></td></tr></table></figure>
<h3 id="冲突处理"><a href="#冲突处理" class="headerlink" title="冲突处理"></a>冲突处理</h3><p>悲观并发控制<br>这种方法被关系型数据库广泛使用，它假定有变更冲突可能发生，因此阻塞访问资源以防止冲突。 一个典型的例子是读取一行数据之前先将其锁住，确保只有放置锁的线程能够对这行数据进行修改。<br>乐观并发控制<br>Elasticsearch 中使用的这种方法假定冲突是不可能发生的，并且不会阻塞正在尝试的操作。 然而，如果源数据在读写当中被修改，更新将会失败。应用程序接下来将决定该如何解决冲突。 例如，可以重试更新、使用新的数据、或者将相关情况报告给用户。</p>
<h3 id="乐观并发控制"><a href="#乐观并发控制" class="headerlink" title="乐观并发控制"></a>乐观并发控制</h3><p>我们可以利用 _version 号来确保 应用中相互冲突的变更不会导致数据丢失。我们通过指定想要修改文档的 version 号来达到这个目的。 如果该版本不是当前版本号，我们的请求将会失败。</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">PUT /website/blog/<span class="number">1</span>?version=<span class="number">1</span></span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;My first blog entry&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;Starting to get the hang of this...&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>我们想这个在我们索引中的文档只有现在的 _version 为 1 时，本次更新才能成功。</p>
<p>通过外部系统使用版本控制</p>
<p>如果你的主数据库已经有了版本号 — 或一个能作为版本号的字段值比如 timestamp — 那么你就可以在 Elasticsearch 中通过增加 version_type&#x3D;external 到查询字符串的方式重用这些相同的版本号， 版本号必须是大于零的整数， 且小于 9.2E+18 — 一个 Java 中 long 类型的正值。</p>
<p>外部版本号的处理方式和我们之前讨论的内部版本号的处理方式有些不同， Elasticsearch 不是检查当前 _version 和请求中指定的版本号是否相同， 而是检查当前 _version 是否 小于 指定的版本号。 如果请求成功，外部的版本号作为文档的新 _version 进行存储。</p>
<p>文档是不可变的：他们不能被修改，只能被替换。</p>
<h3 id="部分更新"><a href="#部分更新" class="headerlink" title="部分更新"></a>部分更新</h3><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">POST /website/blog/<span class="number">1</span>/_update</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">   <span class="attr">&quot;doc&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;tags&quot;</span> <span class="punctuation">:</span> <span class="punctuation">[</span> <span class="string">&quot;testing&quot;</span> <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;views&quot;</span><span class="punctuation">:</span> <span class="number">0</span></span><br><span class="line">   <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>对象被合并到一起，覆盖现有的字段，增加新的字段。</p>
<h3 id="取回多个文档"><a href="#取回多个文档" class="headerlink" title="取回多个文档"></a>取回多个文档</h3><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /_mget</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">   <span class="attr">&quot;docs&quot;</span> <span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">      <span class="punctuation">&#123;</span></span><br><span class="line">         <span class="attr">&quot;_index&quot;</span> <span class="punctuation">:</span> <span class="string">&quot;website&quot;</span><span class="punctuation">,</span></span><br><span class="line">         <span class="attr">&quot;_type&quot;</span> <span class="punctuation">:</span>  <span class="string">&quot;blog&quot;</span><span class="punctuation">,</span></span><br><span class="line">         <span class="attr">&quot;_id&quot;</span> <span class="punctuation">:</span>    <span class="number">2</span></span><br><span class="line">      <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="punctuation">&#123;</span></span><br><span class="line">         <span class="attr">&quot;_index&quot;</span> <span class="punctuation">:</span> <span class="string">&quot;website&quot;</span><span class="punctuation">,</span></span><br><span class="line">         <span class="attr">&quot;_type&quot;</span> <span class="punctuation">:</span>  <span class="string">&quot;pageviews&quot;</span><span class="punctuation">,</span></span><br><span class="line">         <span class="attr">&quot;_id&quot;</span> <span class="punctuation">:</span>    <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">         <span class="attr">&quot;_source&quot;</span><span class="punctuation">:</span> <span class="string">&quot;views&quot;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">   <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line"></span><br><span class="line">GET /website/blog/_mget</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">   <span class="attr">&quot;ids&quot;</span> <span class="punctuation">:</span> <span class="punctuation">[</span> <span class="string">&quot;2&quot;</span><span class="punctuation">,</span> <span class="string">&quot;1&quot;</span> <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="代价较小的批量操作"><a href="#代价较小的批量操作" class="headerlink" title="代价较小的批量操作"></a>代价较小的批量操作</h3><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">POST /_bulk</span><br><span class="line"><span class="punctuation">&#123;</span> <span class="attr">&quot;delete&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;_index&quot;</span><span class="punctuation">:</span> <span class="string">&quot;website&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;blog&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;123&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span> <span class="attr">&quot;create&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;_index&quot;</span><span class="punctuation">:</span> <span class="string">&quot;website&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;blog&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;123&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span> <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span>    <span class="string">&quot;My first blog post&quot;</span> <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span> <span class="attr">&quot;index&quot;</span><span class="punctuation">:</span>  <span class="punctuation">&#123;</span> <span class="attr">&quot;_index&quot;</span><span class="punctuation">:</span> <span class="string">&quot;website&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;blog&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span> <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span>    <span class="string">&quot;My second blog post&quot;</span> <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span> <span class="attr">&quot;update&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;_index&quot;</span><span class="punctuation">:</span> <span class="string">&quot;website&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;blog&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;123&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;_retry_on_conflict&quot;</span> <span class="punctuation">:</span> <span class="number">3</span><span class="punctuation">&#125;</span> <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span> <span class="attr">&quot;doc&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span><span class="attr">&quot;title&quot;</span> <span class="punctuation">:</span> <span class="string">&quot;My updated blog post&quot;</span><span class="punctuation">&#125;</span> <span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<h2 id="分布式文档存储"><a href="#分布式文档存储" class="headerlink" title="分布式文档存储"></a>分布式文档存储</h2><p>分片选择公式</p>
<p>shard &#x3D; hash(routing) % number_of_primary_shards<br>routing 是一个可变值，默认是文档的 _id ，也可以设置成一个自定义的值。</p>
<p>相同分片的副本不会放在同一节点</p>
<p>我们可以发送请求到集群中的任一节点。 每个节点都有能力处理任意请求。 每个节点都知道集群中任一文档位置，所以可以直接将请求转发到需要的节点上。</p>
<p>新建、索引和删除 请求都是 写 操作， 必须在主分片上面完成之后才能被复制到相关的副本分片<br>在客户端收到成功响应时，文档变更已经在主分片和所有副本分片执行完成，变更是安全的。</p>
<p>有一些可选的请求参数允许您影响这个过程，可能以数据安全为代价提升性能。这些选项很少使用，因为Elasticsearch已经很快，但是为了完整起见，在这里阐述如下：<a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/distrib-write.html">https://www.elastic.co/guide/cn/elasticsearch/guide/current/distrib-write.html</a></p>
<p>在处理读取请求时，协调结点在每次请求的时候都会通过轮询所有的副本分片来达到负载均衡。</p>
<h2 id="搜索——最基本的工具"><a href="#搜索——最基本的工具" class="headerlink" title="搜索——最基本的工具"></a>搜索——最基本的工具</h2><p>映射（Mapping）<br>描述数据在每个字段内如何存储<br>分析（Analysis）<br>全文是如何处理使之可以被搜索的<br>领域特定查询语言（Query DSL）<br>Elasticsearch 中强大灵活的查询语言</p>
<h3 id="多索引，多类型"><a href="#多索引，多类型" class="headerlink" title="多索引，多类型"></a>多索引，多类型</h3><p>然而，经常的情况下，你 想在一个或多个特殊的索引并且在一个或者多个特殊的类型中进行搜索。我们可以通过在URL中指定特殊的索引和类型达到这种效果，如下所示：</p>
<p>&#x2F;_search<br>在所有的索引中搜索所有的类型<br>&#x2F;gb&#x2F;_search<br>在 gb 索引中搜索所有的类型<br>&#x2F;gb,us&#x2F;_search<br>在 gb 和 us 索引中搜索所有的文档<br>&#x2F;g*,u*&#x2F;_search<br>在任何以 g 或者 u 开头的索引中搜索所有的类型<br>&#x2F;gb&#x2F;user&#x2F;_search<br>在 gb 索引中搜索 user 类型<br>&#x2F;gb,us&#x2F;user,tweet&#x2F;_search<br>在 gb 和 us 索引中搜索 user 和 tweet 类型<br>&#x2F;_all&#x2F;user,tweet&#x2F;_search<br>在所有的索引中搜索 user 和 tweet 类型</p>
<h3 id="分页"><a href="#分页" class="headerlink" title="分页"></a>分页</h3><p>和 SQL 使用 LIMIT 关键字返回单个 page 结果的方法相同，Elasticsearch 接受 from 和 size 参数：</p>
<p>size<br>显示应该返回的结果数量，默认是 10<br>from<br>显示应该跳过的初始结果数量，默认是 0<br>如果每页展示 5 条结果，可以用下面方式请求得到 1 到 3 页的结果：</p>
<p>GET &#x2F;_search?size&#x3D;5<br>GET &#x2F;_search?size&#x3D;5&amp;from&#x3D;5<br>GET &#x2F;_search?size&#x3D;5&amp;from&#x3D;10</p>
<p><strong>在分布式系统中深度分页</strong><br>理解为什么深度分页是有问题的，我们可以假设在一个有 5 个主分片的索引中搜索。 当我们请求结果的第一页（结果从 1 到 10 ），每一个分片产生前 10 的结果，并且返回给 协调节点 ，协调节点对 50 个结果排序得到全部结果的前 10 个。</p>
<p>现在假设我们请求第 1000 页–结果从 10001 到 10010 。所有都以相同的方式工作除了每个分片不得不产生前10010个结果以外。 然后协调节点对全部 50050 个结果排序最后丢弃掉这些结果中的 50040 个结果。</p>
<p>可以看到，在分布式系统中，对结果排序的成本随分页的深度成指数上升。这就是 web 搜索引擎对任何查询都不要返回超过 1000 个结果的原因。</p>
<p>当索引一个文档的时候，Elasticsearch 取出所有字段的值拼接成一个大的字符串，作为 _all 字段进行索引。<br>除非设置特定字段，否则查询字符串就使用 _all 字段进行搜索。</p>
<p>在刚开始开发一个应用时，_all 字段是一个很实用的特性。之后，你会发现如果搜索时用指定字段来代替 _all 字段，将会更好控制搜索结果。当 _all 字段不再有用的时候，可以将它置为失效，正如在 元数据: _all 字段 中所解释的。</p>
<p>下面的查询针对tweents类型，并使用以下的条件：</p>
<p>name 字段中包含 mary 或者 john<br>date 值大于 2014-09-10<br><em>all</em> 字段包含 aggregations 或者 geo<br>+name:(mary john) +date:&gt;2014-09-10 +(aggregations geo)</p>
<p>不推荐直接向用户暴露查询字符串搜索功能，除非对于集群和数据来说非常信任他们。</p>
<p>相反，我们经常在生产环境中更多地使用功能全面的 request body 查询API，除了能完成以上所有功能，还有一些附加功能。</p>
<h2 id="映射和分析"><a href="#映射和分析" class="headerlink" title="映射和分析"></a>映射和分析</h2><p>Elasticsearch 中的数据可以概括的分为两类：精确值和全文。</p>
<p>你只能搜索在索引中出现的词条，所以索引文本和查询字符串必须标准化为相同的格式。</p>
<p>分词和标准化的过程称为 分析（analyze）</p>
<p>分析器 实际上是将三个功能封装到了一个包里：</p>
<p>字符过滤器<br>首先，字符串按顺序通过每个 字符过滤器 。他们的任务是在分词前整理字符串。一个字符过滤器可以用来去掉HTML，或者将 &amp; 转化成 <code>and</code>。<br>分词器<br>其次，字符串被 分词器 分为单个的词条。一个简单的分词器遇到空格和标点的时候，可能会将文本拆分成词条。<br>Token 过滤器<br>最后，词条按顺序通过每个 token 过滤器 。这个过程可能会改变词条（例如，小写化 Quick ），删除词条（例如， 像 a<code>， </code>and<code>， </code>the 等无用词），或者增加词条（例如，像 jump 和 leap 这种同义词）。</p>
<p>当你查询一个 全文 域时， 会对查询字符串应用相同的分析器，以产生正确的搜索词条列表。<br>当你查询一个 精确值 域时，不会分析查询字符串， 而是搜索你指定的精确值。</p>
<p>测试分析器，测试分词结果</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /_analyze</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;analyzer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;standard&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Text to analyze&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>当Elasticsearch在你的文档中检测到一个新的字符串域 ，它会自动设置其为一个全文 字符串 域，使用 标准 分析器对它进行分析。</p>
<p>Elasticsearch 支持 如下简单域类型：</p>
<p>字符串: string<br>整数 : byte, short, integer, long<br>浮点数: float, double<br>布尔型: boolean<br>日期: date</p>
<p>当你索引一个包含新域的文档–之前未曾出现– Elasticsearch 会使用 动态映射 ，通过JSON中基本数据类型，尝试猜测域类型，</p>
<p>查看映射<br>GET &#x2F;gb&#x2F;_mapping&#x2F;tweet</p>
<p>域最重要的属性是 type 。对于不是 string 的域，你一般只需要设置 type ：</p>
<p>默认， string 类型域会被认为包含全文。就是说，它们的值在索引前，会通过 一个分析器，针对于这个域的查询在搜索前也会经过一个分析器。</p>
<p>string 域映射的两个最重要 属性是 index 和 analyzer 。</p>
<p>index 属性控制怎样索引字符串。它可以是下面三个值：</p>
<p>analyzed<br>首先分析字符串，然后索引它。换句话说，以全文索引这个域。<br>not_analyzed<br>  索引这个域，所以它能够被搜索，但索引的是精确值。不会对它进行分析。<br>no<br>不索引这个域。这个域不会被搜索到。</p>
<p>analyzer<br>对于 analyzed 字符串域，用 analyzer 属性指定在搜索和索引时使用的分析器。默认， Elasticsearch 使用 standard 分析器， 但你可以指定一个内置的分析器替代它，例如 whitespace 、 simple 和 <code>english</code>：</p>
<p>当你首次 创建一个索引的时候，可以指定类型的映射。你也可以使用 &#x2F;_mapping 为新类型（或者为存在的类型更新映射）增加映射。<br>不能 _修改 存在的域映射。</p>
<p>创建索引的时候指定映射</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">PUT /gb</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;mappings&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;tweet&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;properties&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;tweet&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;type&quot;</span> <span class="punctuation">:</span>    <span class="string">&quot;string&quot;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="attr">&quot;analyzer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;english&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;date&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;type&quot;</span> <span class="punctuation">:</span>   <span class="string">&quot;date&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;name&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;type&quot;</span> <span class="punctuation">:</span>   <span class="string">&quot;string&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;user_id&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;type&quot;</span> <span class="punctuation">:</span>   <span class="string">&quot;long&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>测试映射结果</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /gb/_analyze</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;field&quot;</span><span class="punctuation">:</span> <span class="string">&quot;tweet&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Black-cats&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>ps:仅支持fulltext字段</p>
<h3 id="复杂核心域类型"><a href="#复杂核心域类型" class="headerlink" title="复杂核心域类型"></a>复杂核心域类型</h3><p>事实上， type 映射只是一种特殊的 对象 映射，我们称之为 根对象 。除了它有一些文档元数据的特殊顶级域，例如 _source 和 _all 域，它和其他对象一样。</p>
<p>Lucene 不理解内部对象。 Lucene 文档是由一组键值对列表组成的。为了能让 Elasticsearch 有效地索引内部类，它把我们的文档转化成这样：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;tweet&quot;</span><span class="punctuation">:</span>            <span class="punctuation">[</span>elasticsearch<span class="punctuation">,</span> flexible<span class="punctuation">,</span> very<span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;user.id&quot;</span><span class="punctuation">:</span>          <span class="punctuation">[</span>@johnsmith<span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;user.gender&quot;</span><span class="punctuation">:</span>      <span class="punctuation">[</span>male<span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;user.age&quot;</span><span class="punctuation">:</span>         <span class="punctuation">[</span><span class="number">26</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;user.name.full&quot;</span><span class="punctuation">:</span>   <span class="punctuation">[</span>john<span class="punctuation">,</span> smith<span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;user.name.first&quot;</span><span class="punctuation">:</span>  <span class="punctuation">[</span>john<span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;user.name.last&quot;</span><span class="punctuation">:</span>   <span class="punctuation">[</span>smith<span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>内部域 可以通过名称引用（例如， first ）。为了区分同名的两个域，我们可以使用全 路径 （例如， user.name.first ） 或 type 名加路径（ tweet.user.name.first ）。</p>
<p>内部对象数组<br>最后，考虑包含 内部对象的数组是如何被索引的。 假设我们有个 followers 数组：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;followers&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span> <span class="attr">&quot;age&quot;</span><span class="punctuation">:</span> <span class="number">35</span><span class="punctuation">,</span> <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Mary White&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#123;</span> <span class="attr">&quot;age&quot;</span><span class="punctuation">:</span> <span class="number">26</span><span class="punctuation">,</span> <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Alex Jones&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#123;</span> <span class="attr">&quot;age&quot;</span><span class="punctuation">:</span> <span class="number">19</span><span class="punctuation">,</span> <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Lisa Smith&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>这个文档会像我们之前描述的那样被扁平化处理，结果如下所示：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;followers.age&quot;</span><span class="punctuation">:</span>    <span class="punctuation">[</span><span class="number">19</span><span class="punctuation">,</span> <span class="number">26</span><span class="punctuation">,</span> <span class="number">35</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;followers.name&quot;</span><span class="punctuation">:</span>   <span class="punctuation">[</span>alex<span class="punctuation">,</span> jones<span class="punctuation">,</span> lisa<span class="punctuation">,</span> smith<span class="punctuation">,</span> mary<span class="punctuation">,</span> white<span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>{age: 35} 和 {name: Mary White} 之间的相关性已经丢失了，因为每个多值域只是一包无序的值，而不是有序数组。这足以让我们问，“有一个26岁的追随者？”</p>
<p>但是我们不能得到一个准确的答案：“是否有一个26岁 名字叫 Alex Jones 的追随者？”</p>
<p>相关内部对象被称为 nested 对象，可以回答上面的查询，我们稍后会在嵌套对象中介绍它。</p>
<h2 id="请求体查询"><a href="#请求体查询" class="headerlink" title="请求体查询"></a>请求体查询</h2><p>对于一个查询请求，Elasticsearch 的工程师偏向于使用 GET 方式，因为他们觉得它比 POST 能更好的描述信息检索（retrieving information）的行为。然而，因为带请求体的 GET 请求并不被广泛支持，所以 search API 同时支持 POST 请求：</p>
<p>过滤情况（filtering context）和查询情况（query context）。<br>当使用于 过滤情况 时，查询被设置成一个“不评分”或者“过滤”查询。<br>当使用于 查询情况 时，查询就变成了一个“评分”的查询.<br>过滤查询（Filtering queries）只是简单的检查包含或者排除，这就使得计算起来非常快。<br>通常的规则是，使用 查询（query）语句来进行 全文 搜索或者其它任何需要影响 相关性得分 的搜索。除此以外的情况都使用过滤（filters)。</p>
<p>查询表达式(Query DSL)是一种非常灵活又富有表现力的 查询语言。</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> YOUR_QUERY_HERE</span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line"></span><br><span class="line">一个查询语句 的典型结构：</span><br><span class="line"></span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    QUERY_NAME<span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        ARGUMENT<span class="punctuation">:</span> VALUE<span class="punctuation">,</span></span><br><span class="line">        ARGUMENT<span class="punctuation">:</span> VALUE<span class="punctuation">,</span>...</span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line">如果是针对某个字段，那么它的结构如下：</span><br><span class="line"></span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    QUERY_NAME<span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        FIELD_NAME<span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            ARGUMENT<span class="punctuation">:</span> VALUE<span class="punctuation">,</span></span><br><span class="line">            ARGUMENT<span class="punctuation">:</span> VALUE<span class="punctuation">,</span>...</span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>无论你在任何字段上进行的是全文搜索还是精确查询，match 查询是你可用的标准查询。<br>如果你在一个全文字段上使用 match 查询，在执行查询前，它将用正确的分析器去分析查询字符串：<br>{ “match”: { “tweet”: “About Search” }}<br>如果在一个精确值的字段上使用它， 例如数字、日期、布尔或者一个 not_analyzed 字符串字段，那么它将会精确匹配给定的值：<br>{ “match”: { “age”:    26           }}</p>
<p>multi_match 查询可以在多个字段上执行相同的 match 查询：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;multi_match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span>    <span class="string">&quot;full text search&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;fields&quot;</span><span class="punctuation">:</span>   <span class="punctuation">[</span> <span class="string">&quot;title&quot;</span><span class="punctuation">,</span> <span class="string">&quot;body&quot;</span> <span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>range 查询找出那些落在指定区间内的数字或者时间：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;range&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;age&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;gte&quot;</span><span class="punctuation">:</span>  <span class="number">20</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;lt&quot;</span><span class="punctuation">:</span>   <span class="number">30</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>term 查询对于输入的文本不 分析 ，所以它将给定的值进行精确查询。</p>
<p>terms 查询和 term 查询一样，但它允许你指定多值进行匹配。如果这个字段包含了指定值中的任何一个值，那么这个文档满足条件：</p>
<p>{ “terms”: { “tag”: [ “search”, “full_text”, “nosql” ] }}</p>
<p>exists 查询和 missing 查询被用于查找那些指定字段中有值 (exists) 或无值 (missing) 的文档。这与SQL中的 IS_NULL (missing) 和 NOT IS_NULL (exists) 在本质上具有共性：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;exists&quot;</span><span class="punctuation">:</span>   <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;field&quot;</span><span class="punctuation">:</span>    <span class="string">&quot;title&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>你可以用 bool 查询来实现你的需求。这种查询将多查询组合在一起，成为用户自己想要的布尔查询。它接收以下参数：</p>
<p>must<br>文档 必须 匹配这些条件才能被包含进来。<br>must_not<br>文档 必须不 匹配这些条件才能被包含进来。<br>should<br>如果满足这些语句中的任意语句，将增加 _score ，否则，无任何影响。它们主要用于修正每个文档的相关性得分。<br>filter<br>必须 匹配，但它以不评分、过滤模式来进行。这些语句对评分没有贡献，只是根据过滤标准来排除或包含文档。<br>由于这是我们看到的第一个包含多个查询的查询，所以有必要讨论一下相关性得分是如何组合的。每一个子查询都独自地计算文档的相关性得分。一旦他们的得分被计算出来， bool 查询就将这些得分进行合并并且返回一个代表整个布尔操作的得分。</p>
<p>下面的查询用于查找 title 字段匹配 how to make millions 并且不被标识为 spam 的文档。那些被标识为 starred 或在2014之后的文档，将比另外那些文档拥有更高的排名。如果 <em>两者</em> 都满足，那么它排名将更高：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;bool&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;must&quot;</span><span class="punctuation">:</span>     <span class="punctuation">&#123;</span> <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;how to make millions&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;must_not&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;tag&quot;</span><span class="punctuation">:</span>   <span class="string">&quot;spam&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;should&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">            <span class="punctuation">&#123;</span> <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;tag&quot;</span><span class="punctuation">:</span> <span class="string">&quot;starred&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span> <span class="attr">&quot;range&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;date&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;gte&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2014-01-01&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>如果没有 must 语句，那么至少需要能够匹配其中的一条 should 语句。但，如果存在至少一条 must 语句，则对 should 语句的匹配没有要求。</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;bool&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;must&quot;</span><span class="punctuation">:</span>     <span class="punctuation">&#123;</span> <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;how to make millions&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;must_not&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;tag&quot;</span><span class="punctuation">:</span>   <span class="string">&quot;spam&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;should&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">            <span class="punctuation">&#123;</span> <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;tag&quot;</span><span class="punctuation">:</span> <span class="string">&quot;starred&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;filter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;range&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;date&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;gte&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2014-01-01&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>过将 range 查询移到 filter 语句中，我们将它转成不评分的查询，将不再影响文档的相关性排名。由于它现在是一个不评分的查询，可以使用各种对 filter 查询有效的优化手段来提升性能。</p>
<p>对于合法查询，使用 explain 参数将返回可读的描述，这对准确理解 Elasticsearch 是如何解析你的 query 是非常有用的：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /_validate/query?explain</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">   <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;match&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">         <span class="attr">&quot;tweet&quot;</span> <span class="punctuation">:</span> <span class="string">&quot;really powerful&quot;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">   <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<h2 id="排序与相关性"><a href="#排序与相关性" class="headerlink" title="排序与相关性"></a>排序与相关性</h2><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;query&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;bool&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;must&quot;</span><span class="punctuation">:</span>   <span class="punctuation">&#123;</span> <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;tweet&quot;</span><span class="punctuation">:</span> <span class="string">&quot;manage text search&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;filter&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;term&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;user_id&quot;</span> <span class="punctuation">:</span> <span class="number">2</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;sort&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span> <span class="attr">&quot;date&quot;</span><span class="punctuation">:</span>   <span class="punctuation">&#123;</span> <span class="attr">&quot;order&quot;</span><span class="punctuation">:</span> <span class="string">&quot;desc&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#123;</span> <span class="attr">&quot;_score&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;order&quot;</span><span class="punctuation">:</span> <span class="string">&quot;desc&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>对于数字或日期，你可以将多值字段减为单值，这可以通过使用 min 、 max 、 avg 或是 sum 排序模式 。 例如你可以按照每个 date 字段中的最早日期进行排序，通过以下方法：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="attr">&quot;sort&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;dates&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;order&quot;</span><span class="punctuation">:</span> <span class="string">&quot;asc&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;mode&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;min&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>所有的 _core_field 类型 (strings, numbers, Booleans, dates) 接收一个 fields 参数</p>
<p>该参数允许你转化一个简单的映射如：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="attr">&quot;tweet&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span>     <span class="string">&quot;string&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;analyzer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;english&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>为一个多字段映射如：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="attr">&quot;tweet&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span>     <span class="string">&quot;string&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;analyzer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;english&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;fields&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;raw&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;string&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;index&quot;</span><span class="punctuation">:</span> <span class="string">&quot;not_analyzed&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>现在，至少只要我们重新索引了我们的数据，使用 tweet 字段用于搜索，tweet.raw 字段用于排序：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;tweet&quot;</span><span class="punctuation">:</span> <span class="string">&quot;elasticsearch&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;sort&quot;</span><span class="punctuation">:</span> <span class="string">&quot;tweet.raw&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>以全文 analyzed 字段排序会消耗大量的内存</p>
<h3 id="相关性"><a href="#相关性" class="headerlink" title="相关性"></a>相关性</h3><p>查询语句会为每个文档生成一个 _score 字段。评分的计算方式取决于查询类型 不同的查询语句用于不同的目的： fuzzy 查询会计算与关键词的拼写相似程度，terms 查询会计算 找到的内容与关键词组成部分匹配的百分比，但是通常我们说的 relevance 是我们用来计算全文本字段的值相对于全文本检索词相似程度的算法。</p>
<p>Elasticsearch 的相似度算法 被定义为检索词频率&#x2F;反向文档频率， TF&#x2F;IDF.</p>
<p>解释为什么匹配或不匹配</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /us/tweet/<span class="number">12</span>/_explain</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">   <span class="attr">&quot;query&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;bool&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">         <span class="attr">&quot;filter&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;term&quot;</span> <span class="punctuation">:</span>  <span class="punctuation">&#123;</span> <span class="attr">&quot;user_id&quot;</span> <span class="punctuation">:</span> <span class="number">2</span>           <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">         <span class="attr">&quot;must&quot;</span> <span class="punctuation">:</span>  <span class="punctuation">&#123;</span> <span class="attr">&quot;match&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;tweet&quot;</span> <span class="punctuation">:</span>   <span class="string">&quot;honeymoon&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">   <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>在 Elasticsearch 中，doc values 就是一种列式存储结构，默认情况下每个字段的 doc values 都是激活的，doc values 是在索引时创建的，当字段索引时，Elasticsearch 为了能够快速检索，会把字段的值加入倒排索引中，同时它也会存储该字段的 doc values。</p>
<h2 id="执行分布式检索"><a href="#执行分布式检索" class="headerlink" title="执行分布式检索"></a>执行分布式检索</h2><p>搜索被执行成一个两阶段过程，我们称之为 query then fetch<br>ps:查询阶段+取回阶段</p>
<p>查询阶段<br>查询阶段包含以下三个步骤:</p>
<p>1.客户端发送一个 search 请求到 Node 3 ， Node 3 会创建一个大小为 from + size 的空优先队列。<br>2.Node 3 将查询请求转发到索引的每个主分片或副本分片中。每个分片在本地执行查询并添加结果到大小为 from + size 的本地有序优先队列中。<br>3.每个分片返回各自优先队列中所有文档的 ID 和排序值给协调节点，也就是 Node 3 ，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。</p>
<p>每个分片在本地执行查询请求并且创建一个长度为 from + size 的优先队列—也就是说，每个分片创建的结果集足够大，均可以满足全局的搜索请求。 分片返回一个轻量级的结果列表到协调节点，它仅包含文档 ID 集合以及任何排序需要用到的值，例如 _score 。</p>
<p>协调节点将这些分片级的结果合并到自己的有序优先队列里，它代表了全局排序结果集合。至此查询过程结束。<br>ps:from 90 size 10 ，需要100长度的队列。</p>
<p>取回阶段<br>分布式阶段由以下步骤构成：</p>
<p>1.协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET 请求。<br>2.每个分片加载并 丰富 文档，如果有需要的话，接着返回文档给协调节点。<br>3.一旦所有的文档都被取回了，协调节点返回结果给客户端。</p>
<p>深分页（Deep Pagination）</p>
<p>给 10,000 到 50,000 的结果文档深分页（ 1,000 到 5,000 页）是完全可行的。但是使用足够大的 from 值，排序过程可能会变得非常沉重，使用大量的CPU、内存和带宽。因为这个原因，我们强烈建议你不要使用深分页。</p>
<p>实际上， “深分页” 很少符合人的行为。当2到3页过去以后，人会停止翻页，并且改变搜索标准。会不知疲倦地一页一页的获取网页直到你的服务崩溃的罪魁祸首一般是机器人或者web spider。</p>
<p>如果你 确实 需要从你的集群取回大量的文档，你可以通过用 scroll 查询禁用排序使这个取回行为更有效率，我们会在 later in this chapter 进行讨论。</p>
<p>超时问题<br>通常分片处理完它所有的数据后再把结果返回给协同节点，协同节点把收到的所有结果合并为最终结果。</p>
<p>这意味着花费的时间是最慢分片的处理时间加结果合并的时间。如果有一个节点有问题，就会导致所有的响应缓慢。</p>
<p>参数 timeout 告诉 分片允许处理数据的最大时间。如果没有足够的时间处理所有数据，这个分片的结果可以是部分的，甚至是空数据。</p>
<p>搜索的返回结果会用属性 timed_out 标明分片是否返回的是部分结果：</p>
<pre><code>...
&quot;timed_out&quot;:     true,
...
</code></pre>
<p>游标查询 Scroll<br>scroll 查询 可以用来对 Elasticsearch 有效地执行大批量的文档查询，而又不用付出深度分页那种代价。</p>
<p>游标查询允许我们 先做查询初始化，然后再批量地拉取结果。 这有点儿像传统数据库中的 cursor 。</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /old_index/_search?scroll=<span class="number">1</span>m</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;match_all&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span><span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;sort&quot;</span> <span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;_doc&quot;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;size&quot;</span><span class="punctuation">:</span>  <span class="number">1000</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>保持游标查询窗口一分钟。</p>
<p>关键字 _doc 是最有效的排序顺序。</p>
<p>这个查询的返回结果包括一个字段 _scroll_id<code>， 它是一个base64编码的长字符串 (((&quot;scroll_id&quot;))) 。 现在我们能传递字段 </code>_scroll_id 到 _search&#x2F;scroll 查询接口获取下一批结果：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /_search/scroll</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;scroll&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1m&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;scroll_id&quot;</span> <span class="punctuation">:</span> <span class="string">&quot;cXVlcnlUaGVuRmV0Y2g7NTsxMDk5NDpkUmpiR2FjOFNhNnlCM1ZDMWpWYnRROzEwOTk1OmRSamJHYWM4U2E2eUIzVkMxalZidFE7MTA5OTM6ZFJqYkdhYzhTYTZ5QjNWQzFqVmJ0UTsxMTE5MDpBVUtwN2lxc1FLZV8yRGVjWlI2QUVBOzEwOTk2OmRSamJHYWM4U2E2eUIzVkMxalZidFE7MDs=&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>注意再次设置游标查询过期时间为一分钟。</p>
<p>这个游标查询返回的下一批结果。 尽管我们指定字段 size 的值为1000，我们有可能取到超过这个值数量的文档。 当查询的时候， 字段 size 作用于单个分片，所以每个批次实际返回的文档数量最大为 size * number_of_primary_shards 。</p>
<h2 id="索引管理"><a href="#索引管理" class="headerlink" title="索引管理"></a>索引管理</h2><p>创建索引</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">PUT /my_index</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;settings&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> ... any settings ... <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;mappings&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;type_one&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> ... any mappings ... <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;type_two&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> ... any mappings ... <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>删除索引<br>用以下的请求来 删除索引:</p>
<p>DELETE &#x2F;my_index<br>你也可以这样删除多个索引：</p>
<p>DELETE &#x2F;index_one,index_two<br>DELETE &#x2F;index_*<br>你甚至可以这样删除 全部 索引：</p>
<p>DELETE &#x2F;_all<br>DELETE &#x2F;*</p>
<p>设置索引</p>
<p>下面是两个 最重要的设置：</p>
<p>number_of_shards<br>每个索引的主分片数，默认值是 5 。这个配置在索引创建后不能修改。<br>number_of_replicas<br>每个主分片的副本数，默认值是 1 。对于活动的索引库，这个配置可以随时修改。</p>
<p>可以用 update-index-settings API 动态修改副本数：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">PUT /my_temp_index/_settings</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;number_of_replicas&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>创建自定义分析器</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">PUT /my_index</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;settings&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;analysis&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;char_filter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> ... custom character filters ... <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;tokenizer&quot;</span><span class="punctuation">:</span>   <span class="punctuation">&#123;</span> ...    custom tokenizers     ... <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;filter&quot;</span><span class="punctuation">:</span>      <span class="punctuation">&#123;</span> ...   custom token filters   ... <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;analyzer&quot;</span><span class="punctuation">:</span>    <span class="punctuation">&#123;</span> ...    custom analyzers      ... <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>分析器应用在一个 string 字段上：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">PUT /my_index/_mapping/my_type</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;properties&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span>      <span class="string">&quot;string&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;analyzer&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;my_analyzer&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="类型和映射"><a href="#类型和映射" class="headerlink" title="类型和映射"></a>类型和映射</h3><p>如果有两个不同的类型，每个类型都有同名的字段，但映射不同（例如：一个是字符串一个是数字），将会出现什么情况？</p>
<p>简单回答是，Elasticsearch 不会允许你定义这个映射。当你配置这个映射时，将会出现异常。</p>
<p>Lucene 没有文档类型的概念，每个文档的类型名被存储在一个叫 _type 的元数据字段上。 当我们要检索某个类型的文档时, Elasticsearch 通过在 _type 字段上使用过滤器限制只返回这个类型的文档。</p>
<p>Lucene 也没有映射的概念。 映射是 Elasticsearch 将复杂 JSON 文档 映射 成 Lucene 需要的扁平化数据的方式。</p>
<p>重要的一点是: 类型可以很好的区分同一个集合中的不同细分。在不同的细分中数据的整体模式是相同的（或相似的）。</p>
<p>类型不适合 完全不同类型的数据 。如果两个类型的字段集是互不相同的，这就意味着索引中将有一半的数据是空的（字段将是 稀疏的 ），最终将导致性能问题。在这种情况下，最好是使用两个单独的索引。</p>
<h3 id="根对象"><a href="#根对象" class="headerlink" title="根对象"></a>根对象</h3><p>_source 字段在被写入磁盘之前先会被压缩。<br>这个字段的存储几乎总是我们想要的，因为它意味着下面的这些：</p>
<ul>
<li>搜索结果包括了整个可用的文档——不需要额外的从另一个的数据仓库来取文档。</li>
<li>如果没有 _source 字段，部分 update 请求不会生效。</li>
<li>当你的映射改变时，你需要重新索引你的数据，有了_source字段你可以直接从Elasticsearch这样做，而不必从另一个（通常是速度更慢的）数据仓库取回你的所有文档。</li>
<li>当你不需要看到整个文档时，单个字段可以从 _source 字段提取和通过 get 或者 search 请求返回。</li>
<li>调试查询语句更加简单，因为你可以直接看到每个文档包括什么，而不是从一列id猜测它们的内容。<br>然而，存储 _source 字段的确要使用磁盘空间。如果上面的原因对你来说没有一个是重要的，你可以用下面的映射禁用 _source 字段：<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">PUT /my_index</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;mappings&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;my_type&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;_source&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;enabled&quot;</span><span class="punctuation">:</span>  <span class="literal"><span class="keyword">false</span></span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
在一个搜索请求里，你可以通过在请求体中指定 _source 参数，来达到只获取特定的字段的效果：<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span>   <span class="punctuation">&#123;</span> <span class="attr">&quot;match_all&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span><span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;_source&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span> <span class="string">&quot;title&quot;</span><span class="punctuation">,</span> <span class="string">&quot;created&quot;</span> <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li>
</ul>
<p>_all 字段：<br>一个把其它字段值 当作一个大字符串来索引的特殊字段。 query_string 查询子句(搜索 ?q&#x3D;john )在没有指定字段时默认使用 _all 字段。</p>
<p>relevance algorithm 考虑的一个最重要的原则是字段的长度：字段越短越重要。 在较短的 title 字段中出现的短语可能比在较长的 content 字段中出现的短语更加重要。字段长度的区别在 _all 字段中不会出现。</p>
<p>如果你不再需要 _all 字段，你可以通过下面的映射来禁用：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">PUT /my_index/_mapping/my_type</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;my_type&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;_all&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;enabled&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span> <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>你可以为所有字段默认禁用 include_in_all 选项，仅在你选择的字段上启用：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">PUT /my_index/my_type/_mapping</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;my_type&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;include_in_all&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;properties&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span>           <span class="string">&quot;string&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;include_in_all&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            ...</span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>记住，_all 字段仅仅是一个 经过分词的 string 字段。它使用默认分词器来分析它的值，不管这个值原本所在字段指定的分词器。就像所有 string 字段，你可以配置 _all 字段使用的分词器：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">PUT /my_index/my_type/_mapping</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;my_type&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;_all&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;analyzer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;whitespace&quot;</span> <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>文档标识与四个元数据字段 相关：</p>
<ul>
<li>_id<br>文档的 ID 字符串</li>
<li>_type<br>文档的类型名</li>
<li>_index<br>文档所在的索引</li>
<li>_uid<br>_type 和 _id 连接在一起构造成 type#id</li>
</ul>
<p>默认情况下， _uid 字段是被存储（可取回）和索引（可搜索）的。 _type 字段被索引但是没有存储， _id 和 _index 字段则既没有被索引也没有被存储，这意味着它们并不是真实存在的。</p>
<h3 id="动态映射"><a href="#动态映射" class="headerlink" title="动态映射"></a>动态映射</h3><p>幸运的是可以用 dynamic 配置来控制这种行为 ，可接受的选项如下：</p>
<p>true<br>动态添加新的字段–缺省<br>false<br>忽略新的字段<br>strict<br>如果遇到新字段抛出异常</p>
<p>把 dynamic 设置为 false 一点儿也不会改变 _source 的字段内容。 _source 仍然包含被索引的整个JSON文档。只是新的字段不会被加到映射中也不可搜索。</p>
<p>日期检测可以通过在根对象上设置 date_detection 为 false 来关闭：</p>
<p>PUT &#x2F;my_index<br>{<br>    “mappings”: {<br>        “my_type”: {<br>            “date_detection”: false<br>        }<br>    }<br>}</p>
<p>使用 dynamic_templates ，你可以完全控制 新检测生成字段的映射。你甚至可以通过字段名称或数据类型来应用不同的映射。</p>
<p>一个索引中的所有类型共享相同的字段和设置。 <em>default</em> 映射更加方便地指定通用设置，而不是每次创建新类型时都要重复设置。 <em>default</em> 映射是新类型的模板。<br>我们可以使用 <em>default</em> 映射为所有的类型禁用 _all 字段， 而只在 blog 类型启用：</p>
<p>PUT &#x2F;my_index<br>{<br>    “mappings”: {<br>        “<em>default</em>“: {<br>            “_all”: { “enabled”:  false }<br>        },<br>        “blog”: {<br>            “_all”: { “enabled”:  true  }<br>        }<br>    }<br>}</p>
<p>重新索引数据<br>从旧索引复制到新索引</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;source&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;index&quot;</span><span class="punctuation">:</span> <span class="string">&quot;twitter&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;dest&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;index&quot;</span><span class="punctuation">:</span> <span class="string">&quot;new_twitter&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>ps:需要先准备好目标索引<br>ps:<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html</a></p>
<p>索引 别名 就像一个快捷方式或软连接，可以指向一个或多个索引，也可以给任何一个需要索引名的API来使用。别名 带给我们极大的灵活性，允许我们做下面这些：</p>
<p>在运行的集群中可以无缝的从一个索引切换到另一个索引<br>给多个索引分组 (例如， last_three_months)<br>给索引的一个子集创建 视图</p>
<p>首先，创建索引 my_index_v1 ，然后将别名 my_index 指向它：</p>
<p>PUT &#x2F;my_index_v1<br>PUT &#x2F;my_index_v1&#x2F;_alias&#x2F;my_index</p>
<p>你可以检测这个别名指向哪一个索引：</p>
<p>GET &#x2F;*&#x2F;_alias&#x2F;my_index<br>或哪些别名指向这个索引：</p>
<p>GET &#x2F;my_index_v1&#x2F;_alias&#x2F;*</p>
<p>在你的应用中使用别名而不是索引名。然后你就可以在任何时候重建索引。别名的开销很小，应该广泛使用。</p>
<h2 id="分片内部原理"><a href="#分片内部原理" class="headerlink" title="分片内部原理"></a>分片内部原理</h2><p>倒排索引被写入磁盘后是 不可改变 的:它永远不会修改。</p>
<p>文档更新也是类似的操作方式：当一个文档被更新时，旧版本文档被标记删除，文档的新版本被索引到一个新的段中。 可能两个版本的文档都会被一个查询匹配到，但被删除的那个旧版本文档在结果集返回前就已经被移除。</p>
<p>在 Elasticsearch 中，写入和打开一个新段的轻量的过程叫做 refresh 。 默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说 Elasticsearch 是 近 实时搜索: 文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。</p>
<p>不要在生产环境下每次索引一个文档都去手动刷新。 相反，你的应用需要意识到 Elasticsearch 的近实时的性质，并接受它的不足。</p>
<p>在生产环境中，当你正在建立一个大的新索引时，可以先关闭自动刷新，待开始使用该索引时，再把它们调回来：</p>
<p>PUT &#x2F;my_logs&#x2F;_settings<br>{ “refresh_interval”: -1 }</p>
<p>PUT &#x2F;my_logs&#x2F;_settings<br>{ “refresh_interval”: “1s” }</p>
<p>refresh_interval 需要一个 持续时间 值， 例如 1s （1 秒） 或 2m （2 分钟）。 一个绝对值 1 表示的是 1毫秒 –无疑会使你的集群陷入瘫痪。</p>
<p>这个执行一个提交并且截断 translog 的行为在 Elasticsearch 被称作一次 flush 。 分片每30分钟被自动刷新（flush），或者在 translog 太大的时候也会刷新。</p>
<p>在文件被 fsync 到磁盘前，被写入的文件在重启之后就会丢失。默认 translog 是每 5 秒被 fsync 刷新到硬盘， 或者在每次写请求完成之后执行(e.g. index, delete, update, bulk)。</p>
<p>段合并的时候会将那些旧的已删除文档 从文件系统中清除。 被删除的文档（或被更新文档的旧版本）不会被拷贝到新的大段中。</p>
<p>optimize API大可看做是 强制合并 API 。它会将一个分片强制合并到 max_num_segments 参数指定大小的段数目。 这样做的意图是减少段的数量（通常减少到一个），来提升搜索性能。</p>
<p>使用optimize优化老的索引，将每一个分片合并为一个单独的段就很有用了；这样既可以节省资源，也可以使搜索更加快速：</p>
<p>POST &#x2F;logstash-2014-10&#x2F;_optimize?max_num_segments&#x3D;1</p>
<p>请注意，使用 optimize API 触发段合并的操作一点也不会受到任何资源上的限制。这可能会消耗掉你节点上全部的I&#x2F;O资源, 使其没有余裕来处理搜索请求，从而有可能使集群失去响应。 如果你想要对索引执行 <code>optimize</code>，你需要先使用分片分配（查看 迁移旧索引）把索引移到一个安全的节点，再执行。</p>
<h1 id="深入搜索"><a href="#深入搜索" class="headerlink" title="深入搜索"></a>深入搜索</h1><h2 id="结构化搜索"><a href="#结构化搜索" class="headerlink" title="结构化搜索"></a>结构化搜索</h2><p>通常当查找一个精确值的时候，我们不希望对查询进行评分计算。只希望对文档进行包括或排除的计算，所以我们会使用 constant_score 查询以非评分模式来执行 term 查询并以一作为统一评分。</p>
<p>最终组合的结果是一个 constant_score 查询，它包含一个 term 查询：</p>
<p>GET &#x2F;my_store&#x2F;products&#x2F;_search<br>{<br>    “query” : {<br>        “constant_score” : {<br>            “filter” : {<br>                “term” : {<br>                    “price” : 20<br>                }<br>            }<br>        }<br>    }<br>}<br>从概念上记住非评分计算是首先执行的，这将有助于写出高效又快速的搜索请求。</p>
<p>一个 bool 过滤器由三部分组成：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">   <span class="attr">&quot;bool&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;must&quot;</span> <span class="punctuation">:</span>     <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;should&quot;</span> <span class="punctuation">:</span>   <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;must_not&quot;</span> <span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">   <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>must<br>所有的语句都 必须（must） 匹配，与 AND 等价。<br>must_not<br>所有的语句都 不能（must not） 匹配，与 NOT 等价。<br>should<br>至少有一个语句要匹配，与 OR 等价。</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /my_store/products/_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">   <span class="attr">&quot;query&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;filtered&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">         <span class="attr">&quot;filter&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;bool&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">              <span class="attr">&quot;should&quot;</span> <span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="punctuation">&#123;</span> <span class="attr">&quot;term&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span><span class="attr">&quot;productID&quot;</span> <span class="punctuation">:</span> <span class="string">&quot;KDKE-B-9947-#kL5&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span> <span class="attr">&quot;bool&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                  <span class="attr">&quot;must&quot;</span> <span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                    <span class="punctuation">&#123;</span> <span class="attr">&quot;term&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span><span class="attr">&quot;productID&quot;</span> <span class="punctuation">:</span> <span class="string">&quot;JODL-X-1937-#pV7&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="punctuation">&#123;</span> <span class="attr">&quot;term&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span><span class="attr">&quot;price&quot;</span> <span class="punctuation">:</span> <span class="number">30</span><span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line">                  <span class="punctuation">]</span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line">              <span class="punctuation">]</span></span><br><span class="line">           <span class="punctuation">&#125;</span></span><br><span class="line">         <span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">   <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p> term 和 terms 是 包含（contains） 操作，而非 等值（equals） （判断）。 如何理解这句话呢？</p>
<p>如果我们有一个 term（词项）过滤器 { “term” : { “tags” : “search” } } ，它会与以下两个文档 同时 匹配：</p>
<p>{ “tags” : [“search”] }<br>{ “tags” : [“search”, “open_source”] }</p>
<p>尽管第二个文档包含除 search 以外的其他词，它还是被匹配并作为结果返回。</p>
<p>如果一定期望得到我们前面说的那种行为（即整个字段完全相等），最好的方式是增加并索引另一个字段， 这个字段用以存储该字段包含词项的数量，同样以上面提到的两个文档为例，现在我们包括了一个维护标签数的新字段：</p>
<p>{ “tags” : [“search”], “tag_count” : 1 }<br>{ “tags” : [“search”, “open_source”], “tag_count” : 2 }</p>
<h3 id="范围查询"><a href="#范围查询" class="headerlink" title="范围查询"></a>范围查询</h3><p>range 支持数值、日期、字符串</p>
<p>“range” : {<br>    “timestamp” : {<br>        “gt” : “2014-01-01 00:00:00”,<br>        “lt” : “2014-01-07 00:00:00”<br>    }<br>}</p>
<p>日期计算<br>“range” : {<br>    “timestamp” : {<br>        “gt” : “now-1h”  # 过去一个小时<br>    }<br>}<br>“range” : {<br>    “timestamp” : {<br>        “gt” : “2014-01-01 00:00:00”,<br>        “lt” : “2014-01-01 00:00:00||+1M”  #加一个月<br>    }<br>}<br>时间格式参考文档</p>
<p>字符串范围可采用 字典顺序（lexicographically） 或字母顺序（alphabetically）。例如，下面这些字符串是采用字典序（lexicographically）排序的：</p>
<p>5, 50, 6, B, C, a, ab, abb, abc, b</p>
<p>“range” : {<br>    “title” : {<br>        “gte” : “a”,<br>        “lt” :  “b”<br>    }<br>}<br>字符串范围在过滤 低基数（low cardinality） 字段（即只有少量唯一词项）时可以正常工作，但是唯一词项越多，字符串范围的计算会越慢。</p>
<p>那么我们如何用 exists 或 missing 查询 name 字段呢？ name 字段并不真实存在于倒排索引中。</p>
<p>原因是当我们执行下面这个过滤的时候：</p>
<p>{<br>    “exists” : { “field” : “name” }<br>}<br>实际执行的是：</p>
<p>{<br>    “bool”: {<br>        “should”: [<br>            { “exists”: { “field”: “name.first” }},<br>            { “exists”: { “field”: “name.last” }}<br>        ]<br>    }<br>}<br>这也就意味着，如果 first 和 last 都是空，那么 name 这个命名空间才会被认为不存在。</p>
<p>Elasticsearch 会基于使用频次自动缓存查询。如果一个非评分查询在最近的 256 词查询中被使用过（次数取决于查询类型），那么这个查询就会作为缓存的候选。但是，并不是所有的片段都能保证缓存 bitset 。只有那些文档数量超过 10,000 （或超过总文档数量的 3% )才会缓存 bitset 。因为小的片段可以很快的进行搜索和合并，这里缓存的意义不大。</p>
<h2 id="全文搜索-1"><a href="#全文搜索-1" class="headerlink" title="全文搜索"></a>全文搜索</h2><h3 id="基于词项与基于全文"><a href="#基于词项与基于全文" class="headerlink" title="基于词项与基于全文"></a>基于词项与基于全文</h3><p>文本查询可以划分成两大家族：</p>
<ul>
<li><p>基于词项的查询<br>如 term 或 fuzzy 这样的底层查询不需要分析阶段，它们对单个词项进行操作。用 term 查询词项 Foo 只要在倒排索引中查找 准确词项 ，并且用 TF&#x2F;IDF 算法为每个包含该词项的文档计算相关度评分 _score 。<br>记住 term 查询只对倒排索引的词项精确匹配，这点很重要，它不会对词的多样性进行处理（如， foo 或 FOO ）。这里，无须考虑词项是如何存入索引的。如果是将 [“Foo”,”Bar”] 索引存入一个不分析的（ not_analyzed ）包含精确值的字段，或者将 Foo Bar 索引到一个带有 whitespace 空格分析器的字段，两者的结果都会是在倒排索引中有 Foo 和 Bar 这两个词。</p>
</li>
<li><p>基于全文的查询<br>像 match 或 query_string 这样的查询是高层查询，它们了解字段映射的信息：<br>如果查询 日期（date） 或 整数（integer）字段，它们会将查询字符串分别作为日期或整数对待。<br>如果查询一个（ not_analyzed ）未分析的精确值字符串字段， 它们会将整个查询字符串作为单个词项对待。<br>但如果要查询一个（ analyzed ）已分析的全文字段， 它们会先将查询字符串传递到一个合适的分析器，然后生成一个供查询的词项列表。<br>一旦组成了词项列表，这个查询会对每个词项逐一执行底层的查询，再将结果合并，然后为每个文档生成一个最终的相关度评分。</p>
</li>
</ul>
<p>Elasticsearch 执行上面这个 match 查询的步骤是：</p>
<p>1.检查字段类型 。</p>
<p>2.分析查询字符串 。</p>
<p>3.查找匹配文档 。</p>
<p>4.为每个文档评分 。</p>
<h3 id="多词查询"><a href="#多词查询" class="headerlink" title="多词查询"></a>多词查询</h3><p>因为 match 查询必须查找两个词（ [“brown”,”dog”] ），它在内部实际上先执行两次 term 查询，然后将两次查询的结果合并作为最终结果输出。</p>
<p>match 查询还可以接受 operator 操作符作为输入参数，默认情况下该操作符是 or 。我们可以将它修改成 and 让所有指定词项都必须匹配：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /my_index/my_type/_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span>    <span class="string">&quot;BROWN DOG!&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;operator&quot;</span><span class="punctuation">:</span> <span class="string">&quot;and&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>match 查询支持 minimum_should_match 最小匹配参数， 这让我们可以指定必须匹配的词项数用来表示一个文档是否相关。我们可以将其设置为某个具体数字，更常用的做法是将其设置为一个百分数，因为我们无法控制用户搜索时输入的单词数量：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /my_index/my_type/_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span>                <span class="string">&quot;quick brown dog&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;minimum_should_match&quot;</span><span class="punctuation">:</span> <span class="string">&quot;75%&quot;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="组合查询"><a href="#组合查询" class="headerlink" title="组合查询"></a>组合查询</h3><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /my_index/my_type/_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;bool&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;must&quot;</span><span class="punctuation">:</span>     <span class="punctuation">&#123;</span> <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;quick&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;must_not&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;lazy&quot;</span>  <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;should&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                  <span class="punctuation">&#123;</span> <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;brown&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                  <span class="punctuation">&#123;</span> <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;dog&quot;</span>   <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>拷贝为 CURL在 SENSE 中查看<br>以上的查询结果返回 title 字段包含词项 quick 但不包含 lazy 的任意文档。目前为止，这与 bool 过滤器的工作方式非常相似。</p>
<p>&#x3D;&#x3D;区别就在于两个 should 语句，也就是说：一个文档不必包含 brown 或 dog 这两个词项，但如果一旦包含，我们就认为它们 更相关&#x3D;&#x3D;</p>
<p>bool 查询会为每个文档计算相关度评分 _score ， 再将所有匹配的 must 和 should 语句的分数 _score 求和，最后除以 must 和 should 语句的总数。</p>
<p>must_not 语句不会影响评分； 它的作用只是将不相关的文档排除。<br>默认情况下，没有 should 语句是必须匹配的，只有一个例外：那就是当没有 must 语句的时候，至少有一个 should 语句必须匹配。</p>
<h3 id="如何使用布尔匹配"><a href="#如何使用布尔匹配" class="headerlink" title="如何使用布尔匹配"></a>如何使用布尔匹配</h3><p>以下两个查询是等价的：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;brown fox&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;bool&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;should&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">      <span class="punctuation">&#123;</span> <span class="attr">&quot;term&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;brown&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="punctuation">&#123;</span> <span class="attr">&quot;term&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;fox&quot;</span>   <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="查询语句提升权重"><a href="#查询语句提升权重" class="headerlink" title="查询语句提升权重"></a>查询语句提升权重</h3><p>我们可以通过指定 boost 来控制任何查询语句的相对的权重， boost 的默认值为 1 ，大于 1 会提升一个语句的相对权重。所以下面重写之前的查询：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;bool&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;must&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span>    <span class="string">&quot;full text search&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;operator&quot;</span><span class="punctuation">:</span> <span class="string">&quot;and&quot;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;should&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="punctuation">&#123;</span> <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Elasticsearch&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;boost&quot;</span><span class="punctuation">:</span> <span class="number">3</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span> <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Lucene&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;boost&quot;</span><span class="punctuation">:</span> <span class="number">2</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="控制分析"><a href="#控制分析" class="headerlink" title="控制分析"></a>控制分析</h3><p>分析器可以从三个层面进行定义：按字段（per-field）、按索引（per-index）或全局缺省（global default）。Elasticsearch 会按照以下顺序依次处理，直到它找到能够使用的分析器。</p>
<ul>
<li><p>索引时的顺序如下：<br>字段映射里定义的 analyzer ，否则<br>索引设置中名为 default 的分析器，默认为<br>standard 标准分析器</p>
</li>
<li><p>在搜索时，顺序有些许不同：<br>查询自己定义的 analyzer ，否则<br>字段映射里定义的 analyzer ，否则<br>索引设置中名为 default 的分析器，默认为<br>standard 标准分析器</p>
</li>
</ul>
<p>持一个可选的 search_analyzer 映射，它仅会应用于搜索时（ analyzer 还用于索引时）。还有一个等价的 default_search 映射，用以指定索引层的默认配置。</p>
<p>如果考虑到这些额外参数，一个搜索时的 完整 顺序会是下面这样：</p>
<p>查询自己定义的 analyzer ，否则<br>字段映射里定义的 search_analyzer ，否则<br>字段映射里定义的 analyzer ，否则<br>索引设置中名为 default_search 的分析器，默认为<br>索引设置中名为 default 的分析器，默认为<br>standard 标准分析器</p>
<p>&#x3D;&#x3D;最简单的途径就是在创建索引或者增加类型映射时，为每个全文字段设置分析器。这种方式尽管有点麻烦，但是它让我们可以清楚的看到每个字段每个分析器是如何设置的。可以在索引级别设置中，为绝大部分的字段设置你想指定的 default 默认分析器。然后在字段级别设置中，对某一两个字段配置需要指定的分析器。&#x3D;&#x3D;</p>
<h3 id="被破坏的相关度！"><a href="#被破坏的相关度！" class="headerlink" title="被破坏的相关度！"></a>被破坏的相关度！</h3><p>由于性能原因， Elasticsearch 不会计算索引内所有文档的 IDF 。 相反，每个分片会根据 该分片 内的所有文档计算一个本地 IDF 。</p>
<p>在实际应用中，这并不是一个问题，本地和全局的 IDF 的差异会随着索引里文档数的增多渐渐消失，在真实世界的数据量下，局部的 IDF 会被迅速均化，所以上述问题并不是相关度被破坏所导致的，而是由于数据太少。</p>
<p>在搜索请求后添加 ?search_type&#x3D;dfs_query_then_fetch ， dfs 是指 分布式频率搜索（Distributed Frequency Search） ， 它告诉 Elasticsearch ，先分别获得每个分片本地的 IDF ，然后根据结果再计算整个索引的全局 IDF 。<br><strong>不要在生产环境上使用 dfs_query_then_fetch</strong> 。完全没有必要。只要有足够的数据就能保证词频是均匀分布的。没有理由给每个查询额外加上 DFS 这步。</p>
<h2 id="多字段搜索"><a href="#多字段搜索" class="headerlink" title="多字段搜索"></a>多字段搜索</h2><h3 id="多字符串查询"><a href="#多字符串查询" class="headerlink" title="多字符串查询"></a>多字符串查询</h3><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;bool&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;should&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span> <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;War and Peace&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#123;</span> <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;author&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Leo Tolstoy&quot;</span>   <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#123;</span> <span class="attr">&quot;bool&quot;</span><span class="punctuation">:</span>  <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;should&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">            <span class="punctuation">&#123;</span> <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;translator&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Constance Garnett&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span> <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;translator&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Louise Maude&quot;</span>      <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line">          <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>答案在于评分的计算方式。 bool 查询运行每个 match 查询，再把评分加在一起，然后将结果与所有匹配的语句数量相乘，最后除以所有的语句数量。处于同一层的每条语句具有相同的权重。在前面这个例子中，包含 translator 语句的 bool 查询，只占总评分的三分之一。如果将 translator 语句与 title 和 author 两条语句放入同一层，那么 title 和 author 语句只贡献四分之一评分。</p>
<h3 id="单字符串查询"><a href="#单字符串查询" class="headerlink" title="单字符串查询"></a>单字符串查询</h3><h3 id="最佳字段"><a href="#最佳字段" class="headerlink" title="最佳字段"></a>最佳字段</h3><p><strong>dis_max 查询</strong><br>不使用 bool 查询，可以使用 dis_max 即分离 最大化查询（Disjunction Max Query） 。分离（Disjunction）的意思是 或（or） ，这与可以把结合（conjunction）理解成 与（and） 相对应。分离最大化查询（Disjunction Max Query）指的是： 将任何与任一查询匹配的文档作为结果返回，但&#x3D;&#x3D;只将最佳匹配的评分作为查询的评分结果返回&#x3D;&#x3D; ：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;dis_max&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;queries&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="punctuation">&#123;</span> <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Brown fox&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span> <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;body&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;Brown fox&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="最佳字段查询调优"><a href="#最佳字段查询调优" class="headerlink" title="最佳字段查询调优"></a>最佳字段查询调优</h3><p><strong>tie_breaker 参数</strong><br>可以通过指定 tie_breaker 这个参数将其他匹配语句的评分也考虑其中：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;dis_max&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;queries&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="punctuation">&#123;</span> <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Quick pets&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span> <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;body&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;Quick pets&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;tie_breaker&quot;</span><span class="punctuation">:</span> <span class="number">0.3</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>tie_breaker 参数提供了一种 dis_max 和 bool 之间的折中选择，它的评分方式如下：</p>
<ul>
<li>1.获得最佳匹配语句的评分 _score 。</li>
<li>2.将其他匹配语句的评分结果与 tie_breaker 相乘。</li>
<li>3.对以上评分求和并规范化。</li>
</ul>
<p>有了 tie_breaker ，会考虑所有匹配语句，但最佳匹配语句依然占最终结果里的很大一部分。</p>
<p>注意</p>
<blockquote>
<p>tie_breaker 可以是 0 到 1 之间的浮点数，其中 0 代表使用 dis_max 最佳匹配语句的普通逻辑， 1 表示所有匹配语句同等重要。最佳的精确值需要根据数据与查询调试得出，但是合理值应该与零接近（处于 0.1 - 0.4 之间），这样就不会颠覆 dis_max 最佳匹配性质的根本。</p>
</blockquote>
<h3 id="multi-match-查询"><a href="#multi-match-查询" class="headerlink" title="multi_match 查询"></a>multi_match 查询</h3><p>multi_match 查询为能在多个字段上反复执行相同查询提供了一种便捷方式。</p>
<blockquote>
<p>注意<br>multi_match 多匹配查询的类型有多种，其中的三种恰巧与 了解我们的数据 中介绍的三个场景对应，即： best_fields 、 most_fields 和 cross_fields （最佳字段、多数字段、跨字段）。</p>
</blockquote>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;multi_match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span>                <span class="string">&quot;Quick brown fox&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span>                 <span class="string">&quot;best_fields&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;fields&quot;</span><span class="punctuation">:</span>               <span class="punctuation">[</span> <span class="string">&quot;title&quot;</span><span class="punctuation">,</span> <span class="string">&quot;body&quot;</span> <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;tie_breaker&quot;</span><span class="punctuation">:</span>          <span class="number">0.3</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;minimum_should_match&quot;</span><span class="punctuation">:</span> <span class="string">&quot;30%&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h4 id="查询字段名称的模糊匹配"><a href="#查询字段名称的模糊匹配" class="headerlink" title="查询字段名称的模糊匹配"></a>查询字段名称的模糊匹配</h4><p>字段名称可以用模糊匹配的方式给出：任何与模糊模式正则匹配的字段都会被包括在搜索条件中， 例如可以使用以下方式同时匹配 book_title 、 chapter_title 和 section_title （书名、章名、节名）这三个字段：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;multi_match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;Quick brown fox&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;fields&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*_title&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<h4 id="提升单个字段的权重编辑"><a href="#提升单个字段的权重编辑" class="headerlink" title="提升单个字段的权重编辑"></a>提升单个字段的权重编辑</h4><p>可以使用 ^ 字符语法为单个字段提升权重，在字段名称的末尾添加 ^boost ， 其中 boost 是一个浮点数：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;multi_match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;Quick brown fox&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;fields&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span> <span class="string">&quot;*_title&quot;</span><span class="punctuation">,</span> <span class="string">&quot;chapter_title^2&quot;</span> <span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>chapter_title 这个字段的 boost 值为 2 ，而其他两个字段 book_title 和 section_title 字段的默认 boost 值为 1 。</p>
<h3 id="多数字段"><a href="#多数字段" class="headerlink" title="多数字段"></a>多数字段</h3><p>多字段映射编辑<br>首先要做的事情就是对我们的字段索引两次： 一次使用词干模式以及一次非词干模式。为了做到这点，采用 multifields 来实现，已经在 multifields 有所介绍：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">DELETE /my_index</span><br><span class="line"></span><br><span class="line">PUT /my_index</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;settings&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;number_of_shards&quot;</span><span class="punctuation">:</span> <span class="number">1</span> <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;mappings&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;my_type&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;properties&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span>     <span class="string">&quot;string&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;analyzer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;english&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;fields&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;std&quot;</span><span class="punctuation">:</span>   <span class="punctuation">&#123;</span></span><br><span class="line">                            <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span>     <span class="string">&quot;string&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="attr">&quot;analyzer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;standard&quot;</span></span><br><span class="line">                        <span class="punctuation">&#125;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="跨字段实体搜索"><a href="#跨字段实体搜索" class="headerlink" title="跨字段实体搜索"></a>跨字段实体搜索</h3><figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;multi_match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span>       <span class="string">&quot;Poland Street W1V&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span>        <span class="string">&quot;most_fields&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;fields&quot;</span><span class="punctuation">:</span>      <span class="punctuation">[</span> <span class="string">&quot;street&quot;</span><span class="punctuation">,</span> <span class="string">&quot;city&quot;</span><span class="punctuation">,</span> <span class="string">&quot;country&quot;</span><span class="punctuation">,</span> <span class="string">&quot;postcode&quot;</span> <span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>most_fields 方式的问题<br>用 most_fields 这种方式搜索也存在某些问题，这些问题并不会马上显现：</p>
<ul>
<li>它是为多数字段匹配 任意 词设计的，而不是在 所有字段 中找到最匹配的。</li>
<li>它不能使用 operator 或 minimum_should_match 参数来降低次相关结果造成的长尾效应。</li>
<li>词频对于每个字段是不一样的，而且它们之间的相互影响会导致不好的排序结果。</li>
</ul>
<h3 id="字段中心式查询"><a href="#字段中心式查询" class="headerlink" title="字段中心式查询"></a>字段中心式查询</h3><p><strong>解决方案</strong><br>存在这些问题仅仅是因为我们在处理着多个字段，如果将所有这些字段组合成单个字段，问题就会消失。可以为 person 文档添加 full_name 字段来解决这个问题：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;first_name&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;Peter&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;last_name&quot;</span><span class="punctuation">:</span>   <span class="string">&quot;Smith&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;full_name&quot;</span><span class="punctuation">:</span>   <span class="string">&quot;Peter Smith&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>当查询 full_name 字段时：</p>
<p>具有更多匹配词的文档会比只有一个重复匹配词的文档更重要。<br>minimum_should_match 和 operator 参数会像期望那样工作。<br>姓和名的逆向文档频率被合并，所以 Smith 到底是作为姓还是作为名出现，都会变得无关紧要。<br>这么做当然是可行的，但我们并不太喜欢存储冗余数据。取而代之的是 Elasticsearch 可以提供两个解决方案——一个在索引时，而另一个是在搜索时——随后会讨论它们。</p>
<h3 id="自定义-all-字段"><a href="#自定义-all-字段" class="headerlink" title="自定义 _all 字段"></a>自定义 _all 字段</h3><p>Elasticsearch 在字段映射中为我们提供 copy_to 参数来实现这个功能：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">PUT /my_index</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;mappings&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;person&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;properties&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;first_name&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span>     <span class="string">&quot;string&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;copy_to&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;full_name&quot;</span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;last_name&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span>     <span class="string">&quot;string&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;copy_to&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;full_name&quot;</span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;full_name&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span>     <span class="string">&quot;string&quot;</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>first_name 和 last_name 字段中的值会被复制到 full_name 字段。</p>
<h3 id="cross-fields-跨字段查询"><a href="#cross-fields-跨字段查询" class="headerlink" title="cross-fields 跨字段查询"></a>cross-fields 跨字段查询</h3><p>cross_fields 使用词中心式（term-centric）的查询方式，这与 best_fields 和 most_fields 使用字段中心式（field-centric）的查询方式非常不同，它将所有字段当成一个大字段，并在 每个字段 中查找 每个词 。</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /_validate/query?explain</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;multi_match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span>       <span class="string">&quot;peter smith&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span>        <span class="string">&quot;cross_fields&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;operator&quot;</span><span class="punctuation">:</span>    <span class="string">&quot;and&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;fields&quot;</span><span class="punctuation">:</span>      <span class="punctuation">[</span> <span class="string">&quot;first_name&quot;</span><span class="punctuation">,</span> <span class="string">&quot;last_name&quot;</span> <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>用 cross_fields 词中心式匹配。</p>
<p>它通过 混合 不同字段逆向索引文档频率的方式解决了词频的问题：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+blended(&quot;peter&quot;, fields: [first_name, last_name])</span><br><span class="line">+blended(&quot;smith&quot;, fields: [first_name, last_name])</span><br></pre></td></tr></table></figure>

<p>&#x3D;&#x3D;自定义单字段查询是否能够优于多字段查询，取决于在多字段查询与单字段自定义 _all 之间代价的权衡，即哪种解决方案会带来更大的性能优化就选择哪一种。&#x3D;&#x3D;</p>
<h3 id="Exact-Value-精确值字段"><a href="#Exact-Value-精确值字段" class="headerlink" title="Exact-Value 精确值字段"></a>Exact-Value 精确值字段</h3><p>在 multi_match 查询中避免使用 not_analyzed 字段。</p>
<h2 id="近似匹配"><a href="#近似匹配" class="headerlink" title="近似匹配"></a>近似匹配</h2><h3 id="短语匹配"><a href="#短语匹配" class="headerlink" title="短语匹配"></a>短语匹配</h3><p><strong>match_phrase 查询首先将查询字符串解析成一个词项列表，然后对这些词项进行搜索，但只保留那些包含 全部 搜索词项，且 位置 与搜索词项相同的文档。</strong></p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /my_index/my_type/_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;match_phrase&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;quick brown fox&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="混合起来"><a href="#混合起来" class="headerlink" title="混合起来"></a>混合起来</h3><p>slop 参数告诉 match_phrase 查询词条相隔多远时仍然能将文档视为匹配 。 相隔多远的意思是为了让查询和文档匹配你需要移动词条多少次？</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /my_index/my_type/_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;match_phrase&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="string">&quot;quick fox&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;slop&quot;</span><span class="punctuation">:</span>  <span class="number">1</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>为了使查询 <code>fox quick</code> 匹配我们的文档， 我们需要 <code>slop</code> 的值为 <code>3</code>:</p>
<pre><code>            Pos 1         Pos 2         Pos 3
-----------------------------------------------
Doc:        quick         brown         fox
-----------------------------------------------
Query:      fox           quick
Slop 1:     fox|quick  ↵  &lt;1&gt;
Slop 2:     quick      ↳  fox
Slop 3:     quick                 ↳     fox
</code></pre>
<p>&lt;1&gt; 注意 <code>fox</code> 和 <code>quick</code> 在这步中占据同样的位置。 因此将 <code>fox quick</code> 转换顺序成 <code>quick fox</code> 需要两步， 或者值为 <code>2</code> 的 <code>slop</code> 。</p>
<p>PS:是将查询词进行移动，不是移动文档中的词。</p>
<h3 id="多值字段"><a href="#多值字段" class="headerlink" title="多值字段"></a>多值字段</h3><p>数组相关，略。</p>
<h3 id="越近越好"><a href="#越近越好" class="headerlink" title="越近越好"></a>越近越好</h3><p>通过设置一个像 50 或者 100 这样的高 slop 值, 你能够排除单词距离太远的文档， 但是也给予了那些单词临近的的文档更高的分数。</p>
<h3 id="使用邻近度提高相关度"><a href="#使用邻近度提高相关度" class="headerlink" title="使用邻近度提高相关度"></a>使用邻近度提高相关度</h3><p>略</p>
<h3 id="性能优化"><a href="#性能优化" class="headerlink" title="性能优化"></a>性能优化</h3><p><strong>短语查询和邻近查询都比简单的 query 查询代价更高</strong> 。 一个 match 查询仅仅是看词条是否存在于倒排索引中，而一个 match_phrase 查询是必须计算并比较多个可能重复词项的位置。</p>
<p>Lucene nightly benchmarks 表明一个简单的 term 查询比一个短语查询大约快 10 倍，比邻近查询(有 slop 的短语 查询)大约快 20 倍。当然，这个代价指的是在搜索时而不是索引时。</p>
<p><strong>重新评分</strong>阶段支持一个代价更高的评分算法–比如 phrase 查询–只是为了从每个分片中获得前 K 个结果。 然后会根据它们的最新评分 重新排序。</p>
<p>该请求如下所示：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /my_index/my_type/_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span>                <span class="string">&quot;quick brown fox&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;minimum_should_match&quot;</span><span class="punctuation">:</span> <span class="string">&quot;30%&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;rescore&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;window_size&quot;</span><span class="punctuation">:</span> <span class="number">50</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;rescore_query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;match_phrase&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="string">&quot;quick brown fox&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;slop&quot;</span><span class="punctuation">:</span>  <span class="number">50</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>match 查询决定哪些文档将包含在最终结果集中，并通过 TF&#x2F;IDF 排序。<br>window_size 是每一分片进行重新评分的顶部文档数量。<br>目前唯一支持的重新打分算法就是另一个查询，但是以后会有计划增加更多的算法。</p>
<h3 id="寻找相关词"><a href="#寻找相关词" class="headerlink" title="寻找相关词"></a>寻找相关词</h3><p>要将每个单词 以及它的邻近词 作为单个词项索引：</p>
<p>[“sue ate”, “ate the”, “the alligator”]<br>这些单词对（或者 bigrams ）被称为 shingles 。<br>ps:索引三个单词（ trigrams ）</p>
<p>Trigrams 提供了更高的精度，但是也大大增加了索引中唯一词项的数量。在大多数情况下，Bigrams 就够了。</p>
<p>生成 Shingles</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">PUT /my_index</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;settings&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;number_of_shards&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;analysis&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;filter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;my_shingle_filter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span>             <span class="string">&quot;shingle&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;min_shingle_size&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;max_shingle_size&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;output_unigrams&quot;</span><span class="punctuation">:</span>  <span class="literal"><span class="keyword">false</span></span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;analyzer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;my_shingle_analyzer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span>             <span class="string">&quot;custom&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;tokenizer&quot;</span><span class="punctuation">:</span>        <span class="string">&quot;standard&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;filter&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                        <span class="string">&quot;lowercase&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="string">&quot;my_shingle_filter&quot;</span></span><br><span class="line">                    <span class="punctuation">]</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line">PUT /my_index/_mapping/my_type</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;my_type&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;properties&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;string&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;fields&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;shingles&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span>     <span class="string">&quot;string&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;analyzer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;my_shingle_analyzer&quot;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>现在在查询里添加 shingles 字段。不要忘了在 shingles 字段上的匹配是充当一 种信号–为了提高相关度评分–所以我们仍然需要将基本 title 字段包含到查询中：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /my_index/my_type/_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">   <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;bool&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">         <span class="attr">&quot;must&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">               <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;the hungry alligator ate sue&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">         <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">         <span class="attr">&quot;should&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">               <span class="attr">&quot;title.shingles&quot;</span><span class="punctuation">:</span> <span class="string">&quot;the hungry alligator ate sue&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">         <span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">   <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p><strong>shingles 不仅比短语查询更灵活， 而且性能也更好。</strong> shingles 查询跟一个简单的 match 查询一样高效，而不用每次搜索花费短语查询的代价。只是在索引期间因为更多词项需要被索引会付出一些小的代价， 这也意味着有 shingles 的字段会占用更多的磁盘空间。 然而，大多数应用写入一次而读取多次，所以在索引期间优化我们的查询速度是有意义的。</p>
<h2 id="部分匹配"><a href="#部分匹配" class="headerlink" title="部分匹配"></a>部分匹配</h2><h3 id="prefix-前缀查询"><a href="#prefix-前缀查询" class="headerlink" title="prefix 前缀查询"></a>prefix 前缀查询</h3><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /my_index/address/_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;prefix&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;postcode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;W1&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>prefix 查询或过滤对于一些特定的匹配是有效的，但使用方式还是应当注意。 当字段中词的集合很小时，可以放心使用，但是它的伸缩性并不好，会对我们的集群带来很多压力。可以使用较长的前缀来限制这种影响，减少需要访问的量。</p>
</blockquote>
<h3 id="通配符与正则表达式查询"><a href="#通配符与正则表达式查询" class="headerlink" title="通配符与正则表达式查询"></a>通配符与正则表达式查询</h3><p>wildcard 通配符查询: ? 匹配任意字符， * 匹配 0 或多个字符.</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /my_index/address/_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;wildcard&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;postcode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;W?F*HW&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line">GET /my_index/address/_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;regexp&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;postcode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;W[0-9].+&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>wildcard 和 regexp 查询的工作方式与 prefix 查询完全一样，它们也需要扫描倒排索引中的词列表才能找到所有匹配的词，然后依次获取每个词相关的文档 ID ，与 prefix 查询的唯一不同是：它们能支持更为复杂的匹配模式。</p>
<blockquote>
<p>prefix 、 wildcard 和 regexp 查询是基于词操作的，如果用它们来查询 analyzed 字段，它们会检查字段里面的每个词，而不是将字段作为整体来处理。</p>
</blockquote>
<h3 id="查询时输入即搜索"><a href="#查询时输入即搜索" class="headerlink" title="查询时输入即搜索"></a>查询时输入即搜索</h3><figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;match_phrase_prefix&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;brand&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span>          <span class="string">&quot;johnnie walker bl&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;max_expansions&quot;</span><span class="punctuation">:</span> <span class="number">50</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;slop&quot;</span><span class="punctuation">:</span>  <span class="number">10</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>与 match_phrase 一样，它也可以接受 slop 参数（参照 slop ）让相对词序位置不那么严格：<br>参数 max_expansions 控制着可以与前缀匹配的词的数量，它会先查找第一个与前缀 bl 匹配的词，然后依次查找搜集与之匹配的词（按字母顺序），直到没有更多可匹配的词或当数量超过 max_expansions 时结束。</p>
<h3 id="索引时输入即搜索"><a href="#索引时输入即搜索" class="headerlink" title="索引时输入即搜索"></a>索引时输入即搜索</h3><p>自定义分析器 autocomplete</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">PUT /my_index</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;settings&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;number_of_shards&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;analysis&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;filter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;autocomplete_filter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span>     <span class="string">&quot;edge_ngram&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;min_gram&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;max_gram&quot;</span><span class="punctuation">:</span> <span class="number">20</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;analyzer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;autocomplete&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span>      <span class="string">&quot;custom&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;tokenizer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;standard&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;filter&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                        <span class="string">&quot;lowercase&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="string">&quot;autocomplete_filter&quot;</span></span><br><span class="line">                    <span class="punctuation">]</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>应用在字段</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">PUT /my_index/my_type/_mapping</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;my_type&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;properties&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span>            <span class="string">&quot;string&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;index_analyzer&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;autocomplete&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;search_analyzer&quot;</span><span class="punctuation">:</span> <span class="string">&quot;standard&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>因为大多数工作是在索引时完成的，所有的查询只要查找 brown 和 fo 这两个词，这比使用 match_phrase_prefix 查找所有以 fo 开始的词的方式要高效许多。</p>
<h2 id="控制相关度"><a href="#控制相关度" class="headerlink" title="控制相关度"></a>控制相关度</h2><h3 id="相关度评分背后的理论"><a href="#相关度评分背后的理论" class="headerlink" title="相关度评分背后的理论"></a>相关度评分背后的理论</h3><p>以下三个因素——词频（term frequency）、逆向文档频率（inverse document frequency）和字段长度归一值（field-length norm）——<strong>是在索引时计算并存储的</strong>。 最后将它们结合在一起计算单个词在特定文档中的 权重 。</p>
<h3 id="Lucene-的实用评分函数"><a href="#Lucene-的实用评分函数" class="headerlink" title="Lucene 的实用评分函数"></a>Lucene 的实用评分函数</h3><p><em>实用评分函数（practical scoring function）</em><br>…………………………..<br>score(q,d)  &#x3D;  &lt;1&gt;<br>            queryNorm(q)  &lt;2&gt;<br>          · coord(q,d)    &lt;3&gt;<br>          · ∑ (           &lt;4&gt;<br>                tf(t in d)   &lt;5&gt;<br>              · idf(t)²      &lt;6&gt;<br>              · t.getBoost() &lt;7&gt;<br>              · norm(t,d)    &lt;8&gt;<br>            ) (t in q)    &lt;4&gt;<br>…………………………..</p>
<p>&lt;1&gt; <code>score(q,d)</code> 是文档 <code>d</code> 与查询 <code>q</code> 的相关度评分。<br>&lt;2&gt; <code>queryNorm(q)</code> 是 &lt;&lt;query-norm,_查询归一化_ 因子&gt;&gt; （新）。<br>&lt;3&gt; <code>coord(q,d)</code> 是 &lt;&lt;coord,_协调_ 因子&gt;&gt; （新）。<br>&lt;4&gt; 查询 <code>q</code> 中每个词 <code>t</code> 对于文档 <code>d</code> 的权重和。<br>&lt;5&gt; <code>tf(t in d)</code> 是词 <code>t</code> 在文档 <code>d</code> 中的 &lt;&lt;tf,词频&gt;&gt; 。<br>&lt;6&gt; <code>idf(t)</code> 是词 <code>t</code> 的 &lt;&lt;idf,逆向文档频率&gt;&gt; 。<br>&lt;7&gt; <code>t.getBoost()</code> 是查询中使用的 &lt;&lt;query-time-boosting,_boost_&gt;&gt;（新）。<br>&lt;8&gt; <code>norm(t,d)</code> 是 &lt;&lt;field-norm,字段长度归一值&gt;&gt; ，与 &lt;&lt;index-boost,索引时字段层 boost&gt;&gt; （如果存在）的和（新）。</p>
<blockquote>
<p>我们不建议在建立索引时对字段提升权重，有以下原因：<br>将提升值与字段长度归一值合在单个字节中存储会丢失字段长度归一值的精度，这样会导致 Elasticsearch 不知如何区分包含三个词的字段和包含五个词的字段。<br>要想改变索引时的提升值，就必须重新为所有文档建立索引，与此不同的是，查询时的提升值可以随着每次查询的不同而更改。<br>如果一个索引时权重提升的字段有多个值，提升值会按照每个值来自乘，这会导致该字段的权重急剧上升。<br>查询时赋予权重 是更为简单、清楚、灵活的选择。</p>
</blockquote>
<h3 id="查询时权重提升"><a href="#查询时权重提升" class="headerlink" title="查询时权重提升"></a>查询时权重提升</h3><p>在实际应用中，无法通过简单的公式得出某个特定查询语句的 “正确” 权重提升值，只能通过不断尝试获得。需要记住的是 boost 只是影响相关度评分的其中一个因子；它还需要与其他因子相互竞争。在前例中， <strong>title 字段相对 content 字段可能已经有一个 “缺省的” 权重提升值</strong>，这因为在 字段长度归一值 中，标题往往比相关内容要短，所以不要想当然的去盲目提升一些字段的权重。选择权重，检查结果，如此反复。</p>
<p>当在多个索引中搜索时， 可以使用参数 indices_boost 来提升整个索引的权重</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /docs_2014_*/_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;indices_boost&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;docs_2014_10&quot;</span><span class="punctuation">:</span> <span class="number">3</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;docs_2014_09&quot;</span><span class="punctuation">:</span> <span class="number">2</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;quick brown fox&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="使用查询结构修改相关度"><a href="#使用查询结构修改相关度" class="headerlink" title="使用查询结构修改相关度"></a>使用查询结构修改相关度</h3><p>略</p>
<h3 id="function-score-查询"><a href="#function-score-查询" class="headerlink" title="function_score 查询"></a>function_score 查询</h3><p>function_score 查询 是用来控制评分过程的终极武器，它允许为每个与主查询匹配的文档应用一个函数， 以达到改变甚至完全替换原始查询评分 _score 的目的。<br>实际上，也能用过滤器对结果的 子集 应用不同的函数，这样一箭双雕：既能高效评分，又能利用过滤器缓存。</p>
<p>Elasticsearch 预定义了一些函数：</p>
<ul>
<li>weight<br>为每个文档应用一个简单而不被规范化的权重提升值：当 weight 为 2 时，最终结果为 2 * _score 。</li>
<li>field_value_factor<br>使用这个值来修改 _score ，如将 popularity 或 votes （受欢迎或赞）作为考虑因素。</li>
<li>random_score<br>为每个用户都使用一个不同的随机评分对结果排序，但对某一具体用户来说，看到的顺序始终是一致的。</li>
<li>衰减函数 —— linear 、 exp 、 gauss<br>将浮动值结合到评分 _score 中，例如结合 publish_date 获得最近发布的文档，结合 geo_location 获得更接近某个具体经纬度（lat&#x2F;lon）地点的文档，结合 price 获得更接近某个特定价格的文档。</li>
<li>script_score<br>如果需求超出以上范围时，用自定义脚本可以完全控制评分计算，实现所需逻辑。<br>如果没有 function_score 查询，就不能将全文查询与最新发生这种因子结合在一起评分，而不得不根据评分 _score 或时间 date 进行排序；这会相互影响抵消两种排序各自的效果。这个查询可以使两个效果融合：可以仍然根据全文相关度进行排序，但也会同时考虑最新发布文档、流行文档、或接近用户希望价格的产品。</li>
</ul>
<h3 id="按受欢迎度提升权重"><a href="#按受欢迎度提升权重" class="headerlink" title="按受欢迎度提升权重"></a>按受欢迎度提升权重</h3><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /blogposts/post/_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;function_score&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;multi_match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span>    <span class="string">&quot;popularity&quot;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="attr">&quot;fields&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span> <span class="string">&quot;title&quot;</span><span class="punctuation">,</span> <span class="string">&quot;content&quot;</span> <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;field_value_factor&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;field&quot;</span><span class="punctuation">:</span>    <span class="string">&quot;votes&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;modifier&quot;</span><span class="punctuation">:</span> <span class="string">&quot;log1p&quot;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>修饰语 modifier 的值可以为： none （默认状态）、 log 、 log1p 、 log2p 、 ln 、 ln1p 、 ln2p 、 square 、 sqrt 以及 reciprocal 。想要了解更多信息请参照： <a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/query-dsl-function-score-query.html#function-field-value-factor">field_value_factor 文档</a>.</p>
<p><strong>boost_mode</strong><br>或许将全文评分与 field_value_factor 函数值乘积的效果仍然可能太大， 我们可以通过参数 boost_mode 来控制函数与查询评分 _score 合并后的结果，参数接受的值为：</p>
<p>multiply<br>评分 _score 与函数值的积（默认）<br>sum<br>评分 _score 与函数值的和<br>min<br>评分 _score 与函数值间的较小值<br>max<br>评分 _score 与函数值间的较大值<br>replace<br>函数值替代评分 _score</p>
<p>可以使用 <strong>max_boost</strong> 参数限制一个函数的最大效果</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /blogposts/post/_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;function_score&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;multi_match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span>    <span class="string">&quot;popularity&quot;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="attr">&quot;fields&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span> <span class="string">&quot;title&quot;</span><span class="punctuation">,</span> <span class="string">&quot;content&quot;</span> <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;field_value_factor&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;field&quot;</span><span class="punctuation">:</span>    <span class="string">&quot;votes&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;modifier&quot;</span><span class="punctuation">:</span> <span class="string">&quot;log1p&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;factor&quot;</span><span class="punctuation">:</span>   <span class="number">0.1</span></span><br><span class="line">      <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;boost_mode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sum&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;max_boost&quot;</span><span class="punctuation">:</span>  <span class="number">1.5</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="过滤集提升权重"><a href="#过滤集提升权重" class="headerlink" title="过滤集提升权重"></a>过滤集提升权重</h3><p>用过滤器将结果划分为多个子集（每个特性一个过滤器），并为每个子集使用不同的函数。</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;function_score&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;filter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;term&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;city&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Barcelona&quot;</span> <span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;functions&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;filter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;term&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;features&quot;</span><span class="punctuation">:</span> <span class="string">&quot;wifi&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="attr">&quot;weight&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;filter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;term&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;features&quot;</span><span class="punctuation">:</span> <span class="string">&quot;garden&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="attr">&quot;weight&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;filter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;term&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;features&quot;</span><span class="punctuation">:</span> <span class="string">&quot;pool&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="attr">&quot;weight&quot;</span><span class="punctuation">:</span> <span class="number">2</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;score_mode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sum&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="随机评分"><a href="#随机评分" class="headerlink" title="随机评分"></a>随机评分</h3><p>略</p>
<h3 id="越近越好（衰减函数）"><a href="#越近越好（衰减函数）" class="headerlink" title="越近越好（衰减函数）"></a>越近越好（衰减函数）</h3><p>function_score 查询会提供一组 衰减函数（decay functions） ， 让我们有能力在两个滑动标准，如地点和价格，之间权衡。</p>
<p>有三种衰减函数—— linear 、 exp 和 gauss （线性、指数和高斯函数），它们可以操作数值、时间以及经纬度地理坐标点这样的字段。所有三个函数都能接受以下参数：</p>
<p>origin<br>中心点 或字段可能的最佳值，落在原点 origin 上的文档评分 _score 为满分 1.0 。<br>scale<br>衰减率，即一个文档从原点 origin 下落时，评分 _score 改变的速度。（例如，每 £10 欧元或每 100 米）。<br>decay<br>从原点 origin 衰减到 scale 所得的评分 _score ，默认值为 0.5 。<br>offset<br>以原点 origin 为中心点，为其设置一个非零的偏移量 offset 覆盖一个范围，而不只是单个原点。在范围 -offset &lt;&#x3D; origin &lt;&#x3D; +offset 内的所有评分 _score 都是 1.0 。</p>
<h3 id="理解-price-价格语句"><a href="#理解-price-价格语句" class="headerlink" title="理解 price 价格语句"></a>理解 price 价格语句</h3><p>略</p>
<h3 id="脚本评分"><a href="#脚本评分" class="headerlink" title="脚本评分"></a>脚本评分</h3><p><a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/script-score.html">https://www.elastic.co/guide/cn/elasticsearch/guide/current/script-score.html</a></p>
<h3 id="可插拔的相似度算法"><a href="#可插拔的相似度算法" class="headerlink" title="可插拔的相似度算法"></a>可插拔的相似度算法</h3><p><a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/pluggable-similarites.html">https://www.elastic.co/guide/cn/elasticsearch/guide/current/pluggable-similarites.html</a></p>
<h3 id="更改相似度"><a href="#更改相似度" class="headerlink" title="更改相似度"></a>更改相似度</h3><p><a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/changing-similarities.html">https://www.elastic.co/guide/cn/elasticsearch/guide/current/changing-similarities.html</a></p>
<h3 id="调试相关度是最后-10-要做的事情"><a href="#调试相关度是最后-10-要做的事情" class="headerlink" title="调试相关度是最后 10% 要做的事情"></a>调试相关度是最后 10% 要做的事情</h3><p><strong>相关度的调试就有如兔子洞，一旦跳进去就很难再出来</strong>。 最相关 这个概念是一个难以触及的模糊目标，通常不同人对文档排序又有着不同的想法，这很容易使人陷入持续反复调整而没有明显进展的怪圈。</p>
<p>我们强烈建议不要陷入这种怪圈，而要监控测量搜索结果。监控用户点击最顶端结果的频次，这可以是前 10 个文档，也可以是第一页的；用户不查看首次搜索的结果而直接执行第二次查询的频次；用户来回点击并查看搜索结果的频次，等等诸如此类的信息。</p>
<h1 id="处理人类语言"><a href="#处理人类语言" class="headerlink" title="处理人类语言"></a>处理人类语言</h1><h2 id="开始处理各种语言"><a href="#开始处理各种语言" class="headerlink" title="开始处理各种语言"></a>开始处理各种语言</h2><p>略，混合不同语言的处理。</p>
<h2 id="词汇识别"><a href="#词汇识别" class="headerlink" title="词汇识别"></a>词汇识别</h2><p><a href="https://www.elastic.co/guide/en/elasticsearch/plugins/current/analysis.html">官方分词插件</a></p>
<h2 id="归一化词元"><a href="#归一化词元" class="headerlink" title="归一化词元"></a>归一化词元</h2><p>略</p>
<h2 id="将单词还原为词根"><a href="#将单词还原为词根" class="headerlink" title="将单词还原为词根"></a>将单词还原为词根</h2><p>略</p>
<h2 id="停用词-性能与精度"><a href="#停用词-性能与精度" class="headerlink" title="停用词: 性能与精度"></a>停用词: 性能与精度</h2><h3 id="停用词的优缺点"><a href="#停用词的优缺点" class="headerlink" title="停用词的优缺点"></a>停用词的优缺点</h3><p>在此基础上，从索引里将这些词移除会使我们降低某种类型的搜索能力。将前面这些所列单词移除会让我们难以完成以下事情：</p>
<p>区分 happy 和 _not happy_。<br>搜索乐队名称 The The。<br>查找莎士比亚的名句 “To be, or not to be” （生存还是毁灭)。<br>使用挪威的国家代码: <code>no</code>。<br>移除停用词的最主要好处是性能，假设我们在个具有上百万文档的索引中搜索单词 fox<code>。或许 </code>fox 只在其中 20 个文档中出现，也就是说 Elasticsearch 需要计算 20 个文档的相关度评分 <code>_score </code>从而排出前十。现在我们把搜索条件改为 <code>the OR fox</code>，几乎所有的文件都包含 <code>the 这个词，也就是说 Elasticsearch 需要为所有一百万文档计算评分 </code>_score&#96;。 由此可见第二个查询肯定没有第一个的结果好。</p>
<p>幸运的是，我们可以用来保持常用词搜索，同时还可以保持良好的性能。</p>
<h3 id="停用词与性能"><a href="#停用词与性能" class="headerlink" title="停用词与性能"></a>停用词与性能</h3><p><strong>保留停用词最大的缺点就影响搜索性能。</strong><br>我们想要减少待评分文档的数量，最简单的方式就是在and 操作符 match 查询时使用 and 操作符， 这样可以让所有词都是必须的。</p>
<p>以下是 match 查询：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span>    <span class="string">&quot;the quick brown fox&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;operator&quot;</span><span class="punctuation">:</span> <span class="string">&quot;and&quot;</span></span><br><span class="line">             <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>最少匹配数(minimum_should_match)<br>在精度匹配控制精度的章节里面，我们讨论过使用 minimum_should_match 配置去掉结果中次相关的长尾。 虽然它只对这个目的奏效，但是也为我们从侧面带来一个好处，它提供 and 操作符相似的性能。</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="string">&quot;the quick brown fox&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;minimum_should_match&quot;</span><span class="punctuation">:</span> <span class="string">&quot;75%&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>在上面这个示例中，四分之三的词都必须匹配，这意味着我们只需考虑那些包含最低频或次低频词的文档。 相比默认使用 or 操作符的简单查询，这为我们带来了巨大的性能提升。不过我们有办法可以做得更好……</p>
<h3 id="词项的分别管理"><a href="#词项的分别管理" class="headerlink" title="词项的分别管理"></a>词项的分别管理</h3><p>match 查询接受一个参数 <strong>cutoff_frequency</strong> ，从而可以让它将查询字符串里的词项分为低频和高频两组。 低频组（更重要的词项）组成 bulk 大量查询条件，而高频组（次重要的词项）只会用来评分，而不参与匹配过程。通过对这两组词的区分处理，我们可以在之前慢查询的基础上获得巨大的速度提升。<br>cutoff_frequency 会查看索引里词项的具体频率，这些词会被自动归类为 高频词汇 。</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Quick and the dead&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;cutoff_frequency&quot;</span><span class="punctuation">:</span> <span class="number">0.01</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>任何词项出现在文档中超过1%，被认为是高频词。cutoff_frequency 配置可以指定为一个分数（ 0.01 ）或者一个正整数（ 5 ）。</p>
<p>此查询通过 cutoff_frequency 配置，将查询条件划分为低频组（ quick , dead ）和高频组（ and , the ）。然后，此查询会被重写为以下的 bool 查询：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;bool&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;must&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;bool&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;should&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">          <span class="punctuation">&#123;</span> <span class="attr">&quot;term&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;quick&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="punctuation">&#123;</span> <span class="attr">&quot;term&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;dead&quot;</span>  <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">]</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;should&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;bool&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;should&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">          <span class="punctuation">&#123;</span> <span class="attr">&quot;term&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;and&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="punctuation">&#123;</span> <span class="attr">&quot;term&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;the&quot;</span> <span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">]</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="停用词与短语查询"><a href="#停用词与短语查询" class="headerlink" title="停用词与短语查询"></a>停用词与短语查询</h3><p>一个典型的索引会可能包含部分或所有以下数据：</p>
<p>词项字典（Terms dictionary）<br>索引中所有文档内所有词项的有序列表，以及包含该词的文档数量。<br>倒排表（Postings list）<br>包含每个词项的文档（ID）列表。<br>词频（Term frequency）<br>每个词项在每个文档里出现的频率。<br>位置（Positions）<br>每个词项在每个文档里出现的位置，供短语查询或近似查询使用。<br>偏移（Offsets）<br>每个词项在每个文档里开始与结束字符的偏移，供词语高亮使用，默认是禁用的。<br>规范因子（Norms）<br>用来对字段长度进行规范化处理的因子，给较短字段予以更多权重。</p>
<p>我们首先应该问自己：是否真的需要使用短语查询 或 近似查询 ？</p>
<p>答案通常是：不需要。在很多应用场景下，比如说日志，我们需要知道一个词 是否 在文档中（这个信息由倒排表提供）而不是关心词的位置在哪里。或许我们要对一两个字段使用短语查询，但是我们完全可以在其他 analyzed 字符串字段上禁用位置信息。</p>
<p>index_options 参数 允许我们控制索引里为每个字段存储的信息。 可选值如下:</p>
<p>docs<br>只存储文档及其包含词项的信息。这对 not_analyzed 字符串字段是默认的。<br>freqs<br>存储 docs 信息，以及每个词在每个文档里出现的频次。词频是完成TF&#x2F;IDF 相关度计算的必要条件，但如果只想知道一个文档是否包含某个特定词项，则无需使用它。<br>positions<br>存储 docs 、 freqs 、 analyzed ，以及每个词项在每个文档里出现的位置。 这对 analyzed 字符串字段是默认的，但当不需使用短语或近似匹配时，可以将其禁用。<br>offsets<br>存储 docs,freqs,positions, 以及每个词在原始字符串中开始与结束字符的偏移信息( postings highlighter )。这个信息被用以高亮搜索结果，但它默认是禁用的。</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">PUT /my_index</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;mappings&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;my_type&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;properties&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span>          <span class="string">&quot;string&quot;</span></span><br><span class="line">       <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span>          <span class="string">&quot;string&quot;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="attr">&quot;index_options&quot;</span><span class="punctuation">:</span> <span class="string">&quot;freqs&quot;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="common-grams-过滤器"><a href="#common-grams-过滤器" class="headerlink" title="common_grams 过滤器"></a>common_grams 过滤器</h3><p>common_grams 过滤器是针对短语查询能更高效的使用停用词而设计的。</p>
<h3 id="停用词与相关性"><a href="#停用词与相关性" class="headerlink" title="停用词与相关性"></a>停用词与相关性</h3><p>在索引中保留停用词会降低相关度计算的准确性，特别是当我们的文档非常长时。<br>基于逆文档频率的影响，非常常用的词可能只有很低的权重，但是在长文档中，单个文档出现的绝对数量很大的停用词会导致这些词被不自然的加权。</p>
<h2 id="同义词"><a href="#同义词" class="headerlink" title="同义词"></a>同义词</h2><p>&#x2F;&#x2F;TODO</p>
<h3 id="拼写错误-模糊查询"><a href="#拼写错误-模糊查询" class="headerlink" title="拼写错误(模糊查询)"></a>拼写错误(模糊查询)</h3><p>&#x2F;&#x2F;TODO</p>
<h1 id="聚合-1"><a href="#聚合-1" class="headerlink" title="聚合"></a>聚合</h1><h2 id="高阶概念"><a href="#高阶概念" class="headerlink" title="高阶概念"></a>高阶概念</h2><p>要掌握聚合，你只需要明白两个主要的概念：</p>
<p>桶（Buckets）<br>满足特定条件的文档的集合<br>指标（Metrics）<br>对桶内的文档进行统计计算</p>
<h2 id="尝试聚合"><a href="#尝试聚合" class="headerlink" title="尝试聚合"></a>尝试聚合</h2><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /cars/transactions/_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">   <span class="attr">&quot;size&quot;</span> <span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">   <span class="attr">&quot;aggs&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;colors&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">         <span class="attr">&quot;terms&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;field&quot;</span><span class="punctuation">:</span> <span class="string">&quot;color&quot;</span></span><br><span class="line">         <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">         <span class="attr">&quot;aggs&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;avg_price&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">               <span class="attr">&quot;avg&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                  <span class="attr">&quot;field&quot;</span><span class="punctuation">:</span> <span class="string">&quot;price&quot;</span></span><br><span class="line">               <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">         <span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">   <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>正如所见，我们用前面的例子加入了新的 aggs 层。这个新的聚合层让我们可以将 avg 度量嵌套置于 terms 桶内。实际上，这就为每个颜色生成了平均价格。</p>
<p>正如 颜色 的例子，我们需要给度量起一个名字（ avg_price ）这样可以稍后根据名字获取它的值。最后，我们指定度量本身（ avg ）以及我们想要计算平均值的字段（ price ）</p>
<p>嵌套桶</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /cars/transactions/_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">   <span class="attr">&quot;size&quot;</span> <span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">   <span class="attr">&quot;aggs&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;colors&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">         <span class="attr">&quot;terms&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;field&quot;</span><span class="punctuation">:</span> <span class="string">&quot;color&quot;</span></span><br><span class="line">         <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">         <span class="attr">&quot;aggs&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;avg_price&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;avg&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;field&quot;</span><span class="punctuation">:</span> <span class="string">&quot;price&quot;</span> <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;make&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;terms&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;field&quot;</span> <span class="punctuation">:</span> <span class="string">&quot;make&quot;</span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;aggs&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;min_price&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;min&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;field&quot;</span><span class="punctuation">:</span> <span class="string">&quot;price&quot;</span><span class="punctuation">&#125;</span> <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;max_price&quot;</span> <span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;max&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;field&quot;</span><span class="punctuation">:</span> <span class="string">&quot;price&quot;</span><span class="punctuation">&#125;</span> <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">         <span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">   <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>新增的这个 make 聚合，它是一个 terms 桶（嵌套在 colors 、 terms 桶内）。这意味着它 会为数据集中的每个唯一组合生成（ color 、 make ）元组。</p>
<h2 id="条形图"><a href="#条形图" class="headerlink" title="条形图"></a>条形图</h2><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /cars/transactions/_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">   <span class="attr">&quot;size&quot;</span> <span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">   <span class="attr">&quot;aggs&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;price&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span></span><br><span class="line">         <span class="attr">&quot;histogram&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;field&quot;</span><span class="punctuation">:</span> <span class="string">&quot;price&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;interval&quot;</span><span class="punctuation">:</span> <span class="number">20000</span></span><br><span class="line">         <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">         <span class="attr">&quot;aggs&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;revenue&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">               <span class="attr">&quot;sum&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                 <span class="attr">&quot;field&quot;</span> <span class="punctuation">:</span> <span class="string">&quot;price&quot;</span></span><br><span class="line">               <span class="punctuation">&#125;</span></span><br><span class="line">             <span class="punctuation">&#125;</span></span><br><span class="line">         <span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">   <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>如我们所见，查询是围绕 price 聚合构建的，它包含一个 histogram 桶。它要求字段的类型必须是数值型的同时需要设定分组的间隔范围。 间隔设置为 20,000 意味着我们将会得到如 [0-19999, 20000-39999, …] 这样的区间。</p>
<h2 id="Doc-Values-and-Fielddata"><a href="#Doc-Values-and-Fielddata" class="headerlink" title="Doc Values and Fielddata"></a>Doc Values and Fielddata</h2><p>Doc values 可以使聚合更快、更高效并且内存友好。<br>文档值是在索引时与倒排索引同时产生的。也就是说文档值是按段来产生的并且是不可变的，正如用于搜索的倒排索引一样。 同样，和倒排索引一样，文档值也序列化到磁盘。这些对于性能和伸缩性很重要。</p>
<p>&#x3D;&#x3D;文档值默认对所有字段启用，除了分析字符类型字段。也就是说所有的数字、地理坐标、日期、IP 和不分析（ not_analyzed ）字符类型。&#x3D;&#x3D;</p>
<p>字段 “session_id” 禁用了文档值：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">PUT my_index</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;mappings&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;my_type&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;properties&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;session_id&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span>       <span class="string">&quot;string&quot;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="attr">&quot;index&quot;</span><span class="punctuation">:</span>      <span class="string">&quot;not_analyzed&quot;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="attr">&quot;doc_values&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h1 id="地理位置"><a href="#地理位置" class="headerlink" title="地理位置"></a>地理位置</h1><p>&#x2F;&#x2F;TODO</p>
<blockquote>
<p>Blockquote</p>
</blockquote>
<h1 id="数据建模"><a href="#数据建模" class="headerlink" title="数据建模"></a>数据建模</h1><h2 id="关联关系处理"><a href="#关联关系处理" class="headerlink" title="关联关系处理"></a>关联关系处理</h2><p> 以下四种常用的方法，用来在 Elasticsearch 中进行关系型数据的管理：</p>
<p>Application-side joins<br>Data denormalization<br>Nested objects<br>Parent&#x2F;child relationships</p>
<p>当非规范化成为很多项目的一个很好的选择，采用锁方案的需求会带来复杂的实现逻辑。 作为替代方案，Elasticsearch 提供两个模型帮助我们处理相关联的实体： 嵌套的对象 和 父子关系 。</p>
<h2 id="嵌套对象"><a href="#嵌套对象" class="headerlink" title="嵌套对象"></a>嵌套对象</h2><p>嵌套对象 就是来解决这个问题的。将 comments 字段类型设置为 nested 而不是 object 后,每一个嵌套对象都会被索引为一个 隐藏的独立文档 ,举例如下:</p>
<p>{<br>  “comments.name”:    [ john, smith ],<br>  “comments.comment”: [ article, great ],<br>  “comments.age”:     [ 28 ],<br>  “comments.stars”:   [ 4 ],<br>  “comments.date”:    [ 2014-09-01 ]<br>}<br>{<br>  “comments.name”:    [ alice, white ],<br>  “comments.comment”: [ like, more, please, this ],<br>  “comments.age”:     [ 31 ],<br>  “comments.stars”:   [ 5 ],<br>  “comments.date”:    [ 2014-10-22 ]<br>}<br>{<br>  “title”:            [ eggs, nest ],<br>  “body”:             [ making, money, work, your ],<br>  “tags”:             [ cash, shares ]<br>}</p>
<p>设置一个字段为 nested 很简单 —  你只需要将字段类型 object 替换为 nested 即可：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">PUT /my_index</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;mappings&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;blogpost&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;properties&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;comments&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;nested&quot;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="attr">&quot;properties&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span>    <span class="punctuation">&#123;</span> <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;string&quot;</span>  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;comment&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;string&quot;</span>  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;age&quot;</span><span class="punctuation">:</span>     <span class="punctuation">&#123;</span> <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;short&quot;</span>   <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;stars&quot;</span><span class="punctuation">:</span>   <span class="punctuation">&#123;</span> <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;short&quot;</span>   <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;date&quot;</span><span class="punctuation">:</span>    <span class="punctuation">&#123;</span> <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;date&quot;</span>    <span class="punctuation">&#125;</span></span><br><span class="line">          <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>由于嵌套对象 被索引在独立隐藏的文档中，我们无法直接查询它们。 相应地，我们必须使用 nested 查询 去获取它们：<br>默认情况下，根文档的分数是这些嵌套文档分数的平均值。可以通过设置 score_mode 参数来控制这个得分策略，相关策略有 avg (平均值), max (最大值), sum (加和) 和 none (直接返回 1.0 常数值分数)。</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /my_index/blogpost/_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;bool&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;must&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;title&quot;</span><span class="punctuation">:</span> <span class="string">&quot;eggs&quot;</span></span><br><span class="line">          <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;nested&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;comments&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;score_mode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;max&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">              <span class="attr">&quot;bool&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;must&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                  <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                      <span class="attr">&quot;comments.name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;john&quot;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                  <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                      <span class="attr">&quot;comments.age&quot;</span><span class="punctuation">:</span> <span class="number">28</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                  <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">]</span></span><br><span class="line">              <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">          <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="使用嵌套字段排序"><a href="#使用嵌套字段排序" class="headerlink" title="使用嵌套字段排序"></a>使用嵌套字段排序</h3><p>假如我们想要查询在10月份收到评论的博客文章，并且按照 stars 数的最小值来由小到大排序，那么查询语句如下：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET /_search</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;nested&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;comments&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;filter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;range&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;comments.date&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;gte&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2014-10-01&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;lt&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;2014-11-01&quot;</span></span><br><span class="line">          <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;sort&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;comments.stars&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;order&quot;</span><span class="punctuation">:</span> <span class="string">&quot;asc&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;mode&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;min&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;nested_path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;comments&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;nested_filter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;range&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;comments.date&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;gte&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2014-10-01&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;lt&quot;</span><span class="punctuation">:</span>  <span class="string">&quot;2014-11-01&quot;</span></span><br><span class="line">          <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>


<p>我们为什么要用 nested_path 和 nested_filter 重复查询条件呢？原因在于，排序发生在查询执行之后。 查询条件限定了只在10月份收到评论的博客文档，但返回整个博客文档。如果我们不在排序子句中加入 nested_filter ， 那么我们对博客文档的排序将基于博客文档的所有评论，而不是仅仅在10月份接收到的评论。</p>
<h2 id="父子关系文档"><a href="#父子关系文档" class="headerlink" title="父子关系文档"></a>父子关系文档</h2><p>与 nested objects 相比，父-子关系的主要优势有：</p>
<p>更新父文档时，不会重新索引子文档。<br>创建，修改或删除子文档时，不会影响父文档或其他子文档。这一点在这种场景下尤其有用：子文档数量较多，并且子文档创建和修改的频率高时。<br>子文档可以作为搜索结果独立返回。<br>Elasticsearch 维护了一个父文档和子文档的映射关系，得益于这个映射，父-子文档关联查询操作非常快。但是这个映射也对父-子文档关系有个限制条件：父文档和其所有子文档，都必须要存储在同一个分片中。</p>
<p>&#x3D;&#x3D;当文档索引性能远比查询性能重要 的时候，父子关系是非常有用的，但是它也是有巨大代价的。其查询速度会比同等的嵌套查询慢5到10倍!&#x3D;&#x3D;</p>
<p><strong>当你考虑父子关系是否适合你现有关系模型时，请考虑下面这些建议 ：</strong></p>
<p><strong>尽量少地使用父子关系</strong>，仅在子文档远多于父文档时使用。<br>避免在一个查询中使用多个父子联合语句。<br>在 has_child 查询中使用 filter 上下文，或者设置 score_mode 为 none 来避免计算文档得分。<br>保证父 IDs 尽量短，以便在 doc values 中更好地压缩，被临时载入时占用更少的内存。<br>最重要的是: 先考虑下我们之前讨论过的其他方式来达到父子关系的效果。</p>
<h2 id="扩容设计"><a href="#扩容设计" class="headerlink" title="扩容设计"></a>扩容设计</h2><p>副本分片与主分片做着相同的工作；它们只是扮演着略微不同的角色。没有必要确保主分片均匀地分布在所有节点中。<br>PS:主分片数&gt;&#x3D;节点数，大于是预留给节点数的扩充。</p>
<p>使用索引别名来指向当前版本的索引。 举例来说，给你的索引命名为 tweets_v1 而不是 tweets 。你的应用程序会与 tweets 进行交互，但事实上它是一个指向 tweets_v1 的别名。 这允许你将别名切换至一个更新版本的索引而保持服务运转。</p>
<p>一个搜索请求可以以多个索引为目标，所以将搜索别名指向 tweets_1 以及 tweets_2 是完全有效的。 然而，索引写入请求只能以单个索引为目标。因此，我们必须将索引写入的别名只指向新的索引。<br>PUT &#x2F;tweets_1&#x2F;_alias&#x2F;tweets_search<br>PUT &#x2F;tweets_1&#x2F;_alias&#x2F;tweets_index<br>POST &#x2F;_aliases<br>{<br>  “actions”: [<br>    { “add”:    { “index”: “tweets_2”, “alias”: “tweets_search” }},<br>    { “remove”: { “index”: “tweets_1”, “alias”: “tweets_index”  }},<br>    { “add”:    { “index”: “tweets_2”, “alias”: “tweets_index”  }}<br>  ]<br>}<br>一个文档 GET 请求，像一个索引写入请求那样，只能以单个索引为目标。 这导致在通过ID获取文档这样的场景下有一点复杂。作为代替，你可以对 tweets_1 以及 tweets_2 运行一个 ids 查询 搜索请求， 或者 multi-get 请求。</p>
<p>按时间范围索引</p>
<p>POST &#x2F;_aliases<br>{<br>  “actions”: [<br>    { “add”:    { “alias”: “logs_current”,  “index”: “logs_2014-10” }},<br>    { “remove”: { “alias”: “logs_current”,  “index”: “logs_2014-09” }},<br>    { “add”:    { “alias”: “last_3_months”, “index”: “logs_2014-10” }},<br>    { “remove”: { “alias”: “last_3_months”, “index”: “logs_2014-07” }}<br>  ]<br>}</p>
<h3 id="索引模板"><a href="#索引模板" class="headerlink" title="索引模板"></a>索引模板</h3><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">PUT /_template/my_logs</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;template&quot;</span><span class="punctuation">:</span> <span class="string">&quot;logstash-*&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;order&quot;</span><span class="punctuation">:</span>    <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;settings&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;number_of_shards&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;mappings&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;_default_&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;_all&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;enabled&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;aliases&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;last_3_months&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span><span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>这个模板指定了所有名字以 logstash- 为起始的索引的默认设置，不论它是手动还是自动创建的。 如果我们认为明天的索引需要比今天更大的容量，我们可以更新这个索引以使用更多的分片。</p>
<p>这个模板还将新建索引添加至了 last_3_months 别名中，然而从那个别名中删除旧的索引则需要手动执行。</p>
<h3 id="数据过期"><a href="#数据过期" class="headerlink" title="数据过期"></a>数据过期</h3><p>删除整个索引比删除单个文档要更加高效：Elasticsearch 只需要删除整个文件夹。</p>
<p>这些索引可以被关闭。它们还会存在于集群中，但它们不会消耗磁盘空间以外的资源。重新打开一个索引要比从备份中恢复快得多。<br>在关闭之前，值得我们去刷写索引来确保没有事务残留在事务日志中。一个空白的事务日志会使得索引在重新打开时恢复得更快：<br>POST &#x2F;logs_2014-01-<em>&#x2F;_flush<br>POST &#x2F;logs_2014-01-</em>&#x2F;_close<br>POST &#x2F;logs_2014-01-*&#x2F;_open</p>
<h3 id="扩容并不是无限的"><a href="#扩容并不是无限的" class="headerlink" title="扩容并不是无限的"></a>扩容并不是无限的</h3><p>需要记住的是相同的数据结构需要在每个节点的内存中保存，并且当它发生更改时必须发布到每一个节点。 集群状态的数据量越大，这个操作就会越久。</p>
<h1 id="管理、监控和部署"><a href="#管理、监控和部署" class="headerlink" title="管理、监控和部署"></a>管理、监控和部署</h1><h2 id="监控"><a href="#监控" class="headerlink" title="监控"></a>监控</h2><p>X-Pack</p>
<h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><h3 id="Transport-Client-与-Node-Client"><a href="#Transport-Client-与-Node-Client" class="headerlink" title="Transport Client 与 Node Client"></a>Transport Client 与 Node Client</h3><p>如果你使用的是 Java，你可能想知道何时使用传输客户端（注：Transport Client，下同）与节点客户端（注：Node Client，下同）。 在书的开头所述， 传输客户端作为一个集群和应用程序之间的通信层。它知道 API 并能自动帮你在节点之间轮询，帮你嗅探集群等等。但它是集群 外部的 ，和 REST 客户端类似。</p>
<p>另一方面，节点客户端，实际上是一个集群中的节点（但不保存数据，不能成为主节点）。因为它是一个节点，它知道整个集群状态（所有节点驻留，分片分布在哪些节点，等等）。 这意味着它可以执行 APIs 但少了一个网络跃点。</p>
<p>这里有两个客户端案例的使用情况：</p>
<p>如果要将应用程序和 Elasticsearch 集群进行解耦，传输客户端是一个理想的选择。例如，如果您的应用程序需要快速的创建和销毁到集群的连接，传输客户端比节点客户端”轻”，因为它不是一个集群的一部分。</p>
<p>类似地，如果您需要创建成千上万的连接，你不想有成千上万节点加入集群。传输客户端（ TC ）将是一个更好的选择。</p>
<p>另一方面，如果你只需要有少数的、长期持久的对象连接到集群，客户端节点可以更高效，因为它知道集群的布局。但是它会使你的应用程序和集群耦合在一起，所以从防火墙的角度，它可能会构成问题。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://www.elastic.co/guide/cn/elasticsearch/guide/cn/index.html">Elasticsearch: 权威指南</a></p>
]]></content>
      <tags>
        <tag>book</tag>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title>Maven用户都应该知道的一些事：构建生命周期和插件</title>
    <url>/2019/06/16/maven-lifecycle-and-plugin-goal/</url>
    <content><![CDATA[<p><strong>Maven的所有实际操作都是由插件完成的，如果没有插件，Maven什么都不会干。</strong><br>（即时你没有在POM中配置<code>&lt;plugin&gt;</code>元素，Super POM中也已经帮你引入了若干核心插件）</p>
<p>那么问题来了，项目构建过程中，Maven是怎么知道应该在什么时候调用哪个插件的呢？</p>
<p>插件的调用时机，跟’生命周期’和’插件目标’有很大关系。</p>
<span id="more"></span>

<h1 id="插件目标-Plugin-Goal-是个什么鬼"><a href="#插件目标-Plugin-Goal-是个什么鬼" class="headerlink" title="插件目标(Plugin Goal)是个什么鬼"></a>插件目标(Plugin Goal)是个什么鬼</h1><p>首先需要知道的是，Maven的所有具体任务都是交由插件实现的，而一个插件往往能实现若干个任务或功能。又或者可以说一个插件能实现若干个目标，比如编译源码、执行测试用例、打包项目等等。<br>简单来说，在Maven中Plugin Goal可以看做是插件的一个功能。</p>
<p>Plugin Goal通常用‘插件前缀:插件目标‘的格式描述，常见的有：<code>compiler:compile</code>,<code>surefire:test</code>,<code>dependency:tree</code>等。</p>
<p>所谓插件前缀就是插件的名称的一个简写，比如compiler指的就是maven-complier-plugin这个插件。<br><code>compiler:compile</code>的意思既是maven-complier-plugin插件的compile目标。</p>
<h1 id="构建生命周期-Build-Lifecycle-又是什么"><a href="#构建生命周期-Build-Lifecycle-又是什么" class="headerlink" title="构建生命周期(Build Lifecycle)又是什么"></a>构建生命周期(Build Lifecycle)又是什么</h1><p>构建生命周期是对构建过程的抽象和统一，包括了几乎所有的构建步骤，如清理、初始化、编译、测试、打包等等。</p>
<p>Maven为不用的目的定义了3套相互独立的生命周期：</p>
<ul>
<li>clean 用于清理项目</li>
<li>default 用于构建项目</li>
<li>site 用于构建项目站点</li>
</ul>
<p>每种生命周期都是由若干阶段(phase)组成，如clean生命周期由pre-clean,clean,post-clean三个阶段组成。</p>
<p>阶段之间是有序的，而且后面的阶段依赖前面的阶段。<br>用户执行<code>mvn clean</code>时，实际上maven会先执行pre-clean阶段，然后再执行clean阶段；执行<code>mvn post-clean</code>时，则会依次执行pre-clean,clean,post-clean三个阶段。</p>
<p>3个生命周期各自的阶段如下：</p>
<h3 id="Clean-Lifecycle"><a href="#Clean-Lifecycle" class="headerlink" title="Clean Lifecycle"></a>Clean Lifecycle</h3><table>
<thead>
<tr>
<th>Phase</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>pre-clean</td>
<td>execute processes needed prior to the actual project cleaning</td>
</tr>
<tr>
<td>clean</td>
<td>remove all files generated by the previous build</td>
</tr>
<tr>
<td>post-clean</td>
<td>execute processes needed to finalize the project cleaning</td>
</tr>
</tbody></table>
<h3 id="Default-Lifecycle"><a href="#Default-Lifecycle" class="headerlink" title="Default Lifecycle"></a>Default Lifecycle</h3><table>
<thead>
<tr>
<th>Phase</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>validate</td>
<td>validate the project is correct and all necessary information is available.</td>
</tr>
<tr>
<td>initialize</td>
<td>initialize build state, e.g. set properties or create directories.</td>
</tr>
<tr>
<td>generate-sources</td>
<td>generate any source code for inclusion in compilation.</td>
</tr>
<tr>
<td>process-sources</td>
<td>process the source code, for example to filter any values.</td>
</tr>
<tr>
<td>generate-resources</td>
<td>generate resources for inclusion in the package.</td>
</tr>
<tr>
<td>process-resources</td>
<td>copy and process the resources into the destination directory, ready for packaging.</td>
</tr>
<tr>
<td>compile</td>
<td>compile the source code of the project.</td>
</tr>
<tr>
<td>process-classes</td>
<td>post-process the generated files from compilation, for example to do bytecode enhancement on Java classes.</td>
</tr>
<tr>
<td>generate-test-sources</td>
<td>generate any test source code for inclusion in compilation.</td>
</tr>
<tr>
<td>process-test-sources</td>
<td>process the test source code, for example to filter any values.</td>
</tr>
<tr>
<td>generate-test-resources</td>
<td>create resources for testing.</td>
</tr>
<tr>
<td>process-test-resources</td>
<td>copy and process the resources into the test destination directory.</td>
</tr>
<tr>
<td>test-compile</td>
<td>compile the test source code into the test destination directory</td>
</tr>
<tr>
<td>process-test-classes</td>
<td>post-process the generated files from test compilation, for example to do bytecode enhancement on Java classes. For Maven 2.0.5 and above.</td>
</tr>
<tr>
<td>test</td>
<td>run tests using a suitable unit testing framework. These tests should not require the code be packaged or deployed.</td>
</tr>
<tr>
<td>prepare-package</td>
<td>perform any operations necessary to prepare a package before the actual packaging. This often results in an unpacked, processed version of the package. (Maven 2.1 and above)</td>
</tr>
<tr>
<td>package</td>
<td>take the compiled code and package it in its distributable format, such as a JAR.</td>
</tr>
<tr>
<td>pre-integration-test</td>
<td>perform actions required before integration tests are executed. This may involve things such as setting up the required environment.</td>
</tr>
<tr>
<td>integration-test</td>
<td>process and deploy the package if necessary into an environment where integration tests can be run.</td>
</tr>
<tr>
<td>post-integration-test</td>
<td>perform actions required after integration tests have been executed. This may including cleaning up the environment.</td>
</tr>
<tr>
<td>verify</td>
<td>run any checks to verify the package is valid and meets quality criteria.</td>
</tr>
<tr>
<td>install</td>
<td>install the package into the local repository, for use as a dependency in other projects locally.</td>
</tr>
<tr>
<td>deploy</td>
<td>done in an integration or release environment, copies the final package to the remote repository for sharing with other developers and projects.</td>
</tr>
</tbody></table>
<h3 id="Site-Lifecycle"><a href="#Site-Lifecycle" class="headerlink" title="Site Lifecycle"></a>Site Lifecycle</h3><table>
<thead>
<tr>
<th>Phase</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>pre-site</td>
<td>execute processes needed prior to the actual project site generation</td>
</tr>
<tr>
<td>site</td>
<td>generate the project’s site documentation</td>
</tr>
<tr>
<td>post-site</td>
<td>execute processes needed to finalize the site generation, and to prepare for site deployment</td>
</tr>
<tr>
<td>site-deploy</td>
<td>deploy the generated site documentation to the specified web server</td>
</tr>
</tbody></table>
<h1 id="插件目标与生命周期的绑定"><a href="#插件目标与生命周期的绑定" class="headerlink" title="插件目标与生命周期的绑定"></a>插件目标与生命周期的绑定</h1><p><strong>插件目标是与生命周期的某个阶段绑定的</strong>，比如maven-clean-plugin的clean目标与clean阶段绑定，也就意味着当maven执行clean生命周期时，到达clean阶段的时候，会调用maven-clean-plugin的clean目标(删除项目输出目录)。</p>
<p>也就是说，<strong>插件目标的执行顺序是由其绑定到的生命周期阶段的顺序决定的，如果多个插件目标绑定到同一个阶段，那么先声明的插件会在后声明的插件之前执行。</strong></p>
<p>对于一些核心插件，Maven已经内置默认将一些插件目标绑定到了生命周期阶段，特别是default生命周期，对于不同的打包类型（jar,war,pom等），Maven提供了不同的默认绑定。如下所示：</p>
<p><strong>Clean Lifecycle Bindings</strong></p>
<table>
<thead>
<tr>
<th>Phase</th>
<th>plugin:goal</th>
</tr>
</thead>
<tbody><tr>
<td>clean</td>
<td>clean:clean</td>
</tr>
</tbody></table>
<p><strong>Default Lifecycle Bindings - Packaging ejb &#x2F; ejb3 &#x2F; jar &#x2F; par &#x2F; rar &#x2F; war</strong></p>
<table>
<thead>
<tr>
<th>Phase</th>
<th>plugin:goal</th>
</tr>
</thead>
<tbody><tr>
<td>process-resources</td>
<td>resources:resources</td>
</tr>
<tr>
<td>compile</td>
<td>compiler:compile</td>
</tr>
<tr>
<td>process-test-resources</td>
<td>resources:testResources</td>
</tr>
<tr>
<td>test-compile</td>
<td>compiler:testCompile</td>
</tr>
<tr>
<td>test</td>
<td>surefire:test</td>
</tr>
<tr>
<td>package</td>
<td>ejb:ejb  or  ejb3:ejb3  or  jar:jar  or  par:par  or  rar:rar  or  war:war</td>
</tr>
<tr>
<td>install</td>
<td>install:install</td>
</tr>
<tr>
<td>deploy</td>
<td>deploy:deploy</td>
</tr>
</tbody></table>
<p><strong>Default Lifecycle Bindings - Packaging ear</strong></p>
<table>
<thead>
<tr>
<th>Phase</th>
<th>plugin:goal</th>
</tr>
</thead>
<tbody><tr>
<td>generate-resources</td>
<td>ear:generate-application-xml</td>
</tr>
<tr>
<td>process-resources</td>
<td>resources:resources</td>
</tr>
<tr>
<td>package</td>
<td>ear:ear</td>
</tr>
<tr>
<td>install</td>
<td>install:install</td>
</tr>
<tr>
<td>deploy</td>
<td>deploy:deploy</td>
</tr>
</tbody></table>
<p><strong>Default Lifecycle Bindings - Packaging maven-plugin</strong></p>
<table>
<thead>
<tr>
<th>Phase</th>
<th>plugin:goal</th>
</tr>
</thead>
<tbody><tr>
<td>generate-resources</td>
<td>plugin:descriptor</td>
</tr>
<tr>
<td>process-resources</td>
<td>resources:resources</td>
</tr>
<tr>
<td>compile</td>
<td>compiler:compile</td>
</tr>
<tr>
<td>process-test-resources</td>
<td>resources:testResources</td>
</tr>
<tr>
<td>test-compile</td>
<td>compiler:testCompile</td>
</tr>
<tr>
<td>test</td>
<td>surefire:test</td>
</tr>
<tr>
<td>package</td>
<td>jar:jar and plugin:addPluginArtifactMetadata</td>
</tr>
<tr>
<td>install</td>
<td>install:install</td>
</tr>
<tr>
<td>deploy</td>
<td>deploy:deploy</td>
</tr>
</tbody></table>
<p><strong>Default Lifecycle Bindings - Packaging pom</strong></p>
<table>
<thead>
<tr>
<th>Phase</th>
<th>plugin:goal</th>
</tr>
</thead>
<tbody><tr>
<td>package</td>
<td></td>
</tr>
<tr>
<td>install</td>
<td>install:install</td>
</tr>
<tr>
<td>deploy</td>
<td>deploy:deploy</td>
</tr>
</tbody></table>
<p><strong>Site Lifecycle Bindings</strong></p>
<table>
<thead>
<tr>
<th>Phase</th>
<th>plugin:goal</th>
</tr>
</thead>
<tbody><tr>
<td>site</td>
<td>site:site</td>
</tr>
<tr>
<td>site-deploy</td>
<td>site:deploy</td>
</tr>
</tbody></table>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ul>
<li><a href="https://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html">Introduction to the Build Lifecycle</a></li>
<li><a href="https://book.douban.com/subject/5345682/">《Maven实战》</a></li>
</ul>
<p>转载请保留原文地址：<a href="https://liam-blog.ml/2019/06/16/maven-lifecycle-and-plugin-goal/">Maven用户都应该知道的一些事：构建生命周期和插件</a></p>
]]></content>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title>Scala并发编程实战 2：Lock 锁</title>
    <url>/2019/07/21/Scala-Concurrency-in-Practice-2/</url>
    <content><![CDATA[<p>synchronized作为内置锁，使用简单，不易出错，然鹅确有相当的局限性，例如，无法从等待获取锁的阻塞中中断，无法设置获取锁的超时。<br>所以JUC提供了另一种更灵活的加锁方式，即Lock。</p>
<span id="more"></span>

<h1 id="Lock"><a href="#Lock" class="headerlink" title="Lock"></a>Lock</h1><p>Lock接口定义如下</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">Lock</span> &#123;</span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">lock</span><span class="params">()</span>;     </span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">lockInterruptibly</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException;     </span><br><span class="line">    <span class="type">boolean</span> <span class="title function_">tryLock</span><span class="params">()</span>;     </span><br><span class="line">    <span class="type">boolean</span> <span class="title function_">tryLock</span><span class="params">(<span class="type">long</span> timeout, TimeUnit unit)</span> <span class="keyword">throws</span> InterruptedException;     </span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">unlock</span><span class="params">()</span>;     </span><br><span class="line">    Condition <span class="title function_">newCondition</span><span class="params">()</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从接口的定义不难发现，Lock不仅提供了常规的lock()阻塞式加锁，也提供了tryLock使得线程能在获取不到锁时，马上返回，<br>甚至可以等待锁一段时间后，再返回。lockInterruptibly则提供了可中断的阻塞式获取锁方式。</p>
<p>Lock的锁需要显示释放，通常要与<code>try...finally</code>语句一起使用，避免死锁。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">lock.lock(); </span><br><span class="line"><span class="keyword">try</span> &#123;     </span><br><span class="line">   <span class="comment">// update object state     </span></span><br><span class="line">   <span class="comment">// catch exceptions and restore invariants if necessary </span></span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;     </span><br><span class="line">   lock.unlock(); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="ReentrantLock"><a href="#ReentrantLock" class="headerlink" title="ReentrantLock"></a>ReentrantLock</h2><p>Lock最常用的实现类是ReentrantLock，这是一个<em>可重入锁</em>(synchronized也是)。</p>
<p>ReentrantLock默认和内置锁一样，是非公平锁，但是支持<em>公平锁</em>模式，可以用<code>ReentrantLock(true)</code>创建公平锁。</p>
<h3 id="可重入锁"><a href="#可重入锁" class="headerlink" title="可重入锁"></a>可重入锁</h3><p>所谓可重入锁，也就是说一个线程可以在持有该锁的时候，再次获取该锁。可重入锁通常与一个计数器关联，第一次获取锁的时候，计数器从0变为1，再次获取锁，变为2，以此类推。释放锁的时候，计数器每次减1，直至减为0，该锁才真正释放给其他线程。<br><strong>为啥需要可重入锁</strong><br>举个例子(JCP书上的)</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Widget</span> &#123;     </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title function_">doSomething</span><span class="params">()</span> &#123;         </span><br><span class="line">        ... </span><br><span class="line">    &#125; </span><br><span class="line">&#125;  </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LoggingWidget</span> <span class="keyword">extends</span> <span class="title class_">Widget</span> &#123;     </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title function_">doSomething</span><span class="params">()</span> &#123;         </span><br><span class="line">        System.out.println(toString() + <span class="string">&quot;: calling doSomething&quot;</span>);         </span><br><span class="line">        <span class="built_in">super</span>.doSomething();     </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>子类覆盖了父类方法，并再次调用了父类的同步方法，如果锁不支持重入，则会导致死锁。</p>
<h3 id="公平锁"><a href="#公平锁" class="headerlink" title="公平锁"></a>公平锁</h3><p>所谓公平锁，其实就是指锁的等待队列执行先进先出，等待最久的线程优先获得锁。<br><strong>但是内置锁和ReentrantLock默认都是非公平的，为啥？</strong><br>因为非公平锁的性能更好。一个事实是，一个线程从被唤醒到真正运行中间有不可忽视的延迟，这个延迟时间很可能长到足够一个运行中的线程获取锁，并完成操作，然后释放锁。也即是说，把锁给’等待最久的线程‘的过程中，可以让其他线程插队获取锁，并归还锁，还不会影响’等待最久的线程‘的运行。这样一来吞吐量就得到了提升。</p>
<h1 id="Scala栗子"><a href="#Scala栗子" class="headerlink" title="Scala栗子"></a>Scala栗子</h1><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> io.github.liam8.con</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.<span class="type">TimeUnit</span></span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.locks.&#123;<span class="type">Lock</span>, <span class="type">ReentrantLock</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LockDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> rtl: <span class="type">Lock</span> = <span class="keyword">new</span> <span class="type">ReentrantLock</span>()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> inc: <span class="type">Int</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get</span></span>(): <span class="type">Int</span> = &#123;</span><br><span class="line">    rtl.lock()</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      inc</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      rtl.unlock()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">addOne</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    rtl.lock()</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>.sleep(<span class="number">1</span>)</span><br><span class="line">      inc = <span class="number">1</span> + get()</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      rtl.unlock()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">10</span>) &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">Thread</span> &#123;</span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">            println(<span class="string">s&quot;run thread <span class="subst">$i</span>&quot;</span>)</span><br><span class="line">            addOne()</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;.start()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">      println(<span class="string">s&quot;inc=<span class="subst">$inc</span>&quot;</span>)</span><br><span class="line">      <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>.sleep(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><em>output</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">run thread 3</span><br><span class="line">run thread 8</span><br><span class="line">run thread 1</span><br><span class="line">run thread 9</span><br><span class="line">run thread 7</span><br><span class="line">run thread 4</span><br><span class="line">run thread 5</span><br><span class="line">run thread 2</span><br><span class="line">run thread 10</span><br><span class="line">run thread 6</span><br><span class="line">inc=0</span><br><span class="line">inc=0</span><br><span class="line">inc=2</span><br><span class="line">inc=3</span><br><span class="line">inc=4</span><br><span class="line">inc=5</span><br><span class="line">inc=6</span><br><span class="line">inc=7</span><br><span class="line">inc=8</span><br><span class="line">inc=8</span><br><span class="line">inc=10</span><br><span class="line">inc=10</span><br><span class="line">inc=10</span><br></pre></td></tr></table></figure>

<h1 id="本文代码"><a href="#本文代码" class="headerlink" title="本文代码"></a>本文代码</h1><p><a href="https://github.com/Liam8/learn-scala">Github仓库</a></p>
<p>转载请注明原文地址：<a href="https://liam-blog.ml/2019/07/21/Scala-Concurrency-in-Practice-2/">https://liam-blog.ml/2019/07/21/Scala-Concurrency-in-Practice-2/</a></p>
]]></content>
      <tags>
        <tag>Scala</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title>Scala并发编程实战 3：Condition 条件变量</title>
    <url>/2019/08/03/Scala-Concurrency-in-Practice-3/</url>
    <content><![CDATA[<p>在<a href="/2019/07/21/Scala-Concurrency-in-Practice-2/">Scala并发编程实战：Lock 锁</a>中我们了解到如何通过Lock来实现互斥操作，但是获取锁之后，如果发现条件不满足（如消费一个队列中的数据时，发现队列为空），线程要如何等待条件满足（如队列不为空）并让出锁呢？这需要用到Condition条件变量，又称作条件队列。</p>
<span id="more"></span>

<p>Condition与JDK内置的管程的等待队列功能类似，主要功能都是让线程在队列上等待被唤醒，但是内置管程的锁只能有一个对应的队列，而一个Lock锁可以有多个Condition队列。</p>
<p>Condition接口的定义如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">Condition</span> &#123;</span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">await</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException;</span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">awaitUninterruptibly</span><span class="params">()</span>;</span><br><span class="line">    <span class="type">long</span> <span class="title function_">awaitNanos</span><span class="params">(<span class="type">long</span> nanosTimeout)</span> <span class="keyword">throws</span> InterruptedException;</span><br><span class="line">    <span class="type">boolean</span> <span class="title function_">await</span><span class="params">(<span class="type">long</span> time, TimeUnit unit)</span> <span class="keyword">throws</span> InterruptedException;</span><br><span class="line">    <span class="type">boolean</span> <span class="title function_">awaitUntil</span><span class="params">(Date deadline)</span> <span class="keyword">throws</span> InterruptedException;</span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">signal</span><span class="params">()</span>;</span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">signalAll</span><span class="params">()</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Condition的await,signal,signalAll方法分别对应Object类的wait,notify,notifyAll方法。</p>
<p>下面这个栗子就是实现一个线程安全的有界队列，队列满的时候，让入队操作的线程wait，队列空的时候就让出队操作的线程wait，当队列情况变化的时候，又能及时唤醒wait状态的线程。<br>队列实现类中创建了两个Condition条件变量notFull和notEmpty。<br>当put元素进入队列时，如果队列已满，则需要等待notFull条件，即在notFull的等待队列等候；<br>当从队列take元素时，如果队列为空，则需要等待notEmpty条件，即在notEmpty的等待队列等候。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"> <span class="class"><span class="keyword">object</span> <span class="title">ConditionDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> queue = <span class="keyword">new</span> <span class="type">BoundedBuffer</span></span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">10</span>) &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">Thread</span> &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">          queue.put(<span class="string">s&quot;Item<span class="subst">$i</span>&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;.start()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">10</span>) &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">Thread</span> &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">          queue.take</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;.start()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">BoundedBuffer</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">val</span> lock = <span class="keyword">new</span> <span class="type">ReentrantLock</span></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">val</span> notFull = lock.newCondition</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">val</span> notEmpty = lock.newCondition</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">val</span> items = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Any</span>](<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">var</span> putptr = <span class="number">0</span></span><br><span class="line">    <span class="keyword">var</span> takeptr = <span class="number">0</span></span><br><span class="line">    <span class="keyword">var</span> count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put</span></span>(x: <span class="type">Any</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      lock.lock()</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">while</span> (count == items.length) &#123;</span><br><span class="line">          notFull.await()</span><br><span class="line">        &#125;</span><br><span class="line">        items(putptr) = x</span><br><span class="line">        putptr += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> (putptr == items.length) &#123;</span><br><span class="line">          putptr = <span class="number">0</span></span><br><span class="line">        &#125;</span><br><span class="line">        count += <span class="number">1</span></span><br><span class="line">        println(<span class="string">s&quot;put <span class="subst">$x</span>&quot;</span>)</span><br><span class="line">        notEmpty.signal()</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        lock.unlock()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">take</span></span>: <span class="type">Any</span> = &#123;</span><br><span class="line">      lock.lock()</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">while</span> (count == <span class="number">0</span>) &#123;</span><br><span class="line">          notEmpty.await()</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> x = items(takeptr)</span><br><span class="line">        takeptr += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> (takeptr == items.length) &#123;</span><br><span class="line">          takeptr = <span class="number">0</span></span><br><span class="line">        &#125;</span><br><span class="line">        count -= <span class="number">1</span></span><br><span class="line">        println(<span class="string">s&quot;take <span class="subst">$x</span>&quot;</span>)</span><br><span class="line">        notFull.signal()</span><br><span class="line">        x</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        lock.unlock()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>output:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">put Item8</span><br><span class="line">put Item10</span><br><span class="line">put Item5</span><br><span class="line">take Item8</span><br><span class="line">put Item7</span><br><span class="line">take Item10</span><br><span class="line">put Item9</span><br><span class="line">take Item5</span><br><span class="line">put Item1</span><br><span class="line">take Item7</span><br><span class="line">put Item4</span><br><span class="line">take Item9</span><br><span class="line">put Item2</span><br><span class="line">take Item1</span><br><span class="line">put Item6</span><br><span class="line">take Item4</span><br><span class="line">put Item3</span><br><span class="line">take Item2</span><br><span class="line">take Item6</span><br><span class="line">take Item3</span><br></pre></td></tr></table></figure>

<h1 id="本文代码"><a href="#本文代码" class="headerlink" title="本文代码"></a>本文代码</h1><p><a href="https://github.com/Liam8/learn-scala">Github仓库</a></p>
]]></content>
      <tags>
        <tag>Scala</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title>Scala并发编程实战 4：Semaphore 信号量模型</title>
    <url>/2019/08/26/Scala-Concurrency-Semaphore/</url>
    <content><![CDATA[<p>Semaphore信号量模型，是一种通过维护计数器数值来控制并发数量的模型，Lock实现的互斥锁只允许一个线程访问临界区，而Semaphore允许有限多个线程访问临界区。</p>
<p>什么情况需要允许多个线程同时访问？最常见的需求就是池化资源，连接池、线程池、对象池等等。</p>
<span id="more"></span>

<p>java.util.concurren.Semaphore 是JDK中的实现类，常用方法有这些：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="title function_">Semaphore</span><span class="params">(<span class="type">int</span> <span class="keyword">permits</span>)</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">acquire</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">release</span><span class="params">()</span>;</span><br></pre></td></tr></table></figure>
<p>构造函数可以传入permits，设置计数器的初始值，表示可以同时获取锁的线程数。<br>acquire函数用于获取锁，release相反。</p>
<p>下面利用Semaphore实现一个对象池，里面可以存放任意需要复用的对象，如果存放的是连接对象，就变成了连接池。<br>下面例子往对象池存放了几个字符串对象，再用若干线程同时去申请这个字符串资源。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> io.github.liam8.con</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Date</span></span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.&#123;<span class="type">ConcurrentLinkedDeque</span>, <span class="type">Semaphore</span>, <span class="type">TimeUnit</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> collection.<span class="type">JavaConversions</span>._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SemaphoreDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> pool = <span class="keyword">new</span> <span class="type">SemaphoreDemo</span>[<span class="type">String</span>](<span class="number">2</span>, <span class="type">List</span>(<span class="string">&quot;aaaa&quot;</span>, <span class="string">&quot;bbbbb&quot;</span>))</span><br><span class="line">    <span class="keyword">val</span> formatDate = (date: <span class="type">Date</span>) =&gt; <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">&quot;HH:mm:ss.SSS&quot;</span>).format(date)</span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">10</span>) &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">Thread</span> &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">          pool.exec &#123; e =&gt;</span><br><span class="line">            println(<span class="string">s&quot;<span class="subst">$&#123;formatDate(new Date)&#125;</span> thread <span class="subst">$i</span> using <span class="subst">$e</span>&quot;</span>)</span><br><span class="line">            <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>.sleep(<span class="number">3</span>)</span><br><span class="line">            println(<span class="string">s&quot;<span class="subst">$&#123;formatDate(new Date)&#125;</span> thread <span class="subst">$i</span> done with <span class="subst">$e</span>&quot;</span>)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;.start()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SemaphoreDemo</span>[<span class="type">T</span>](<span class="params">size: <span class="type">Int</span>, items: <span class="type">List</span>[<span class="type">T</span>]</span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> pool: <span class="type">ConcurrentLinkedDeque</span>[<span class="type">T</span>] = <span class="keyword">new</span> <span class="type">ConcurrentLinkedDeque</span>[<span class="type">T</span>](items)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> semaphore: <span class="type">Semaphore</span> = <span class="keyword">new</span> <span class="type">Semaphore</span>(size)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">exec</span></span>(func: <span class="type">T</span> =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    semaphore.acquire()</span><br><span class="line">    <span class="keyword">val</span> t = pool.pop()</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      func(t)</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      pool.add(t)</span><br><span class="line">      semaphore.release()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>output</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">09:56:52.313 thread 4 using bbbbb</span><br><span class="line">09:56:52.313 thread 2 using aaaa</span><br><span class="line">09:56:55.351 thread 2 done with aaaa</span><br><span class="line">09:56:55.351 thread 4 done with bbbbb</span><br><span class="line">09:56:55.352 thread 1 using aaaa</span><br><span class="line">09:56:55.352 thread 3 using bbbbb</span><br><span class="line">09:56:58.353 thread 1 done with aaaa</span><br><span class="line">09:56:58.353 thread 3 done with bbbbb</span><br><span class="line">09:56:58.353 thread 5 using bbbbb</span><br><span class="line">09:56:58.353 thread 6 using aaaa</span><br><span class="line">09:57:01.354 thread 6 done with aaaa</span><br><span class="line">09:57:01.354 thread 5 done with bbbbb</span><br><span class="line">09:57:01.355 thread 7 using aaaa</span><br><span class="line">09:57:01.355 thread 8 using bbbbb</span><br><span class="line">09:57:04.359 thread 7 done with aaaa</span><br><span class="line">09:57:04.359 thread 8 done with bbbbb</span><br><span class="line">09:57:04.360 thread 9 using aaaa</span><br><span class="line">09:57:04.360 thread 10 using bbbbb</span><br><span class="line">09:57:07.363 thread 9 done with aaaa</span><br><span class="line">09:57:07.363 thread 10 done with bbbbb</span><br></pre></td></tr></table></figure>

<h1 id="本文代码"><a href="#本文代码" class="headerlink" title="本文代码"></a>本文代码</h1><p><a href="https://github.com/Liam8/learn-scala">Github仓库</a></p>
]]></content>
      <tags>
        <tag>Scala</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title>Scala并发编程实战 5：Executor线程池</title>
    <url>/2019/09/22/Scala-Concurrency-Executor/</url>
    <content><![CDATA[<p>创建线程是一个重量级操作，因为需要调用操作系统内核的API，所以最好不要频繁的创建和销毁线程，为了能够复用创建的线程，常用的办法的就是创建线程池。</p>
<span id="more"></span>

<h1 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h1><p>java.util.concurren包中提供了若干接口和类来实现线程池，最常用的有Executor，ExecutorService，ThreadPoolExecutor。</p>
<p>Executor接口很简单定义如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">Executor</span> &#123;</span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">execute</span><span class="params">(Runnable command)</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个接口的目的在于将任务与执行机制解耦，使得用户不需要手动创建线程，只要交给Executor就行了。</p>
<h1 id="ExecutorService"><a href="#ExecutorService" class="headerlink" title="ExecutorService"></a>ExecutorService</h1><p>ExecutorService接口则扩展了Executor接口，增加了若干实用的方法，最常用的两个方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//关闭线程池</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">shutdown</span><span class="params">()</span>;</span><br><span class="line"><span class="comment">//提交Callable任务以获取返回值</span></span><br><span class="line">&lt;T&gt; Future&lt;T&gt; <span class="title function_">submit</span><span class="params">(Callable&lt;T&gt; task)</span>;</span><br></pre></td></tr></table></figure>

<p>AbstractExecutorService抽象类是ExecutorService的实现，实现了若干模板方法。</p>
<p>最重要的类莫过于ThreadPoolExecutor，它是最最常用的ExecutorService实现类，下面重点说说。</p>
<h1 id="ThreadPoolExecutor"><a href="#ThreadPoolExecutor" class="headerlink" title="ThreadPoolExecutor"></a>ThreadPoolExecutor</h1><p>ThreadPoolExecutor在构造时可以指定的参数最多有7个，另外还有3个使用一些默认参数的简化版本。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="title function_">ThreadPoolExecutor</span><span class="params">(<span class="type">int</span> corePoolSize,</span></span><br><span class="line"><span class="params">                          <span class="type">int</span> maximumPoolSize,</span></span><br><span class="line"><span class="params">                          <span class="type">long</span> keepAliveTime,</span></span><br><span class="line"><span class="params">                          TimeUnit unit,</span></span><br><span class="line"><span class="params">                          BlockingQueue&lt;Runnable&gt; workQueue,</span></span><br><span class="line"><span class="params">                          ThreadFactory threadFactory,</span></span><br><span class="line"><span class="params">                          RejectedExecutionHandler handler)</span></span><br></pre></td></tr></table></figure>
<ul>
<li>corePoolSize 是保留的核心线程数，即使线程处于空闲也不会被回收，除非设置了allowCoreThreadTimeOut属性。</li>
<li>maximumPoolSize 最大线程数。当workQueue满了，会给新提交的任务创建新线程，这种情况下线程数会超过corePoolSize，但整个线程池的线程数必须有个上限，就是maximumPoolSize了。</li>
<li>keepAliveTime 回收线程前，允许保留空闲线程的时长。</li>
<li>workQueue 存储提交的任务的队列</li>
<li>threadFactory 创建线程的工厂类(ThreadFactory这个接口就定义了一个方法<code>Thread newThread(Runnable r);</code>)</li>
<li>handler handler用于没有可用线程（线程数达到最大值，没有空闲线程）且workQueue队列满了的时候。</li>
</ul>
<blockquote>
<p>ThreadPoolExecutor 已经提供了以下 4 种策略。<br>CallerRunsPolicy：提交任务的线程自己去执行该任务。<br>AbortPolicy：默认的拒绝策略，会 throws RejectedExecutionException。<br>DiscardPolicy：直接丢弃任务，没有任何异常抛出。<br>DiscardOldestPolicy：丢弃最老的任务，其实就是把最早进入工作队列的任务丢弃，然后把新任务加入到工作队列。</p>
</blockquote>
<p>ThreadPoolExecutory的构造函数一共有四种，使得用户可以省略threadFactory和handler中的一个或两个。</p>
<p><em>需要注意的情况</em><br>当maximumPoolSize&gt;corePoolSize时，如果workQueue满了，新提交的任务会被新线程马上执行，而之前提交的在队列中等待的队列则继续等待。<br>也就是说后提交的任务可能先执行了。<br>当新线程执行完新提交的这个任务后，会转去执行队列中的数据，这时消费任务队列的线程数可能会大于corePoolSize，消费速度加快了。<br>下面做个实验。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> io.github.liam8.con</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.&#123;<span class="type">ArrayBlockingQueue</span>, <span class="type">Callable</span>, <span class="type">Future</span>, <span class="type">RejectedExecutionException</span>, <span class="type">ThreadPoolExecutor</span>, <span class="type">TimeUnit</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ExecutorDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// corePoolSize=1,maximumPoolSize=2,queue capacity=1</span></span><br><span class="line">    <span class="keyword">val</span> executor = <span class="keyword">new</span> <span class="type">ThreadPoolExecutor</span>(</span><br><span class="line">      <span class="number">1</span>,</span><br><span class="line">      <span class="number">2</span>,</span><br><span class="line">      <span class="number">10</span>,</span><br><span class="line">      <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>,</span><br><span class="line">      <span class="keyword">new</span> <span class="type">ArrayBlockingQueue</span>[<span class="type">Runnable</span>](<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> task1 = <span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        println(<span class="string">&quot;task1 running&quot;</span>)</span><br><span class="line">        <span class="type">Thread</span>.sleep(<span class="number">3000</span>)</span><br><span class="line">        println(<span class="string">&quot;task1 complete&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> task2 = <span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        println(<span class="string">&quot;task2 running&quot;</span>)</span><br><span class="line">        <span class="type">Thread</span>.sleep(<span class="number">3000</span>)</span><br><span class="line">        println(<span class="string">&quot;task2 complete&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> task3 = <span class="keyword">new</span> <span class="type">Callable</span>[<span class="type">String</span>] &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">call</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">        println(<span class="string">&quot;task3 running&quot;</span>)</span><br><span class="line">        <span class="type">Thread</span>.sleep(<span class="number">3000</span>)</span><br><span class="line">        println(<span class="string">&quot;task3 complete&quot;</span>)</span><br><span class="line">        <span class="string">&quot;xxx&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> task4 = <span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        println(<span class="string">&quot;task4 running&quot;</span>)</span><br><span class="line">        <span class="type">Thread</span>.sleep(<span class="number">3000</span>)</span><br><span class="line">        println(<span class="string">&quot;task4 complete&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">var</span> task2Result: <span class="type">Future</span>[<span class="type">String</span>] = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">var</span> taskCount = <span class="number">1</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      executor.execute(task1)</span><br><span class="line">      println(<span class="string">&quot;task1 submitted&quot;</span>)</span><br><span class="line">      taskCount += <span class="number">1</span></span><br><span class="line">      executor.execute(task2)</span><br><span class="line">      println(<span class="string">&quot;task2 submitted&quot;</span>)</span><br><span class="line">      taskCount += <span class="number">1</span></span><br><span class="line">      task2Result = executor.submit(task3)</span><br><span class="line">      println(<span class="string">&quot;task3 submitted&quot;</span>)</span><br><span class="line">      taskCount += <span class="number">1</span></span><br><span class="line">      executor.execute(task4)</span><br><span class="line">      println(<span class="string">&quot;task4 submitted&quot;</span>)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">RejectedExecutionException</span> =&gt; println(<span class="string">s&quot;task <span class="subst">$taskCount</span> be rejected&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 起一个线程跟踪线程池大小</span></span><br><span class="line">    <span class="keyword">val</span> th = <span class="keyword">new</span> <span class="type">Thread</span> &#123;</span><br><span class="line">      <span class="keyword">var</span> threadNum = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> =</span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">          <span class="keyword">if</span> (executor.getPoolSize != threadNum) &#123;</span><br><span class="line">            threadNum = executor.getPoolSize</span><br><span class="line">            println(<span class="string">&quot;pool size:&quot;</span> + threadNum)</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="type">Thread</span>.sleep(<span class="number">100</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    th.setDaemon(<span class="literal">true</span>)</span><br><span class="line">    th.start()</span><br><span class="line">    <span class="keyword">if</span> (task2Result != <span class="literal">null</span>) &#123;</span><br><span class="line">      println(task2Result.get(<span class="number">7</span>, <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>))</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">Thread</span>.sleep(<span class="number">5000</span>)</span><br><span class="line">    executor.shutdown()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>output</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">task1 running</span><br><span class="line">task1 submitted</span><br><span class="line">task2 submitted</span><br><span class="line">task3 submitted</span><br><span class="line">task3 running  //task3在task2之前运行了！</span><br><span class="line">task 4 be rejected // 线程数达到最大值，任务队列也满了，task4被拒绝(默认的handler)</span><br><span class="line">pool size:2</span><br><span class="line">task1 complete</span><br><span class="line">task3 complete</span><br><span class="line">xxx</span><br><span class="line">task2 running // 空闲的线程开始消费队列</span><br><span class="line">task2 complete</span><br><span class="line">pool size:0</span><br></pre></td></tr></table></figure>

<h1 id="Executors"><a href="#Executors" class="headerlink" title="Executors"></a>Executors</h1><p>Executors是JUC包中的一个静态工厂类，其中除了newFixedThreadPool，newSingleThreadExecutor方法，其他方法都不推荐使用，因为其他方法创建的线程池使用的是无界队列，可能会占用过多内存，甚至OOM，所以建议使用有界队列。</p>
<h1 id="ExecutionContext"><a href="#ExecutionContext" class="headerlink" title="ExecutionContext"></a>ExecutionContext</h1><p>Scala另外提供了ExecutionContext和Future来简化线程池的使用，Future可以接受一个ExecutionContext类型的隐式参数，将传入的函数提交到ExecutionContext的线程池中运行。<br>下面举个栗子，不做深入探讨。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> io.github.liam8.con</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.<span class="type">Executors</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.concurrent.&#123;<span class="type">Await</span>, <span class="type">ExecutionContext</span>, <span class="type">ExecutionContextExecutorService</span>, <span class="type">Future</span>&#125;</span><br><span class="line"><span class="keyword">import</span> scala.concurrent.duration._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ExecutionContextDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> pool = <span class="type">Executors</span>.newFixedThreadPool(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">implicit</span> <span class="keyword">val</span> ec: <span class="type">ExecutionContextExecutorService</span> = <span class="type">ExecutionContext</span>.fromExecutorService(pool)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> f = <span class="type">Future</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> t = <span class="type">Thread</span>.currentThread().getName</span><br><span class="line">      println(<span class="string">s&quot;<span class="subst">$t</span>: future is coming&quot;</span>)</span><br><span class="line">      <span class="number">123</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> re = f.map(r =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> t = <span class="type">Thread</span>.currentThread().getName</span><br><span class="line">      println(<span class="string">s&quot;<span class="subst">$t</span>: mapping&quot;</span>)</span><br><span class="line">      r * r</span><br><span class="line">    &#125;)</span><br><span class="line">    re.onSuccess &#123; <span class="keyword">case</span> x: <span class="type">Int</span> =&gt; println(x) &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">Await</span>.result(f, <span class="number">3.</span>seconds)</span><br><span class="line">    ec.shutdown()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>output </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pool-1-thread-1: future is coming</span><br><span class="line">pool-1-thread-2: mapping</span><br><span class="line">15129</span><br></pre></td></tr></table></figure>

<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://time.geekbang.org/column/article/90771">Executor与线程池：如何创建正确的线程池？</a></p>
<p><a href="https://www.freecodecamp.org/news/futures-made-easy-with-scala-da1beb3bb281/">Futures Made Easy with Scala</a></p>
<h1 id="本文代码"><a href="#本文代码" class="headerlink" title="本文代码"></a>本文代码</h1><p><a href="https://github.com/Liam8/learn-scala">Github仓库</a></p>
<p>转载请注明原文地址：<a href="https://liam-blog.ml/2019/09/22/Scala-Concurrency-Executor/">https://liam-blog.ml/2019/09/22/Scala-Concurrency-Executor/</a></p>
]]></content>
      <tags>
        <tag>Scala</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title>Scala implicit 隐式转换安全驾驶指南</title>
    <url>/2019/09/28/scala-implicit/</url>
    <content><![CDATA[<p>这篇短文将结合实例对隐式转换的各种场景进行解释和总结，希望看完的人能够安全驶过隐式转换这个大坑。</p>
<span id="more"></span>

<h1 id="隐式转换函数"><a href="#隐式转换函数" class="headerlink" title="隐式转换函数"></a>隐式转换函数</h1><p>隐式转换函数有两种作用场景。</p>
<ul>
<li>1 转换为期望类型：就是指一旦编译器看到X，但需要Y，就会检查从X到Y的隐式转换函数。</li>
<li>2 转换方法的调用者：简单来说，如obj.f()，如果obj对象没有f方法，则尝试将obj转换为拥有f方法的类型。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ImpFunction</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">Dog</span>(<span class="params">val name: <span class="type">String</span></span>) </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bark</span></span>(): <span class="type">Unit</span> = println(<span class="string">s&quot;<span class="subst">$name</span> say: Wang !&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">double2int</span></span>(d: <span class="type">Double</span>): <span class="type">Int</span> = d.toInt</span><br><span class="line"></span><br><span class="line">  <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">string2Dog</span></span>(s: <span class="type">String</span>): <span class="type">Dog</span> = <span class="keyword">new</span> <span class="type">Dog</span>(s)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> f: <span class="type">Int</span> = <span class="number">1.1</span> <span class="comment">//转换为期望类型,1.1通过double2int转成了Int类型</span></span><br><span class="line"></span><br><span class="line">  println(f)</span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;Teddy&quot;</span>.bark() <span class="comment">// 转换方法的调用者,字符串通过string2Dog转成了Dog, 于是有了bark方法</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// output</span></span><br><span class="line"><span class="comment">// 1</span></span><br><span class="line"><span class="comment">// Teddy say: Wang !</span></span><br></pre></td></tr></table></figure>

<p><code>val f: Int = 1.1</code> 因为类型不匹配，这段本来是无法通过编译的，但是编译器发现存在一个Double至Int的隐式转换函数，所以进行了隐式转换。</p>
<p><code>&quot;Teddy&quot;.bark()</code> String类型本来是没有bark方法的，但是编译器发现了隐式转换string2Dog可以使得String转成一种拥有bark方法的类型，相当于进行了这样的转换：<code>string2Dog(&quot;Teddy&quot;).bark()</code>。</p>
<h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><p>需要注意的是，编译器只关心隐式转换函数的输入输出类型，不关心函数名，为避免歧义，同一个作用域中不能有输入输出类型相同的两个隐式转换函数，不然编译器会报错。</p>
<h1 id="隐式类"><a href="#隐式类" class="headerlink" title="隐式类"></a>隐式类</h1><p>Scala 2.10引入了一种叫做隐式类的新特性。隐式类指的是用implicit关键字修饰的类。使用情况与隐式转换函数类似，<strong>可以看做将类的构造函数定义为隐式转换函数</strong>，返回类型就是这个类。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> io.github.liam8.impl</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ImpClass</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">implicit</span> <span class="class"><span class="keyword">class</span> <span class="title">Dog</span>(<span class="params">val name: <span class="type">String</span></span>) </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bark</span></span>(): <span class="type">Unit</span> = println(<span class="string">s&quot;<span class="subst">$name</span> say: Wang !&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;Teddy&quot;</span>.bark()</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h2 id="注意事项-1"><a href="#注意事项-1" class="headerlink" title="注意事项"></a>注意事项</h2><p><em>这段来自官网<a href="https://docs.scala-lang.org/zh-cn/overviews/core/implicit-classes.html">IMPLICIT CLASSES</a></em><br>隐式类有以下限制条件：</p>
<ul>
<li><p>1 只能在别的trait&#x2F;类&#x2F;对象内部定义。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Helpers</span> </span>&#123;</span><br><span class="line">   <span class="keyword">implicit</span> <span class="class"><span class="keyword">class</span> <span class="title">RichInt</span>(<span class="params">x: <span class="type">Int</span></span>) <span class="comment">// 正确！</span></span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">implicit</span> <span class="class"><span class="keyword">class</span> <span class="title">RichDouble</span>(<span class="params">x: <span class="type">Double</span></span>) <span class="comment">// 错误！</span></span></span><br></pre></td></tr></table></figure>
</li>
<li><p>2 构造函数只能携带一个非隐式参数。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">implicit</span> <span class="class"><span class="keyword">class</span> <span class="title">RichDate</span>(<span class="params">date: java.util.<span class="type">Date</span></span>) <span class="comment">// 正确！</span></span></span><br><span class="line"><span class="keyword">implicit</span> <span class="class"><span class="keyword">class</span> <span class="title">Indexer</span>[<span class="type">T</span>](<span class="params">collecton: <span class="type">Seq</span>[<span class="type">T</span>], index: <span class="type">Int</span></span>) <span class="comment">// 错误！</span></span></span><br><span class="line"><span class="keyword">implicit</span> <span class="class"><span class="keyword">class</span> <span class="title">Indexer</span>[<span class="type">T</span>](<span class="params">collecton: <span class="type">Seq</span>[<span class="type">T</span>]</span>)(<span class="params">implicit index: <span class="type">Index</span></span>) <span class="comment">// 正确！</span></span></span><br></pre></td></tr></table></figure>
<p>虽然我们可以创建带有多个非隐式参数的隐式类，但这些类无法用于隐式转换。</p>
</li>
<li><p>3 在同一作用域内，不能有任何方法、成员或对象与隐式类同名。<br>注意：这意味着隐式类不能是case class。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Bar</span></span></span><br><span class="line"><span class="keyword">implicit</span> <span class="class"><span class="keyword">class</span> <span class="title">Bar</span>(<span class="params">x: <span class="type">Int</span></span>) <span class="comment">// 错误！</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> x = <span class="number">5</span></span><br><span class="line"><span class="keyword">implicit</span> <span class="class"><span class="keyword">class</span> <span class="title">x</span>(<span class="params">y: <span class="type">Int</span></span>) <span class="comment">// 错误！</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">implicit</span> <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Baz</span>(<span class="params">x: <span class="type">Int</span></span>) <span class="comment">// 错误！</span></span></span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="隐式参数-amp-隐式值"><a href="#隐式参数-amp-隐式值" class="headerlink" title="隐式参数 &amp; 隐式值"></a>隐式参数 &amp; 隐式值</h1><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> io.github.liam8.impl</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ImpParam</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">bark</span></span>(<span class="keyword">implicit</span> name: <span class="type">String</span>): <span class="type">Unit</span> = println(<span class="string">s&quot;<span class="subst">$name</span> say: Wang !&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> t: <span class="type">String</span> = <span class="string">&quot;Hot Dog&quot;</span></span><br><span class="line"></span><br><span class="line">  bark</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>参数加上implicit就成了隐式参数，需要与隐式值(变量定义加上implicit)搭配使用，最后一行的<code>bark</code>缺少了一个String类型的参数，编译器找到了String类型的隐式值，便将其传入，相当于执行了<code>bark(t)</code>。</p>
<p>implicit关键字会作用于函数列表中的的所有参数，如<code>def test(implicit x:Int, y: Double)</code>这样定义函数，x和y就都成了隐式函数。但是通常我们只希望部分参数为隐式参数，就好比通常会给部分参数提供默认值而不是全部都指定默认值，于是隐式参数常常与柯里化函数一起使用，这样可以使得只有最后一个参数为隐式参数，例如<code>def test(x: Int)(implicit y: Double)</code>。</p>
<p>👇是完整的例子。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ImpParamWithCurry</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">bark</span></span>(name: <span class="type">String</span>)(<span class="keyword">implicit</span> word: <span class="type">String</span>): <span class="type">Unit</span> = println(<span class="string">s&quot;<span class="subst">$name</span> say: <span class="subst">$word</span> !&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> w: <span class="type">String</span> = <span class="string">&quot;Wang&quot;</span></span><br><span class="line"></span><br><span class="line">  bark(<span class="string">&quot;Hot Dog&quot;</span>)</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="注意事项-2"><a href="#注意事项-2" class="headerlink" title="注意事项"></a>注意事项</h2><p><em>下面这段来自<a href="https://blog.csdn.net/m0_37138008/article/details/78120210">scala的隐式转换学习总结（详细）</a></em></p>
<ul>
<li>1）当函数没有柯里化时，implicit关键字会作用于函数列表中的的所有参数。</li>
<li>2）隐式参数使用时要么全部不指定，要么全不指定，不能只指定部分。</li>
<li>3）同类型的隐式值只能在作用域内出现一次，即不能在同一个作用域中定义多个相同类型的隐式值。</li>
<li>4）在指定隐式参数时，implicit 关键字只能出现在参数开头。</li>
<li>5）如果想要实现参数的部分隐式参数，只能使用函数的柯里化，<br>          如要实现这种形式的函数，def test(x:Int, implicit  y: Double)的形式，必须使用柯里化实现：def test(x: Int)(implicit y: Double).</li>
<li>6）柯里化的函数， implicit 关键字只能作用于最后一个参数。否则，不合法。</li>
<li>7）implicit 关键字在隐式参数中只能出现一次，柯里化的函数也不例外！</li>
</ul>
<h1 id="隐式对象"><a href="#隐式对象" class="headerlink" title="隐式对象"></a>隐式对象</h1><p>类似于隐式值, 要结合隐式参数使用。先看一个栗子(下面的代码需要认真体会)。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> io.github.liam8.impl</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ImpObject</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//定义一个`排序器`接口，能够比较两个相同类型的值的大小</span></span><br><span class="line">  <span class="class"><span class="keyword">trait</span> <span class="title">Ordering</span>[<span class="type">T</span>] </span>&#123;</span><br><span class="line">    <span class="comment">//如果x&lt;y返回-1，x&gt;y返回1，x==y则返回0.</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: <span class="type">T</span>, y: <span class="type">T</span>): <span class="type">Int</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//实现一个Int类型的排序器</span></span><br><span class="line">  <span class="keyword">implicit</span> <span class="class"><span class="keyword">object</span> <span class="title">IntOrdering</span> <span class="keyword">extends</span> <span class="title">Ordering</span>[<span class="type">Int</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: <span class="type">Int</span>, y: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">      <span class="keyword">if</span> (x &lt; y) <span class="number">-1</span></span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span> (x == y) <span class="number">0</span></span><br><span class="line">      <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//实现一个String类型的排序器</span></span><br><span class="line">  <span class="keyword">implicit</span> <span class="class"><span class="keyword">object</span> <span class="title">StringOrdering</span> <span class="keyword">extends</span> <span class="title">Ordering</span>[<span class="type">String</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: <span class="type">String</span>, y: <span class="type">String</span>): <span class="type">Int</span> = x.compareTo(y)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//一个通用的max函数</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">max</span></span>[<span class="type">T</span>](x: <span class="type">T</span>, y: <span class="type">T</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">T</span> = &#123;</span><br><span class="line">      <span class="keyword">if</span> (ord.compare(x, y) &gt;= <span class="number">0</span>) x <span class="keyword">else</span> y</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  println(max(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">  println(max(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//output: </span></span><br><span class="line"><span class="comment">// 2</span></span><br><span class="line"><span class="comment">// b</span></span><br></pre></td></tr></table></figure>
<p>max函数的作用显然是返回x和y中的最大值，但是x和y的值类型不是固定的，max不知道如何比较x和y的大型，于是定义了一个隐式参数<code>implicit ord: Ordering[T]</code>，希望能传入一个Ordering[T]类型的排序器帮助进行x和y的比较。</p>
<p>在调用<code>max(1, 2)</code>的时候，编译器发现需要一个Ordering[Int]类型的参数，刚好<code>implicit object IntOrdering</code>定义了一个隐式对象符合要求，于是被用来传入max函数。</p>
<p>隐式对象跟上面的隐式值非常相似，只是类型特殊而已。</p>
<p>在Scala中scala.math.Ordering很常用的内置特质，如果你理解了这段代码，也就大致理解了Ordering的原理。</p>
<h1 id="上下文界定-context-bounds"><a href="#上下文界定-context-bounds" class="headerlink" title="上下文界定(context bounds)"></a>上下文界定(context bounds)</h1><p>这是一种隐式参数的语法糖。</p>
<p>再看上面隐式对象的例子，如果要添加一个min函数，大致就是这样</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">min</span></span>[<span class="type">T</span>](x: <span class="type">T</span>, y: <span class="type">T</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">T</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (ord.compare(x, y) &gt;= <span class="number">0</span>) y <span class="keyword">else</span> x</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>但是max和min函数的参数都比较长，于是出现了一种简化的写法</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">min</span></span>[<span class="type">T</span>: <span class="type">Ordering</span>](x: <span class="type">T</span>, y: <span class="type">T</span>): <span class="type">T</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> ord = implicitly[<span class="type">Ordering</span>[<span class="type">T</span>]]</span><br><span class="line">  <span class="keyword">if</span> (ord.compare(x, y) &gt;= <span class="number">0</span>) y <span class="keyword">else</span> x</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>[T: Ordering]</code>这种语法就叫上下文界定，含义是上下文中必须有一个Ordering[T]类型的隐式值，这个值会被传入min函数。但是由于这个隐式值并没有明确赋值给某个变量，没法直接使用它，所以需要一个implicitly函数把隐式值取出来。</p>
<p>implicitly函数的定义非常简单，作用就是将T类型的隐含值返回：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="meta">@inline</span> <span class="function"><span class="keyword">def</span> <span class="title">implicitly</span></span>[<span class="type">T</span>](<span class="keyword">implicit</span> e: <span class="type">T</span>) = e</span><br></pre></td></tr></table></figure>

<h1 id="视界"><a href="#视界" class="headerlink" title="视界"></a>视界</h1><p>这个语法已经被废弃了，但是你还是可能会看到，简单解释下。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">min</span></span>[<span class="type">T</span> &lt;% <span class="type">Ordered</span>[<span class="type">T</span>]](x: <span class="type">T</span>, y: <span class="type">T</span>): <span class="type">T</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (x &gt; y) y <span class="keyword">else</span> x</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>视界的定义<code>T &lt;% Ordered[T]</code>的含义是T可以被隐式转换成Ordered[T]，这也是为什么<code>x &gt; y</code>可以编译通过。</p>
<p>上面的写法其实等同于下面这样，所以视界的语法不能用了也不要紧。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">min</span></span>[<span class="type">T</span>](x: <span class="type">T</span>, y: <span class="type">T</span>)(<span class="keyword">implicit</span> c: <span class="type">T</span> =&gt; <span class="type">Ordered</span>[<span class="type">T</span>]): <span class="type">T</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (x &gt; y) y <span class="keyword">else</span> x</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h1 id="隐式转换机制"><a href="#隐式转换机制" class="headerlink" title="隐式转换机制"></a>隐式转换机制</h1><h2 id="隐式转换通用规则"><a href="#隐式转换通用规则" class="headerlink" title="隐式转换通用规则"></a>隐式转换通用规则</h2><ul>
<li><p>标记规则：只有标记为implicit的定义才是可用的。</p>
</li>
<li><p>作用域规则：插入的隐式转换必须以单一标识符的形式处于作用域中，或与转换的源或目标类型关联在一起。</p>
</li>
</ul>
<p>单一标识符意思是不能插入形式为someVariable.convert(x)的转换，只能是convert(x)。<br>单一标识符规则有个例外，编译器还将在源类型或转换的期望目标类型的伴生对象中寻找隐式定义。</p>
<p>有点难理解?看个例子!</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> io.github.liam8.impl</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ImpCompObject</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">object</span> <span class="title">Dog</span> </span>&#123;</span><br><span class="line">    <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">dogToCat</span></span>(d: <span class="type">Dog</span>) = <span class="keyword">new</span> <span class="type">Cat</span>(d.name)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">Cat</span>(<span class="params">val name: <span class="type">String</span></span>) </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">miao</span></span>(): <span class="type">Unit</span> = println(<span class="string">s&quot;<span class="subst">$name</span> say: Miao !&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">Dog</span>(<span class="params">val name: <span class="type">String</span></span>) </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bark</span></span>(): <span class="type">Unit</span> = println(<span class="string">s&quot;<span class="subst">$name</span> say: Wang !&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">new</span> <span class="type">Dog</span>(<span class="string">&quot;Teddy&quot;</span>).miao()</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//Teddy say: Miao !</span></span><br></pre></td></tr></table></figure>
<p>当前作用域中没有定义和引入隐式函数，但是在Dog的伴生对象中找到了，所以Dog可以被转成Cat，这个跟上下文没有关系，而是Dog自带技能。</p>
<ul>
<li><p>无歧义规则：隐式转换唯有不存在其他转换的前提下有效。</p>
</li>
<li><p>单一调用规则：只会尝试一个隐式操作。</p>
</li>
<li><p>显示操作先行规则：若编写的代码类型检查无误，则不会尝试隐式操作。</p>
</li>
</ul>
<h2 id="转换时机"><a href="#转换时机" class="headerlink" title="转换时机"></a>转换时机</h2><ul>
<li>当类型与目标类型不一致时</li>
<li>当对象调用类中不存在的方法或成员时</li>
<li>缺少隐式参数时</li>
</ul>
<p>也即是能用到隐式操作的有三个地方：转换为期望类型、指定（方法）调用者的转换、隐式参数。</p>
<h2 id="转换机制"><a href="#转换机制" class="headerlink" title="转换机制"></a>转换机制</h2><p><em>这段来自<a href="https://www.cnblogs.com/MOBIN/p/5351900.html">深入理解Scala的隐式转换</a></em></p>
<p>即编译器是如何查找到缺失信息的，解析具有以下两种规则：</p>
<ul>
<li><p>1.首先会在当前代码作用域下查找隐式实体（隐式方法  隐式类 隐式对象）</p>
</li>
<li><p>2.如果第一条规则查找隐式实体失败，会继续在隐式参数的类型的作用域里查找<br>类型的作用域是指与该类型相关联的全部伴生模块，一个隐式实体的类型T它的查找范围如下：</p>
<ul>
<li>1 如果T被定义为T with A with B with C,那么A,B,C都是T的部分，在T的隐式解析过程中，它们的伴生对象都会被搜索</li>
<li>2 如果T是参数化类型，那么类型参数和与类型参数相关联的部分都算作T的部分，比如List[String]的隐式搜索会搜索List的伴生对象和String的伴生对象</li>
<li>3  如果T是一个单例类型p.T，即T是属于某个p对象内，那么这个p对象也会被搜索</li>
<li>4  如果T是个类型注入S#T，那么S和T都会被搜索</li>
</ul>
</li>
</ul>
<h1 id="上路前的话"><a href="#上路前的话" class="headerlink" title="上路前的话"></a>上路前的话</h1><p>这段话来自《Scala编程》</p>
<blockquote>
<p>隐式操作若过于频繁使用，会让代码变得晦涩难懂。因此，在考虑添加新的隐式转换之前，请首先自问是否能够通过其他手段，诸如继承、混入组合或方法重载，达到同样的目的。如果所有这些都不能成功，并且你感觉代码仍有一些繁复和冗余，那么隐式操作或许正好能帮到你。</p>
</blockquote>
<p>所以。。。谨慎使用，小心翻车，good luck!</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://docs.scala-lang.org/zh-cn/overviews/core/implicit-classes.html">IMPLICIT CLASSES</a></p>
<p><a href="https://blog.csdn.net/m0_37138008/article/details/78120210">scala的隐式转换学习总结（详细）</a></p>
<p>《Scala编程》</p>
<p><a href="https://www.cnblogs.com/MOBIN/p/5351900.html">深入理解Scala的隐式转换</a></p>
<h1 id="本文代码"><a href="#本文代码" class="headerlink" title="本文代码"></a>本文代码</h1><p><a href="https://github.com/Liam8/learn-scala">Github仓库</a></p>
<p>转载请注明原文地址：<a href="https://liam-blog.ml/2019/09/28/scala-implicit/">https://liam-blog.ml/2019/09/28/scala-implicit/</a></p>
]]></content>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark Core解析 1：RDD 弹性分布式数据集</title>
    <url>/2019/10/23/spark-core-rdd/</url>
    <content><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>Spark Core是Spark的核心部分，是Spark SQL，Spark Streaming，Spark MLlib等等其他模块的基础, Spark Core提供了开发分布式应用的脚手架，使得其他模块或应用的开发者不必关心复杂的分布式计算如何实现，只需使用Spark Core提供的分布式数据结构RDD及丰富的算子API，以类似开发单机应用的方式来进行开发。</p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/spark.png" alt="spark.png"></p>
<p>图中最下面那个就是Spark Core啦，日常使用的RDD相关的API就属于Spark Core，而Dataset、DataFrame则属于Spark SQL。</p>
<span id="more"></span>

<h1 id="RDD-概览"><a href="#RDD-概览" class="headerlink" title="RDD 概览"></a>RDD 概览</h1><p>本文基于Spark 2.x。</p>
<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>RDD (Resilient Distributed Dataset，弹性分布式数据集)：</p>
<ul>
<li>Resilient：不可变的、容错的</li>
<li>Distributed：数据分散在不同节点（机器，进程）</li>
<li>Dataset：一个由多个分区组成的数据集</li>
</ul>
<h2 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h2><p>In-Memory：RDD会优先使用内存<br>Immutable（Read-Only）：一旦创建不可修改<br>Lazy evaluated：惰性执行<br>Cacheable：可缓存，可复用<br>Parallel：可并行处理<br>Typed：强类型，单一类型数据<br>Partitioned：分区的<br>Location-Stickiness：可指定分区优先使用的节点</p>
<p>是Spark中最核心的数据抽象，数据处理和计算基本都是基于RDD。</p>
<h2 id="组成"><a href="#组成" class="headerlink" title="组成"></a>组成</h2><p>一个RDD通常由5个要素组成：</p>
<ul>
<li>一组分区(partition)</li>
<li>一个计算函数</li>
<li>一组依赖(直接依赖的父RDD)</li>
<li>一个分区器 (可选) </li>
<li>一组优先计算位置(e.g. 将Task分配至靠近HDFS块的节点进行计算) (可选)</li>
</ul>
<p>与传统数据结构对比，只关心访问，不关心存储。通过迭代器访问数据，只要数据能被不重复地访问即可。</p>
<h2 id="算子"><a href="#算子" class="headerlink" title="算子"></a>算子</h2><p>算子，即对RDD进行变换的操作，按照是否触发Job提交可以分为两大类：</p>
<ul>
<li>transformation：不会立即执行的一类变换，不会触发Job执行，会生成并返回新的RDD，同时记录下依赖关系。如：map,filter,union,join,reduceByKey。</li>
<li>action: 会立即提交Job的一类变换，不会返回新的RDD，而是直接返回计算结果。如：count,reduce,foreach。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/20191022162739.png" alt="20191022162739.png"></p>
<h3 id="算子与RDD的关系"><a href="#算子与RDD的关系" class="headerlink" title="算子与RDD的关系"></a>算子与RDD的关系</h3><p>transformation类型的算子通常都会返回新的RDD，虽然只返回一个新的RDD给用户，但是在RDD的血缘关系图(RDD linage)中，有可能新增了多个RDD。</p>
<p>先看算子与RDD一对一的情况：<br>map &#x3D;&gt; MapPartitionsRDD<br>filter &#x3D;&gt; MapPartitionsRDD<br>reduceByKey &#x3D;&gt; ShuffledRDD<br>…</p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/reduceByKey.png" alt="reduceByKey.png"></p>
<p>一对多：<br>join &#x3D;&gt;  CoGroupedRDD-&gt;MapPartitionsRDD-&gt;MapPartitionsRDD<br>distinct &#x3D;&gt; MapPartitionsRDD-&gt;ShuffledRDD-&gt;MapPartitionsRDD<br>…</p>
<p>一个算子生成的多个RDD，也不一定归属于同一个stage，例如distinct算子，生成的第一个MapPartitionsRDD归属于前一个stage，其他的则归属于后一个stage，其中产生了一次shuffle。</p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/distinct-1.png" alt="distinct-1.png"></p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/distinct-2.png" alt="distinct-2.png"></p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/distinct-3.png" alt="distinct-3.png"></p>
<h1 id="Partition-amp-Partitioner"><a href="#Partition-amp-Partitioner" class="headerlink" title="Partition &amp; Partitioner"></a>Partition &amp; Partitioner</h1><p><strong>为什么要把数据分区？</strong><br>把数据分成若干partition是为了将数据分散到不同节点不同线程，从而能进行分布式的多线程的并行计算。</p>
<p><strong>按什么规则分区？</strong><br>RDD从数据源生成的时候，数据通常是随机分配到不同的partition或者保持数据源的分区，如sc.parallelize(…)，sc.textFile(…)。</p>
<p>这对于某些RDD操作来说是没有问题的，比如filter(),map(),flatMap()，rdd.union(otherRDD)，rdd.intersection(otherRDD)，<br>rdd.subtract(otherRDD)。</p>
<p>但是对于reduceByKey(),foldByKey(),combineByKey(),groupByKey()，sortByKey()，cogroup(), join() ,leftOuterJoin(), rightOuterJoin()这些操作，随机分配分区就非常不友好，会带来很多额外的网络传输。影响一个分布式计算系统性能的最大敌人就是网络传输，所以必须尽量最小化网络传输。</p>
<p><strong>为了减少网络传输，怎么分区才合理？</strong><br>对于reduceByKey操作应该把相同key的数据放到同一分区；<br>对于sortByKey操作应该把同一范围的数据放到同一分区。</p>
<p>可见不同的操作适合不同的数据分区规则，Spark将划分规则抽象为<em>Partitioner(分区器)</em> ，分区器的核心作用是决定数据应归属的分区，本质就是计算数据对应的分区ID。</p>
<p>在Spark Core中内置了2个Partitioner来支持常用的分区规则(Spark MLlib,Spark SQL中有其他的)。</p>
<ul>
<li>HashPartitioner 哈希分区器</li>
<li>RangePartitioner 范围分区器</li>
</ul>
<h2 id="HashPartitioner"><a href="#HashPartitioner" class="headerlink" title="HashPartitioner"></a>HashPartitioner</h2><p>哈希分区器是默认的分区器，也是使用最广泛的一个，作用是将数据按照key的hash值进行分区。</p>
<p>分区ID计算公式非常简单：<code>key的hash值 % 分区个数</code> ， 如果key为null，则返回0.</p>
<p>也就是将key的hash值(Java中每个对象都有hash code,对象相等则hash code相同)，除以分区个数，取余数为分区ID，这样能够保证相同Key的数据被分到同一个分区，但是每个分区的数据量可能会相差很大，出现数据倾斜。</p>
<h2 id="RangePartitioner"><a href="#RangePartitioner" class="headerlink" title="RangePartitioner"></a>RangePartitioner</h2><p>RangePartitioner的作用是根据key，将数据按范围大致平均的分到各个分区，只支持能排序的key。</p>
<p>要知道一个key属于哪个分区，需要知道每个分区的边界值。<br>确定边界值需要对数据进行排序，因为数据量通常较大，通过样本替代总体来估计每个分区的边界值。</p>
<p>采样流程：</p>
<ul>
<li><ol>
<li>使用水塘抽样对总体进行采样；</li>
</ol>
</li>
<li><ol start="2">
<li>针对数据量远超平均值的分区，进行传统抽样(伯努利抽样)。</li>
</ol>
</li>
</ul>
<p>使用场景：sortByKey</p>
<p>扩展问题：</p>
<h2 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h2><p>对于一个没有明确指定Partitioner的情况下，<br>reduceByKey(),foldByKey(),combineByKey(),groupByKey()等操作会默认使用HashPartitioner。<br>sortByKey操作会采用RangePartitioner。</p>
<p>reduceByKey也有一个可以自定义分区器的版本：<code>reduceByKey(partitioner: Partitioner, func: (V, V) =&gt; V)</code></p>
<h1 id="Function"><a href="#Function" class="headerlink" title="Function"></a>Function</h1><h2 id="传入给transformation的函数"><a href="#传入给transformation的函数" class="headerlink" title="传入给transformation的函数"></a>传入给transformation的函数</h2><p>transformation会生成新的RDD，传给RDD transformation的函数最终会以成员变量的形式存储在新生成的RDD中。</p>
<p>以map函数为例。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> r11 = r00.map(n =&gt; (n, n))</span><br></pre></td></tr></table></figure>

<p>map函数接受的参数类型为<code>f: T =&gt; U</code>，因为Scala支持函数式编程，函数可以像值一样存储在变量中，也可以作为参数传递。<br>f参数的类型<code>T =&gt; U</code>代表一种函数类型，这个函数的输入参数的类型必须为T，输出类型为U，这里T和U都是泛型，T代表RDD中数据的类型，对于RDD[String]来说，T就是String。</p>
<p>最终f参数，会转换成有关迭代器的一个函数，存储到RDD的f成员变量中。</p>
<p>最终存储的类型为：<br><code>f: (TaskContext, Int, Iterator[T]) =&gt; Iterator[U]</code><br>对于map来说是这样一个函数<br><code>(context, pid, iter) =&gt; iter.map(f)</code><br>也就是说我们传入到RDD.map的f函数，最终传给了Iterator.map函数。</p>
<h2 id="传入给action的函数"><a href="#传入给action的函数" class="headerlink" title="传入给action的函数"></a>传入给action的函数</h2><p>action不会生成新的RDD，而是将函数传递给Job。</p>
<h1 id="Dependency"><a href="#Dependency" class="headerlink" title="Dependency"></a>Dependency</h1><p>当RDD1经过transformation生成了RDD2，就称作RDD2依赖RDD1，RDD1是RDD2的父RDD，他们是父子关系。</p>
<p>先看一个例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> r00 = sc.parallelize(<span class="number">0</span> to <span class="number">9</span>)</span><br><span class="line"><span class="keyword">val</span> r01 = sc.parallelize(<span class="number">0</span> to <span class="number">90</span> by <span class="number">10</span>)</span><br><span class="line"><span class="keyword">val</span> r10 = r00 cartesian r01</span><br><span class="line"><span class="keyword">val</span> r11 = r00.map(n =&gt; (n, n))</span><br><span class="line"><span class="keyword">val</span> r12 = r00 zip r01</span><br><span class="line"><span class="keyword">val</span> r13 = r01.keyBy(_ / <span class="number">20</span>)</span><br><span class="line"><span class="keyword">val</span> r20 = <span class="type">Seq</span>(r11, r12, r13).foldLeft(r10)(_ union _)</span><br></pre></td></tr></table></figure>

<p>我们看下RDD之间的依赖关系图</p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/20191022162858.png" alt="20191022162858.png"></p>
<p>RDD的依赖关系网又叫RDD的血统(lineage)，可以看做是RDD的逻辑执行计划。</p>
<p>同义词：RDD lineage，RDD operator graph，RDD dependency graph</p>
<h2 id="Dependency存储"><a href="#Dependency存储" class="headerlink" title="Dependency存储"></a>Dependency存储</h2><p>父RDD与子RDD之间的依赖关系记录在<strong>子RDD</strong>的属性中(<code>deps: Seq[Dependency[_]]</code>)，数据类型为Dependency(可以有多个)，Dependency中保存了父RDD的引用，这样通过Dependency就能找到父RDD。</p>
<h2 id="Dependency分类"><a href="#Dependency分类" class="headerlink" title="Dependency分类"></a>Dependency分类</h2><p>Dependency不仅描述了RDD之间的依赖关系，还进一步描述了不同RDD的partition之间的依赖关系。</p>
<p>依据partition之间依赖关系的不同Dependency分为两大类：</p>
<ul>
<li>NarrowDependency 窄依赖，1个父分区只对应1个子分区，这时父RDD不需要改变分区方式。如：map、filter、union，co-paritioned join</li>
<li>ShuffleDependency Shuffle依赖(宽依赖)，1个父分区对应多个子分区，这种情况父RDD必须重新分区，才能符合子RDD的需求。如：groupByKey、reduceByKey、sortByKey，（not co-paritioned）join</li>
</ul>
<h3 id="NarrowDependency"><a href="#NarrowDependency" class="headerlink" title="NarrowDependency"></a>NarrowDependency</h3><p>NarrowDependency是一个抽象类，一共有3中实现类，也就是说有3种NarrowDependency。</p>
<ul>
<li>OneToOneDependency：一对一依赖，比如map,</li>
<li>RangeDependency：范围依赖，如 union</li>
<li>PruneDependency：裁剪依赖，过滤掉部分分区，如PartitionPruningRDD</li>
</ul>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/20191022163015.png" alt="20191022163015.png"></p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/20191022163033.png" alt="20191022163033.png"></p>
<h3 id="ShuffleDependency"><a href="#ShuffleDependency" class="headerlink" title="ShuffleDependency"></a>ShuffleDependency</h3><p>出现shuffle依赖表示父RDD与子RDD的分区方式发生了变化。</p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/20191022163106.png" alt="20191022163106.png"></p>
<h1 id="RDD分类"><a href="#RDD分类" class="headerlink" title="RDD分类"></a>RDD分类</h1><p>RDD的具体实现类有几十种(大概60+)，介绍下最常见的几种。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; r20.toDebugString</span><br><span class="line">res34: <span class="type">String</span> =</span><br><span class="line">(<span class="number">28</span>) <span class="type">UnionRDD</span>[<span class="number">38</span>] at union at &lt;pastie&gt;:<span class="number">31</span> []</span><br><span class="line"> |   <span class="type">UnionRDD</span>[<span class="number">37</span>] at union at &lt;pastie&gt;:<span class="number">31</span> []</span><br><span class="line"> |   <span class="type">UnionRDD</span>[<span class="number">36</span>] at union at &lt;pastie&gt;:<span class="number">31</span> []</span><br><span class="line"> |   <span class="type">CartesianRDD</span>[<span class="number">32</span>] at cartesian at &lt;pastie&gt;:<span class="number">27</span> []</span><br><span class="line"> |   <span class="type">ParallelCollectionRDD</span>[<span class="number">30</span>] at parallelize at &lt;pastie&gt;:<span class="number">25</span> []</span><br><span class="line"> |   <span class="type">ParallelCollectionRDD</span>[<span class="number">31</span>] at parallelize at &lt;pastie&gt;:<span class="number">26</span> []</span><br><span class="line"> |   <span class="type">MapPartitionsRDD</span>[<span class="number">33</span>] at map at &lt;pastie&gt;:<span class="number">28</span> []</span><br><span class="line"> |   <span class="type">ParallelCollectionRDD</span>[<span class="number">30</span>] at parallelize at &lt;pastie&gt;:<span class="number">25</span> []</span><br><span class="line"> |   <span class="type">ZippedPartitionsRDD2</span>[<span class="number">34</span>] at zip at &lt;pastie&gt;:<span class="number">29</span> []</span><br><span class="line"> |   <span class="type">ParallelCollectionRDD</span>[<span class="number">30</span>] at parallelize at &lt;pastie&gt;:<span class="number">25</span> []</span><br><span class="line"> |   <span class="type">ParallelCollectionRDD</span>[<span class="number">31</span>] at parallelize at &lt;pastie&gt;:<span class="number">26</span> []</span><br><span class="line"> |   <span class="type">MapPartitionsRDD</span>[<span class="number">35</span>] at keyBy at &lt;pastie&gt;:<span class="number">30</span> []</span><br><span class="line"> |   <span class="type">ParallelCollectionRDD</span>[<span class="number">31</span>] at parallelize at &lt;pastie&gt;:<span class="number">26</span> []</span><br></pre></td></tr></table></figure>

<p>不同的RDD代表着不同的‘计算模式’：<br>MapPartitionsRDD，对Iterator的每个值应用相同的函数；</p>
<p>ShuffledRDD，对Iterator执行combineByKey的模式，可以指定<br><code> createCombiner: V =&gt; C,mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C</code>, compute函数返回ShuffleReader生成的迭代器。</p>
<h2 id="MapPartitionsRDD"><a href="#MapPartitionsRDD" class="headerlink" title="MapPartitionsRDD"></a>MapPartitionsRDD</h2><p>MapPartitionsRDD对于父RDD的依赖类型只能是OneToOneDependency，代表将函数应用到每一个分区的计算。</p>
<p>相关transformation：map, flatMap, filter, mapPartitions等等</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; sc.parallelize(<span class="number">0</span> to <span class="number">10000</span>).map(x=&gt;(x%<span class="number">9</span>,<span class="number">1</span>)).dependencies</span><br><span class="line">res35: <span class="type">Seq</span>[org.apache.spark.<span class="type">Dependency</span>[_]] = <span class="type">List</span>(org.apache.spark.<span class="type">OneToOneDependency</span>@<span class="number">7</span>c6843f)</span><br></pre></td></tr></table></figure>

<h2 id="ShuffledRDD"><a href="#ShuffledRDD" class="headerlink" title="ShuffledRDD"></a>ShuffledRDD</h2><p>对于父RDD的依赖类型只能是ShuffleDependency，代表需要改变分区方式进行shuffle的计算。</p>
<p>会创建ShuffledRDD的transformation：<br>RDD：coalesce<br>PairRDDFunctions： reduceByKey, combineByKeyWithClassTag ， partitionBy (分区方式不同时) 等<br>OrderedRDDFunctions： sortByKey， repartitionAndSortWithinPartitions </p>
<h1 id="RDD-Checkpoint"><a href="#RDD-Checkpoint" class="headerlink" title="RDD Checkpoint"></a>RDD Checkpoint</h1><p>Checkpoint检查点，是一种截断RDD依赖链，并把RDD数据持久化到存储系统(通常是HDFS或本地)的过程。<br>主要作用是截断RDD依赖关系，防止stack overflow(与DAG递归调用有关)。<br>存储的数据包括RDD计算后的数据和partitioner。</p>
<p>Checkpoint分为两种：</p>
<ul>
<li>reliable ：调用函数为RDD.checkpoint()，数据保存到可靠存储HDFS，RDD的parent替换为ReliableCheckpointRDD；</li>
<li>local：调用函数为RDD.localCheckpoint()，数据保存到spark cache中(不是本地)，RDD的parent替换为LocalCheckpointRDD。当executor挂掉，数据会丢失。</li>
</ul>
<p>注意：与streaming中的checkpointing不同，streaming中的checkpointing会同时保存元数据和RDD数据，可以用于Application容错。</p>
<h2 id="如何使用-1"><a href="#如何使用-1" class="headerlink" title="如何使用"></a>如何使用</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; :paste</span><br><span class="line"><span class="comment">// Entering paste mode (ctrl-D to finish)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> a=sc.parallelize(<span class="number">0</span> to <span class="number">9</span>)</span><br><span class="line"><span class="keyword">val</span> b=a.map(_*<span class="number">10</span>)</span><br><span class="line"><span class="keyword">val</span> c=b.filter(_&gt;<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Exiting paste mode, now interpreting.</span></span><br><span class="line"></span><br><span class="line">a: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line">b: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at map at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">c: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at filter at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; c.toDebugString</span><br><span class="line">res0: <span class="type">String</span> =</span><br><span class="line">(<span class="number">4</span>) <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at filter at &lt;console&gt;:<span class="number">26</span> []</span><br><span class="line"> |  <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at map at &lt;console&gt;:<span class="number">25</span> []</span><br><span class="line"> |  <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">24</span> []</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">scala&gt; sc.setCheckpointDir(<span class="string">&quot;/tmp/spark-checkpoint&quot;</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; b.checkpoint</span><br><span class="line"></span><br><span class="line">scala&gt; b.count</span><br><span class="line">res4: <span class="type">Long</span> = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">scala&gt; c.toDebugString</span><br><span class="line">res5: <span class="type">String</span> =</span><br><span class="line">(<span class="number">4</span>) <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at filter at &lt;console&gt;:<span class="number">26</span> []</span><br><span class="line"> |  <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at map at &lt;console&gt;:<span class="number">25</span> []</span><br><span class="line"> |  <span class="type">ReliableCheckpointRDD</span>[<span class="number">3</span>] at count at &lt;console&gt;:<span class="number">26</span> []</span><br><span class="line"> </span><br><span class="line">scala&gt; b.toDebugString</span><br><span class="line">res6: <span class="type">String</span> =</span><br><span class="line">(<span class="number">4</span>) <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at map at &lt;console&gt;:<span class="number">25</span> []</span><br><span class="line"> |  <span class="type">ReliableCheckpointRDD</span>[<span class="number">3</span>] at count at &lt;console&gt;:<span class="number">26</span> [] </span><br><span class="line"> </span><br><span class="line"><span class="comment">//local </span></span><br><span class="line">scala&gt; c.localCheckpoint</span><br><span class="line">scala&gt; c.count</span><br><span class="line">res9: <span class="type">Long</span> = <span class="number">8</span></span><br><span class="line"></span><br><span class="line">scala&gt; c.toDebugString</span><br><span class="line">res10: <span class="type">String</span> =</span><br><span class="line">(<span class="number">4</span>) <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at filter at &lt;console&gt;:<span class="number">26</span> [<span class="type">Disk</span> <span class="type">Memory</span> <span class="type">Deserialized</span> <span class="number">1</span>x <span class="type">Replicated</span>]</span><br><span class="line"> |       <span class="type">CachedPartitions</span>: <span class="number">4</span>; <span class="type">MemorySize</span>: <span class="number">104.0</span> <span class="type">B</span>; <span class="type">ExternalBlockStoreSize</span>: <span class="number">0.0</span> <span class="type">B</span>; <span class="type">DiskSize</span>: <span class="number">0.0</span> <span class="type">B</span></span><br><span class="line"> |  <span class="type">LocalCheckpointRDD</span>[<span class="number">4</span>] at count at &lt;console&gt;:<span class="number">26</span> [<span class="type">Disk</span> <span class="type">Memory</span> <span class="type">Deserialized</span> <span class="number">1</span>x <span class="type">Replicated</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>查看HDFS上存储的checkpoint文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hdfs dfs -ls /tmp/spark-checkpoint/74acd422-2693-4f47-b786-69b4f8dc33ad/rdd-1</span><br><span class="line">Found 4 items</span><br><span class="line">-rw-r--r--   2 ld-liuyuan_su hdfs         91 2019-10-09 08:40 /tmp/spark-checkpoint/74acd422-2693-4f47-b786-69b4f8dc33ad/rdd-1/part-00000</span><br><span class="line">-rw-r--r--   2 ld-liuyuan_su hdfs        101 2019-10-09 08:40 /tmp/spark-checkpoint/74acd422-2693-4f47-b786-69b4f8dc33ad/rdd-1/part-00001</span><br><span class="line">-rw-r--r--   2 ld-liuyuan_su hdfs         91 2019-10-09 08:40 /tmp/spark-checkpoint/74acd422-2693-4f47-b786-69b4f8dc33ad/rdd-1/part-00002</span><br><span class="line">-rw-r--r--   2 ld-liuyuan_su hdfs        101 2019-10-09 08:40 /tmp/spark-checkpoint/74acd422-2693-4f47-b786-69b4f8dc33ad/rdd-1/part-00003</span><br></pre></td></tr></table></figure>


<h1 id="RDD-Cache"><a href="#RDD-Cache" class="headerlink" title="RDD Cache"></a>RDD Cache</h1><p>Cache机制是Spark提供的一种将数据缓存到内存(或磁盘)的机制，<br>主要用途是使得中间计算结果可以被重用。</p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/20191022163225.png" alt="20191022163225.png"></p>
<p>常见的使用场景有如下几种，底层都是调用RDD的cache，这里只讲RDD的cache。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rdd.cache()</span><br><span class="line">dataset.cache()</span><br><span class="line">spark.sql(&quot;cache table test.test&quot;)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>Spark的Cache不仅能将数据缓存到内存，也能使用磁盘，甚至同时使用内存和磁盘，这种缓存的不同存储方式，称作‘StorageLevel(存储级别)’。</p>
<p>可以这样使用：<code>rdd.persist(StorageLevel.MEMORY_ONLY)</code>。</p>
<p>Spark目前支持的存储级别如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">NONE (default)</span><br><span class="line">DISK_ONL</span><br><span class="line">DISK_ONLY_2</span><br><span class="line">MEMORY_ONLY (cache操作使用的级别)</span><br><span class="line">MEMORY_ONLY_2</span><br><span class="line">MEMORY_ONLY_SER</span><br><span class="line">MEMORY_ONLY_SER_2</span><br><span class="line">MEMORY_AND_DISK</span><br><span class="line">MEMORY_AND_DISK_2</span><br><span class="line">MEMORY_AND_DISK_SER</span><br><span class="line">MEMORY_AND_DISK_SER_2</span><br><span class="line">OFF_HEAP</span><br></pre></td></tr></table></figure>

<p><code>2</code>代表存储份数为2，也就是有个备份存储。<br><code>SER</code>代表存储序列化后的数据。</p>
<p>DISK_ONLY后面没跟SER，但其实只能是存储序列化后的数据。</p>
<p>要cache RDD,常用到两个函数, <code>cache()</code>和<code>persist()</code>，cache方法本质上是<code>persist(StorageLevel.MEMORY_ONLY)</code>，也就是说persist可以指定StorageLevel，而cache不行。</p>
<h3 id="Checkpoint-vs-Cache"><a href="#Checkpoint-vs-Cache" class="headerlink" title="Checkpoint vs Cache"></a>Checkpoint vs Cache</h3><ul>
<li>Cache用于缓存，采用临时保存，Executor挂掉会导致数据丢失，但是数据可以重新计算。</li>
<li>Checkpoint用于截断依赖链，reliable方式下Executor挂掉不会丢失数据，数据一旦丢失不可恢复。</li>
</ul>
<h1 id="RDD-Broadcast"><a href="#RDD-Broadcast" class="headerlink" title="RDD Broadcast"></a>RDD Broadcast</h1><p>一种将数据在不同节点间共享的机制，可以将指定的<strong>只读</strong>数据广播分发到每个Executor，每个Executor有一份完整的备份。</p>
<p>是一种高效的数据共享机制，被广播的数据可以被不同的stage和task共享，而不需要给每个task拷贝一份。</p>
<p>Broadcast机制有个非常重要的作用，Spark就是通过它将task分发给各个Executor。</p>
<p>下面举个使用的例子</p>
<p>rddA</p>
<table>
<thead>
<tr>
<th>k</th>
<th>low</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>a</td>
</tr>
<tr>
<td>2</td>
<td>b</td>
</tr>
<tr>
<td>3</td>
<td>c</td>
</tr>
</tbody></table>
<p>rddB</p>
<table>
<thead>
<tr>
<th>k</th>
<th>up</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>A</td>
</tr>
<tr>
<td>2</td>
<td>B</td>
</tr>
<tr>
<td>3</td>
<td>C</td>
</tr>
</tbody></table>
<p>rddAB</p>
<table>
<thead>
<tr>
<th>k</th>
<th>low</th>
<th>up</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>a</td>
<td>A</td>
</tr>
<tr>
<td>2</td>
<td>b</td>
<td>B</td>
</tr>
<tr>
<td>3</td>
<td>c</td>
<td>C</td>
</tr>
</tbody></table>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rddA=sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="string">&quot;a&quot;</span>),(<span class="number">2</span>,<span class="string">&quot;b&quot;</span>),(<span class="number">3</span>,<span class="string">&quot;c&quot;</span>)))</span><br><span class="line">rddA: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">5</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rddB=sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="string">&quot;A&quot;</span>),(<span class="number">2</span>,<span class="string">&quot;B&quot;</span>),(<span class="number">3</span>,<span class="string">&quot;C&quot;</span>)))</span><br><span class="line">rddB: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">6</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rddAB=rddA.join(rddB)</span><br><span class="line">rddAB: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, (<span class="type">String</span>, <span class="type">String</span>))] = <span class="type">MapPartitionsRDD</span>[<span class="number">9</span>] at join at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; rddAB.collect</span><br><span class="line">res11: <span class="type">Array</span>[(<span class="type">Int</span>, (<span class="type">String</span>, <span class="type">String</span>))] = <span class="type">Array</span>((<span class="number">1</span>,(a,<span class="type">A</span>)), (<span class="number">2</span>,(b,<span class="type">B</span>)), (<span class="number">3</span>,(c,<span class="type">C</span>)))</span><br><span class="line"></span><br><span class="line">scala&gt; rddAB.toDebugString</span><br><span class="line">res12: <span class="type">String</span> =</span><br><span class="line">(<span class="number">4</span>) <span class="type">MapPartitionsRDD</span>[<span class="number">9</span>] at join at &lt;console&gt;:<span class="number">27</span> []</span><br><span class="line"> |  <span class="type">MapPartitionsRDD</span>[<span class="number">8</span>] at join at &lt;console&gt;:<span class="number">27</span> []</span><br><span class="line"> |  <span class="type">CoGroupedRDD</span>[<span class="number">7</span>] at join at &lt;console&gt;:<span class="number">27</span> []</span><br><span class="line"> +-(<span class="number">4</span>) <span class="type">ParallelCollectionRDD</span>[<span class="number">5</span>] at parallelize at &lt;console&gt;:<span class="number">24</span> []</span><br><span class="line"> +-(<span class="number">4</span>) <span class="type">ParallelCollectionRDD</span>[<span class="number">6</span>] at parallelize at &lt;console&gt;:<span class="number">24</span> []</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rddBMap=sc.broadcast(rddB.collectAsMap)</span><br><span class="line">rddBMap: org.apache.spark.broadcast.<span class="type">Broadcast</span>[scala.collection.<span class="type">Map</span>[<span class="type">Int</span>,<span class="type">String</span>]] = <span class="type">Broadcast</span>(<span class="number">9</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rddABMapJoin= rddA.map&#123;<span class="keyword">case</span>(k,v) =&gt; (k,(v,rddBMap.value.get(k).get))&#125;</span><br><span class="line">rddABMapJoin: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, (<span class="type">String</span>, <span class="type">String</span>))] = <span class="type">MapPartitionsRDD</span>[<span class="number">10</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; rddABMapJoin.collect</span><br><span class="line">res13: <span class="type">Array</span>[(<span class="type">Int</span>, (<span class="type">String</span>, <span class="type">String</span>))] = <span class="type">Array</span>((<span class="number">1</span>,(a,<span class="type">A</span>)), (<span class="number">2</span>,(b,<span class="type">B</span>)), (<span class="number">3</span>,(c,<span class="type">C</span>)))</span><br><span class="line"></span><br><span class="line">scala&gt; rddABMapJoin.toDebugString</span><br><span class="line">res14: <span class="type">String</span> =</span><br><span class="line">(<span class="number">4</span>) <span class="type">MapPartitionsRDD</span>[<span class="number">10</span>] at map at &lt;console&gt;:<span class="number">27</span> []</span><br><span class="line"> |  <span class="type">ParallelCollectionRDD</span>[<span class="number">5</span>] at parallelize at &lt;console&gt;:<span class="number">24</span> []</span><br></pre></td></tr></table></figure>
<p>通过broadcast机制，将原本的两个stage计算减少为1个stage。<br>这里模拟实现了map-side join。</p>
<h2 id="Broadcast-VS-Cache"><a href="#Broadcast-VS-Cache" class="headerlink" title="Broadcast VS Cache"></a>Broadcast VS Cache</h2><p>Cache也会把数据分发到各个节点，但是一个节点上通常只有部分分区的数据，而Broadcast会保证每个节点都有完整的数据。<br>Broadcast会消耗更多的内存，但是带来了更好的性能。</p>
<h1 id="RDD-Accumulators"><a href="#RDD-Accumulators" class="headerlink" title="RDD Accumulators"></a>RDD Accumulators</h1><p>Broadcast机制有个短板，它的变量是只读的，于是Spark提供了Accumulators(累加器)来弥补。</p>
<p>Accumulator的值可以增减，但是不能直接修改为指定值。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> acc=sc.longAccumulator</span><br><span class="line">acc: org.apache.spark.util.<span class="type">LongAccumulator</span> = <span class="type">LongAccumulator</span>(id: <span class="number">200</span>, name: <span class="type">None</span>, value: <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; rddA.map(_=&gt;acc.add(<span class="number">-1</span>)).count</span><br><span class="line">res15: <span class="type">Long</span> = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">scala&gt; acc.value</span><br><span class="line">res17: <span class="type">Long</span> = <span class="number">-3</span></span><br></pre></td></tr></table></figure>

<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://vishnuviswanath.com/spark_rdd.html">Spark RDDs Simplified</a></p>
<p><a href="https://techmagie.wordpress.com/2015/12/19/understanding-spark-partitioning/">Understanding Spark Partitioning</a></p>
<p><a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-rdd-checkpointing.html">Checkpointing</a></p>
<p><a href="https://book.douban.com/subject/30157181/">Spark内核设计的艺术</a></p>
<p>转载请注明原文地址：<a href="https://liam-blog.ml/2019/10/23/spark-core-rdd/">https://liam-blog.ml/2019/10/23/spark-core-rdd/</a></p>
]]></content>
      <tags>
        <tag>Spark</tag>
        <tag>Spark Core</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark Core解析 2：Scheduler 调度体系</title>
    <url>/2019/11/07/spark-core-scheduler/</url>
    <content><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>调度系统，是贯穿整个Spark应用的主心骨，从调度系统开始入手了解Spark Core，比较容易理清头绪。</p>
<p>Spark的资源调度采用的是常见的两层调度，底层资源的管理和分配是第一层调度，交给YARN、Mesos或者Spark的Standalone集群处理，Application从第一层调度拿到资源后，还要进行内部的任务和资源调度，将任务和资源进行匹配，这是第二层调度，<strong>本文讲的就是这第二层调度</strong>。</p>
<p>Spark的调度体系涉及的任务包括3个粒度，分别是Job、Stage、Task。<br>Job代表用户提交的一系列操作的总体，一个具体的计算任务，有明确的输入输出，一个Job由多个Stage组成；<br>一个Stage代表Job计算流程的一个组成部分，一个阶段，包含多个Task；<br>一个Task代表对一个分区的数据进行计算的具体任务。</p>
<p>层级关系：Job &gt; Stage &gt; Task</p>
<span id="more"></span>

<p>在<a href="https://liam-blog.ml/2019/10/23/spark-core-rdd/">Spark Core 解析：RDD 弹性分布式数据集</a>中，已经解释了RDD之间的依赖，以及如何组成RDD血缘图。</p>
<p><strong>所以本文主要目的就是解释清楚：Scheduler将RDD血缘图转变成Stage DAG，然后生成Task，最后提交给Executor去执行的过程。</strong></p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/20191212230626.png" alt="20191212230626.png"></p>
<h3 id="Stage"><a href="#Stage" class="headerlink" title="Stage"></a>Stage</h3><p>Job的不同分区的计算通常可以并行，但是有些计算需要将数据进行重新分区，这个过程称作shuffle(混洗)。Shuffle的过程是没法完全并行的，这时候就会出现task之间的等待，task的数量也可能发生变化，所以Spark中以shuffle为边界，对task进行划分，划分出来的每段称为Stage。</p>
<p>Stage代表一组可以并行的执行相同计算的task，每个任务必须有相同的分区规则，这样一个stage中是没有shuffle的。</p>
<p>在一个Spark App中，stage有一个全局唯一ID，stage id是自增的。</p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/20191028171155.png" alt="20191028171155.png"></p>
<p>Stage分为两种：</p>
<ul>
<li>ResultStage：最后执行的stage，负责Job最终的结果输出，<strong>每个Job有且仅有一个ResultStage</strong>；</li>
<li>ShuffleMapStage：该stage的输出不是最终结果，而是其他stage的输入数据，通常涉及一次shuffle计算。</li>
</ul>
<p>stage创建流程：</p>
<ul>
<li>从最终执行action的RDD开始，沿着RDD依赖关系遍历，<br>一旦发现某个RDD的dependency是ShuffleDependency，就创建一个ShuffleMapStage。</li>
<li>最后创建ResultStage。</li>
</ul>
<h4 id="example-1"><a href="#example-1" class="headerlink" title="example 1"></a>example 1</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rg=sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">10</span>),(<span class="number">2</span>,<span class="number">20</span>)))</span><br><span class="line">rg.reduceByKey(_+_).collect</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/stages-simple.png" alt="stages-simple.png"></p>
<p>这里reduceByKey操作引起了一次shuffle，所以job被切分成了2个stage。</p>
<h4 id="example-2"><a href="#example-2" class="headerlink" title="example 2"></a>example 2</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rddA=sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="string">&quot;a&quot;</span>),(<span class="number">2</span>,<span class="string">&quot;b&quot;</span>),(<span class="number">3</span>,<span class="string">&quot;c&quot;</span>)))</span><br><span class="line"><span class="keyword">val</span> rddB=sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="string">&quot;A&quot;</span>),(<span class="number">2</span>,<span class="string">&quot;B&quot;</span>),(<span class="number">3</span>,<span class="string">&quot;C&quot;</span>)))</span><br><span class="line">rddA.join(rddB).collect</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/stages-join.png" alt="stages-join.png"></p>
<p>join操作导致rddA和rddB都进行了一次shuffle，所以有3个stage。</p>
<h4 id="example-3"><a href="#example-3" class="headerlink" title="example 3"></a>example 3</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">HashPartitioner</span></span><br><span class="line"><span class="keyword">val</span> rddA=sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="string">&quot;a&quot;</span>),(<span class="number">2</span>,<span class="string">&quot;b&quot;</span>),(<span class="number">3</span>,<span class="string">&quot;c&quot;</span>))).partitionBy(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">3</span>))</span><br><span class="line"><span class="keyword">val</span> rddB=sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="string">&quot;A&quot;</span>),(<span class="number">2</span>,<span class="string">&quot;B&quot;</span>),(<span class="number">3</span>,<span class="string">&quot;C&quot;</span>)))</span><br><span class="line">rddA.join(rddB).collect</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/stages-co-join.png" alt="stages-co-join.png"></p>
<p>WHAT ?</p>
<p>因为rddA已经定义了Partitioner，这里join操作会保留rddA的分区方式，所以对rddA的依赖是OneToOneDepenency，而对于rddB则是ShuffleDependency。</p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/stage-example-3-2.png" alt="stage-example-3-2.png"></p>
<h4 id="探索：一个RDD被依赖多次，会如何"><a href="#探索：一个RDD被依赖多次，会如何" class="headerlink" title="探索：一个RDD被依赖多次，会如何"></a>探索：一个RDD被依赖多次，会如何</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rddA=sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="string">&quot;a&quot;</span>),(<span class="number">2</span>,<span class="string">&quot;b&quot;</span>),(<span class="number">3</span>,<span class="string">&quot;c&quot;</span>)))</span><br><span class="line">rddA join rddA collect</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/rdd%20use%20twice.png" alt="rdd use twice.png"></p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/rdd-used-twice.png" alt="rdd-used-twice.png"></p>
<p>一个RDD被两个stage使用了。</p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>综上，stage的划分一定是依据shuffle即ShuffleDependency，跟算子和RDD变量的定义没有很强的关系，example2和3中的join操作<code>rddA.join(rddB).collect</code>看起来一模一样，但实际产生的stage划分却差别很大。</p>
<h3 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h3><p>与stage对应，task也分为两种：</p>
<ul>
<li>ShuffleMapTask：即ShuffleMapStage中的task，主要完成map、shuffle计算。</li>
<li>ResultTask：ResultStage中的task，主要完成最终结果输出或者返回结果给driver的任务。</li>
</ul>
<p>一个stage有多少个partition就会创建多少个task，比如一个ShuffleMapStage有10个partition，那么就会创建10个ShuffleMapTask。</p>
<p>一个Stage中的所有task组成一个TaskSet。</p>
<h2 id="Job-Submit"><a href="#Job-Submit" class="headerlink" title="Job Submit"></a>Job Submit</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">graph TB</span><br><span class="line">R(RDD.action)--&gt;S(SparkContext.runJob)-- RDD --&gt;D(DAGScheduler.runJob)</span><br><span class="line">-- TaskSet --&gt;T(TaskScheduler.submitTasks)-- TaskDescription --&gt;E(Executor.launchTask)</span><br></pre></td></tr></table></figure>

<p>RDD在action操作中通过SparkContext.runJob方法触发Job执行流程，该方法将调用DagScheduler.runJob方法，将RDD传入DagScheduler。然后，DAGScheduler创建TaskSet提交给TaskScheduler，TaskScheduler再将TaskSet封装成TaskDescription发送给Executor，最后Executor会将TaskDescription提交给线程池来运行。</p>
<h2 id="Stage-Scheduler-high-level"><a href="#Stage-Scheduler-high-level" class="headerlink" title="Stage Scheduler(high-level)"></a>Stage Scheduler(high-level)</h2><h3 id="DagScheduler"><a href="#DagScheduler" class="headerlink" title="DagScheduler"></a>DagScheduler</h3><p>Stage级别的调度是DagScheduler负责的，也是Spark调度体系的核心。</p>
<h4 id="DagScheduler的工作模式"><a href="#DagScheduler的工作模式" class="headerlink" title="DagScheduler的工作模式"></a>DagScheduler的工作模式</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sequenceDiagram</span><br><span class="line">    participant M as main thread</span><br><span class="line">    participant L as eventProcessLoop</span><br><span class="line">    participant E as event thread</span><br><span class="line">    M--&gt;&gt;L: post event</span><br><span class="line">    E--&gt;&gt;L: handle event</span><br></pre></td></tr></table></figure>

<p>DagScheduler内部维护了一个事件消息总线eventProcessLoop(类型为DAGSchedulerEventProcessLoop)，其实就是一个用来存储DAGSchedulerEvent类型数据的队列。</p>
<p>当DagScheduler的一些方法被调用的时候（如submitJob方法），并不会在主线程中处理该任务，而是post一个event(如JobSubmitted)到eventProcessLoop。eventProcessLoop中有一个守护线程，会不断的依次从队列中取出event，然后调用对应的handle(如handleJobSubmitted)方法来执行具体的任务。</p>
<h3 id="Stage调度流程"><a href="#Stage调度流程" class="headerlink" title="Stage调度流程"></a>Stage调度流程</h3><ul>
<li><p>1.submit job</p>
<p>DagScheduler.runJob方法会调用submitJob方法，向eventProcessLoop发送一个JobSubmitted类型的消息，其中包含了RDD等信息。当eventProcessLoop接收到JobSubmitted类型的消息，会调用DagScheduler.handleJobSubmitted方法来处理消息。</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sequenceDiagram</span><br><span class="line">    participant M as main thread(runJob)</span><br><span class="line">    participant L as eventProcessLoop</span><br><span class="line">    participant E as event thread(handleJobSubmitted)</span><br><span class="line">    M--&gt;&gt;L: post JobSubmitted event</span><br><span class="line">    E--&gt;&gt;L: handle JobSubmitted event</span><br></pre></td></tr></table></figure>

<ul>
<li><p>2.create stage</p>
<ul>
<li><p>DagScheduler在它的handleJobSubmitted方法中开始创建ResultStage。ResultStage中包含了最终执行action的finalRDD，以及计算函数func。</p>
</li>
<li><p>ResultStage有个parents属性，这个属性是个列表，也就是说可以有多个parent stage。创建ResultStage时需要先创建它的parent stage来填充这个属性，也就是说要创建ResultStage直接依赖的所有ShuffleMapStage。</p>
</li>
<li><p>通过stage.rdd.dependencies属性，采用宽度优先遍历，一旦发现某个RDD(假设叫rddA)的dependency是ShuffleDependency，就创建一个ShuffleMapStage，ShuffleMapStage中包含的关键信息与ResultStage不同，是rddA的ShuffleDependency和rddA的ShuffleDependency.rdd，也就是说新创建的ShuffleMapStage持有的信息是他自身的最后一个RDD和该RDD的子RDD的dependency。</p>
</li>
<li><p>创建一个ShuffleMapStage的过程同理会需要创建它的parent stage，也是若干ShuffleMapStage。如此递归下去，直到创建完所有的ShuffleMapStage，最后才完成ResultStage的创建。最后创建出来的这些Stage(若干ShuffleMapStage加一个ResultStage)，通过parent属性串起来，就像这样</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">graph TD</span><br><span class="line">A[ResultStage]-- parent --&gt;B[ShuffleMapStage 1]</span><br><span class="line">A-- parent --&gt;C[ShuffleMapStage 2]</span><br><span class="line">B-- parent --&gt;D[ShuffleMapStage 3]</span><br></pre></td></tr></table></figure>

<p>这就生成了所谓的DAG图，但是这个图的指向跟执行顺序是反过来的，如果按执行顺序来画DAG图，就是常见的形式了：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">graph TD</span><br><span class="line">D[ShuffleMapStage 3]--&gt;C[ShuffleMapStage 2]</span><br><span class="line">C[ShuffleMapStage 2]--&gt;A[ResultStage]</span><br><span class="line">B[ShuffleMapStage 1]--&gt;A[ResultStage]</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>3.submit stage</p>
<p>DagScheduler.handleJobSubmitted方法创建好ResultStage后会提交这个stage(submitStage方法)，在提交一个stage的时候，会要先提交它的parent stage,也是通过递归的形式，直到一个stage的所有parent stage都被提交了，它自己才能被提交，如果一个stage的parent还没有完成，则会把这个stage加入waitingStages。也就是说，DAG图中前面的stage会被先提交。当一个stage的parent都准备好了，也就是执行完了，它才会进入submitMissingTasks的环节。</p>
</li>
<li><p>4.submit task</p>
<p>Task是在DagScheduler（不是TaskScheduler）的submitMissingTasks方法中创建的，包括ShuffleMapTask和ResultTask，与Stage对应。归属于同一个stage的这批Task组成一个TaskSet集合，最后提交给TaskScheduler的就是这个TaskSet集合。</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/20191029095005.png" alt="20191029095005.png"></p>
<h2 id="Task-Scheduler-low-level"><a href="#Task-Scheduler-low-level" class="headerlink" title="Task Scheduler(low-level)"></a>Task Scheduler(low-level)</h2><p>Task的调度工作是由TaskScheduler与SchedulerBackend紧密合作，共同完成的。</p>
<p>TaskScheduler是task级别的调度器，主要作用是管理task的调度和提交，是Spark底层的调度器。</p>
<p>SchedulerBackend是TaskScheduler的后端服务，有独立的线程，所有的Executor都会注册到SchedulerBackend，主要作用是进行资源分配、将task分配给executor等。</p>
<h3 id="Task调度流程"><a href="#Task调度流程" class="headerlink" title="Task调度流程"></a>Task调度流程</h3><p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/spark%20task%20scheduler.png" alt="spark task scheduler.png"></p>
<p>第一个线程是DAGScheduler的事件处理线程，在其中，Task先经过DAGScheduler（蓝色箭头表示）封装成TaskSet，再由TaskScheduler（绿色箭头）封装成TaskSetManager，并加入调度队列中。</p>
<p>SchedulerBackend在收到ReviveOffers消息时，会从线程池取一个线程进行makeOffers操作，WorkerOffer创建后传递给TaskScheduler进行分配。</p>
<p>图中第二个线程就是SchedulerBackend的一个事件分发线程，从Pool中取出最优先的TaskSetManager，然后将WorkerOffer与其中的Task进行配对，生成TaskDescription，发送给WorkerOffer指定的Executor去执行。</p>
<h3 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h3><p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/TaskScheduler.png" alt="TaskScheduler.png"></p>
<ul>
<li>1 DAGScheduler(submitMissingTasks方法中)调用TaskScheduler.submitTasks()创建并提交TaskSet给TaskScheduler；</li>
<li>2 TaskScheduler拿到TaskSet后会创建一个TaskSetManager来管理它，并且把TaskSetManager添加到rootPool调度池中；</li>
<li>3 调用SchedulerBackend.reviveOffers()方法；</li>
<li>4 SchedulerBackend发送ReviveOffers消息给DriverEndpoint；</li>
<li>5 DriverEndpoint收到ReviveOffers消息后，会调用makeOffers()方法创建WorkerOffer，并通过TaskScheduler.resourceOffers()返回offer；</li>
<li>6 TaskScheduler从rootPool获取按调度算法排序后的TaskSetManager列表，取第一个TaskSetManager，逐个给TaskSet的Task分配WorkerOffer，生成TaskDescription(包含offer信息)；</li>
<li>7 调用SchedulerBackend.DriverEndpoint的launchTasks方法，将TaskDescription序列化并封装在LaunchTask消息中，发送给offer指定的executor。LaunchTask消息被ExecutorBackend收到后，会将Task信息反序列化，传给Executor.launchTask()，最后使用Executor的线程池中的线程来执行这个Task。</li>
</ul>
<h3 id="梳理"><a href="#梳理" class="headerlink" title="梳理"></a>梳理</h3><p>Stage,TaskSet,TaskSetManager是一一对应的，数量相等，都是只存在driver上的。<br>Parition,Task,TaskDescription是一一对应，数量相同，Task和TaskDescription是会被发到executor上的。</p>
<h3 id="TaskScheduler的调度池"><a href="#TaskScheduler的调度池" class="headerlink" title="TaskScheduler的调度池"></a>TaskScheduler的调度池</h3><p>与DAGScheduler不同的是TaskScheduler有调度池，有两种调度实体，Pool和TaskSetManager。<br>与YARN的调度队列类似，采用了层级队列的方式，Pool是TaskSetManager的容器，起到将TaskSetManager分组的作用。</p>
<h4 id="Schedulable"><a href="#Schedulable" class="headerlink" title="Schedulable"></a>Schedulable</h4><p>Schedulable是调度实体的基类，有两个子类Pool和TaskSetManager。</p>
<p>要理解调度规则，必须知道下面几个属性：</p>
<ul>
<li>parent：所属调度池，顶层的调度池为root pool；</li>
<li>schedulableQueue：包含的调度对象组成的队列；</li>
<li>schedulingMode：调度模式，FIFO or FAIR；</li>
<li>weight：权重</li>
<li>minShare：最小分配额(CPU核数)</li>
<li>runningTasks：运行中task数</li>
<li>priority：优先级</li>
<li>stageId：就是stageId</li>
<li>name：名称</li>
</ul>
<p>Pool和TaskSetManager对于这些属性的取值有所不同，从而导致了他们的调度行为也不一样。</p>
<table>
<thead>
<tr>
<th align="left">properties</th>
<th align="left">Pool</th>
<th align="left">TaskSetManager</th>
</tr>
</thead>
<tbody><tr>
<td align="left">weight</td>
<td align="left">config</td>
<td align="left">1</td>
</tr>
<tr>
<td align="left">minShare</td>
<td align="left">config</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">priority</td>
<td align="left">0</td>
<td align="left">jobId</td>
</tr>
<tr>
<td align="left">stageId</td>
<td align="left">-1</td>
<td align="left">stageId</td>
</tr>
<tr>
<td align="left">name</td>
<td align="left">config</td>
<td align="left">TaskSet_{taskSet.id}</td>
</tr>
<tr>
<td align="left">runningTasks</td>
<td align="left">Pool所含TaskSetManager的runningTasks和</td>
<td align="left">TaskSetManager运行中task数</td>
</tr>
</tbody></table>
<h4 id="Pools创建流程"><a href="#Pools创建流程" class="headerlink" title="Pools创建流程"></a>Pools创建流程</h4><p>TaskScheduler有个属性schedulingMode，值取决于配置项<code>spark.scheduler.mode</code>，默认为FIFO。这个属性会导致TaskScheduler使用不同的SchedulableBuilder，即FIFOSchedulableBuilder和FairSchedulableBuilder。</p>
<p>TaskScheduler在初始化的时候，就会创建root pool，根调度池，是所有pool的祖先。<br>它的属性取值为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">name: &quot;&quot; (空字符串)</span><br><span class="line">schedulingMode: 同TaskScheduler的schedulingMode属性</span><br><span class="line">weight: 0</span><br><span class="line">minShare: 0</span><br></pre></td></tr></table></figure>

<p>注意root pool的调度模式确定了。</p>
<p>接下来会执行<code>schedulableBuilder.buildPools()</code>方法，</p>
<ul>
<li><p>如果是FIFOSchedulableBuilder，则什么都不会发生。</p>
</li>
<li><p>若是FairSchedulableBuilder</p>
<ul>
<li>1 依据scheduler配置文件(后面会说)，开始创建pool(可以是多个pool，FIFO，FAIR都有可能，取决于配置文件)，并都加入root pool中。</li>
<li>2 如果现在root pool中没有名为”default”的pool(即配置文件中没有定义一个叫default的pool)，创建default pool，并加入root pool中。<br>这时default pool它的属性取值是固定的：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">name: &quot;default&quot;</span><br><span class="line">schedulingMode: FIFO</span><br><span class="line">weight: 1</span><br><span class="line">minShare: 0</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="Task加入pool流程"><a href="#Task加入pool流程" class="headerlink" title="Task加入pool流程"></a>Task加入pool流程</h4><p>当TaskScheduler提交task的时候，会先创建TaskSetManager，然后通过schedulableBuilder添加到pool中。</p>
<ul>
<li><p>如果是FIFOSchedulableBuilder，则会直接把TaskSetManager加入root pool队列中。</p>
</li>
<li><p>若是FairSchedulableBuilder</p>
<ul>
<li>1 从<code>spark.scheduler.pool</code>配置获取pool name，没有定义则用’default’；</li>
<li>2 从root pool遍历找到对应名称的pool，把TaskSetManager加入pool的队列。如果没有找到，则创建一个该名称的pool，采用与default pool相同的属性配置，并加入root pool。</li>
</ul>
</li>
</ul>
<h4 id="调度池结构"><a href="#调度池结构" class="headerlink" title="调度池结构"></a>调度池结构</h4><p>经过上面两部分，最终得到的调度池结构如下：</p>
<p>spark.scheduler.mode&#x3D;FIFO</p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/20191128210416.png" alt="20191128210416.png"></p>
<p>spark.scheduler.mode&#x3D;FAIR</p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/20191128210432.png" alt="20191128210432.png"></p>
<h4 id="Fair-Scheduler-pools配置"><a href="#Fair-Scheduler-pools配置" class="headerlink" title="Fair Scheduler pools配置"></a>Fair Scheduler pools配置</h4><p>Fair Scheduler Pool的划分依赖于配置文件，默认的配置文件为’fairscheduler.xml’，也可以通过配置项”spark.scheduler.allocation.file”指定配置文件。</p>
<p>煮个栗子，文件内容如下：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">allocations</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">pool</span> <span class="attr">name</span>=<span class="string">&quot;prod&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">schedulingMode</span>&gt;</span>FAIR<span class="tag">&lt;/<span class="name">schedulingMode</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">weight</span>&gt;</span>1<span class="tag">&lt;/<span class="name">weight</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">minShare</span>&gt;</span>2<span class="tag">&lt;/<span class="name">minShare</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">pool</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">pool</span> <span class="attr">name</span>=<span class="string">&quot;test&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">schedulingMode</span>&gt;</span>FIFO<span class="tag">&lt;/<span class="name">schedulingMode</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">weight</span>&gt;</span>2<span class="tag">&lt;/<span class="name">weight</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">minShare</span>&gt;</span>3<span class="tag">&lt;/<span class="name">minShare</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">pool</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">allocations</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>这里配置了两个pool，prod和test，并且配置了相关属性，<strong>这两个pool都会添加到root pool中</strong>。</p>
<h4 id="调度算法"><a href="#调度算法" class="headerlink" title="调度算法"></a>调度算法</h4><p>以SchedulingAlgorithm为基类，内置实现的调度算法有两种FIFOSchedulingAlgorithm和FairSchedulingAlgorithm，其逻辑如下：</p>
<ul>
<li><p>FIFO: 先进先出，优先级比较算法如下，</p>
<ul>
<li>1.比较priority，小的优先；</li>
<li>2.priority相同则比较StageId，小的优先。</li>
</ul>
</li>
<li><p>FAIR：公平调度，优先级比较算法如下，</p>
<ul>
<li>1.runningTasks小于minShare的优先级比不小于的优先级要高。</li>
<li>2.若两者运行的runningTasks都比minShare小，则比较minShare使用率(runningTasks&#x2F;max(minShare,1))，使用率越低优先级越高。</li>
<li>3.若两者的minShare使用率相同，则比较权重使用率(runningTasks&#x2F;weight)，使用率越低优先级越高。</li>
<li>4.若权重也相同，则比较name，小的优先。</li>
</ul>
</li>
</ul>
<h5 id="Pool为FIFO模式下的几种情形"><a href="#Pool为FIFO模式下的几种情形" class="headerlink" title="Pool为FIFO模式下的几种情形"></a>Pool为FIFO模式下的几种情形</h5><p>TaskSetManager之间的比较，其实就是先比较jobId再比较stageId，谁小谁优先，意味着就是谁先提交谁优先。</p>
<p>Pool之间的比较，不存在！FIFO的pool队列中是不会有pool的。</p>
<h5 id="Pool为FAIR模式下的几种情形"><a href="#Pool为FAIR模式下的几种情形" class="headerlink" title="Pool为FAIR模式下的几种情形"></a>Pool为FAIR模式下的几种情形</h5><p>TaskSetManager之间的比较，因为minShare&#x3D;0，weight&#x3D;1，FAIR算法变成了：</p>
<ul>
<li>1 runningTasks小的优先</li>
<li>2 runningTasks相同则比较name</li>
</ul>
<p>Pool之间的比较，就是标准的FAIR算法。</p>
<p><strong>当root pool为FAIR模式，先取最优先的pool，再从pool中，按pool的调度模式取优先的TaskSetManager。</strong></p>
<h4 id="开始使用FAIR-mode"><a href="#开始使用FAIR-mode" class="headerlink" title="开始使用FAIR mode"></a>开始使用FAIR mode</h4><p>启用FAIR模式：</p>
<ul>
<li>1 准备好<code>fairscheduler.xml</code>文件</li>
<li>2 启动参数添加 <code>--conf spark.scheduler.mode=FAIR</code></li>
<li>3 运行启动命令，如<code>spark-shell --master yarn --deploy-mode client --conf spark.scheode=FAIR</code></li>
</ul>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/ui-fair.png" alt="ui-fair.png"></p>
<p>启动后如果直接运行Job会自动提交到default pool，那么如何提交Job到指定pool？<br>SparkContext.setLocalProperty(“spark.scheduler.pool”,”poolName”)</p>
<p>如果每次只运行一个Job，开启FAIR模式的意义不大，那么如何同时运行多个Job？<br>要异步提交Job，需要用到RDD的async action，目前有如下几个：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">countAsync</span><br><span class="line">collectAsync</span><br><span class="line">takeAsync</span><br><span class="line">foreachAsync</span><br><span class="line">foreachPartitionAsync</span><br></pre></td></tr></table></figure>

<p>举个例子：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">sc.setLocalProperty(<span class="string">&quot;spark.scheduler.pool&quot;</span>,<span class="string">&quot;test&quot;</span>)</span><br><span class="line">b.foreachAsync(_=&gt;<span class="type">Thread</span>.sleep(<span class="number">100</span>))</span><br><span class="line">sc.setLocalProperty(<span class="string">&quot;spark.scheduler.pool&quot;</span>,<span class="string">&quot;production&quot;</span>)</span><br><span class="line">b.foreachAsync(_=&gt;<span class="type">Thread</span>.sleep(<span class="number">100</span>))</span><br></pre></td></tr></table></figure>

<p>这样就会有两个任务在不同的pool同时运行：</p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/pools.png" alt="pools.png"></p>
<h4 id="FAIR-mode应用场景"><a href="#FAIR-mode应用场景" class="headerlink" title="FAIR mode应用场景"></a>FAIR mode应用场景</h4><p>场景1：Spark SQL thrift server<br>作用：让离线任务和交互式查询任务分配到不同的pool，给交互式查询任务更高的优先级，这样长时间运行的离线任务就不会一直占用所有资源，阻塞交互式查询任务。</p>
<p>场景2：Streaming job与Batch job同时运行<br>作用：比如用Streaming接数据写入HDFS，可能产生很多小文件，可以在低优先级的pool定时运行batch job合并小文件。</p>
<p>另外可以参考Spark Summit 2017的分享：<a href="https://www.slideshare.net/databricks/continuous-application-with-fair-scheduler-with-robert-xue">Continuous Application with FAIR Scheduler</a></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://book.douban.com/subject/30157181/">Spark内核设计的艺术</a></p>
<p><a href="https://blog.csdn.net/xianpanjia4616/article/details/84405145">spark任务调度FIFO和FAIR的详解</a></p>
<p><a href="https://spark.apache.org/docs/latest/job-scheduling.html">Job Scheduling</a></p>
<p>转载请注明原文地址：<br><a href="https://liam-blog.ml/2019/11/07/spark-core-scheduler/">https://liam-blog.ml/2019/11/07/spark-core-scheduler/</a></p>
]]></content>
      <tags>
        <tag>Spark</tag>
        <tag>Spark Core</tag>
        <tag>Scheduler</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark Core 解析 4：内存模型</title>
    <url>/2020/02/29/spark-core-memory/</url>
    <content><![CDATA[<h2 id="Spark内存模型"><a href="#Spark内存模型" class="headerlink" title="Spark内存模型"></a>Spark内存模型</h2><p>Spark之所以快，很大程度上是因为它善于利用内存，大量利用内存进行存储和计算，从而减少磁盘IO，提升执行效率。</p>
<p>从1.6版本开始，Spark引入了统一内存管理模型（之前版本只有静态内存管理，这里不细说），找到两张图描述的很清楚：</p>
<span id="more"></span>

<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/spark-mem-on-heap.png" alt="spark-mem-on-heap.png"></p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/spark-mem-off-heap.png" alt="spark-mem-off-heap.png"></p>
<p>为什么有两张图呢，因为Spark既能使用JVM堆内存(on-heap)，也能使用堆外内存(off-heap)，两张图分别描述的是这两块内存的情况。图中已经把Spark对内存容量的划分很形象的说明了，网上相关文章也不少，不再赘述。</p>
<h2 id="几个需要注意的点"><a href="#几个需要注意的点" class="headerlink" title="几个需要注意的点"></a>几个需要注意的点</h2><ul>
<li><p>Storage与Execution区域之间的虚线，代表Storage与Execution内存的容量是可以动态变化的，比如Storage内存不足的时候，可以占用Execution的内存。但是，on-heap中的Storage不可以占用off-heap中的Execution内存，因为on-heap及off-heap整个的大小是固定的，没法互相占用。</p>
</li>
<li><p>Storage与Execution空间都不足时，都需要溢写至磁盘；Execution空间不足时，若有空间被Storage借用，该空间可以通过淘汰或转存磁盘的方式归还；Storage空间不足时，若有空间被Execution借用，则无法立即归还，只能等待用完释放。</p>
</li>
<li><p>Spark对堆内内存的管理是一种逻辑上的”规划式”的管理，因为对象实例占用内存的申请和释放都由JVM完成，Spark只能在申请后和释放前记录这些内存。说白了Spark只是个记账的，记录每次申请了多少内存，就能算出还剩多少内存。然鹅这个内存的帐并不是那么容易精确记录的，往往会对不上帐。首先，对于off-heap堆外内存来说，内存可以比较精确地申请和释放，问题不大。对于on-heap内存来说，序列化的对象可以精确计算大小，但非序列化的对象就只能估算了(出于性能考虑)，所以存在记账的内存大小不准的情况。另外，on-heap内存的回收是JVM自动进行的，账本上释放掉的内存空间，不一定已经被回收。因为记账的不准，所以即使进行了内存管理还是会有OOM的风险。</p>
</li>
</ul>
<h2 id="统一内存模型的实现"><a href="#统一内存模型的实现" class="headerlink" title="统一内存模型的实现"></a>统一内存模型的实现</h2><p>核心的实现类是UnifiedMemoryManager，其中维护了4个内存账本：<br>onHeapStorageMemoryPool，offHeapStorageMemoryPool，onHeapExecutionMemoryPool，offHeapExecutionMemoryPool，<br>分别对应啥一看名字就知道了。</p>
<p>虽然他们叫MemoryPool，还提供了releaseMemory、acquireMemory之类的方法，看上去挺虎，但实际上只是记录已用内存大小，进行数值上的加加减减而已。而Storage与Execution内存的互相借用，其实只是账本MemoryPool大小数值的调整而已。</p>
<h2 id="Execution执行内存"><a href="#Execution执行内存" class="headerlink" title="Execution执行内存"></a>Execution执行内存</h2><p>每个Executor有一个MemoryManager，用来管理该Executor节点的内存；<br>每个TaskAtempt都有一个TaskMemoryManger管理单次Task执行的内存；<br>每个TaskMemoryManger对当前TaskAtempt中的多个MemoryConsumer进行管理，负责分配内存给MemoryConsumer；<br>MemoryConsumer就是Spark中最终的执行内存消费者，是一个抽象类，比较典型的实现类有ExternalAppendOnlyMap、ShuffleExternalSorter、ExternalSorter等，不难发现这些类都是与shuffle相关的（还有一些实现类是Spark SQL相关的），也即是说执行内存基本就是shuffle使用的内存。</p>
<p>Shuffle相关可以参考<a href="https://liam-blog.ml/2019/12/29/spark-core-shuffle/">Spark Core 解析 3：Shuffle</a></p>
<h2 id="Storage存储内存"><a href="#Storage存储内存" class="headerlink" title="Storage存储内存"></a>Storage存储内存</h2><p>主要用于存储cache和broadcast的数据，为Spark的存储体系服务。<br>介绍存储体系的时候再细说。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/index.html">Apache Spark 内存管理详解</a></p>
]]></content>
      <tags>
        <tag>Spark</tag>
        <tag>Spark Core</tag>
        <tag>memory</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Redis实现分布式锁和分布式限流器</title>
    <url>/2020/03/29/redis-lock-rate-limiter/</url>
    <content><![CDATA[<p>分布式锁和分布式限流器应该是算是比较常见的需求了，而Redis现在几乎是应用的标配了，于是很多人会倾向于选择基于Redis来实现，因为不需要引入额外的依赖。</p>
<p>分布式锁和分布式限流器在Java领域比较成熟和常用的开源实现是Redisson(<a href="https://github.com/redisson/redisson/wiki/Redisson%E9%A1%B9%E7%9B%AE%E4%BB%8B%E7%BB%8D">中文官方介绍</a>)，下面从它的极小部分源码，分析下分布式锁和分布式限流器的实现逻辑。</p>
<span id="more"></span>
<h2 id="分布式锁的实现"><a href="#分布式锁的实现" class="headerlink" title="分布式锁的实现"></a>分布式锁的实现</h2><h3 id="加锁"><a href="#加锁" class="headerlink" title="加锁"></a>加锁</h3><p>Redisson中加锁实现的核心源码如下，为了实现操作的原子性，不得不使用eval命令加Lua脚本的方式。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">&lt;T&gt; RFuture&lt;T&gt; <span class="title function_">tryLockInnerAsync</span><span class="params">(<span class="type">long</span> leaseTime, TimeUnit unit, <span class="type">long</span> threadId, RedisStrictCommand&lt;T&gt; command)</span> &#123;</span><br><span class="line">    internalLockLeaseTime = unit.toMillis(leaseTime);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> commandExecutor.evalWriteAsync(getName(), LongCodec.INSTANCE, command,</span><br><span class="line">              <span class="string">&quot;if (redis.call(&#x27;exists&#x27;, KEYS[1]) == 0) then &quot;</span> +</span><br><span class="line">                  <span class="string">&quot;redis.call(&#x27;hincrby&#x27;, KEYS[1], ARGV[2], 1); &quot;</span> +</span><br><span class="line">                  <span class="string">&quot;redis.call(&#x27;pexpire&#x27;, KEYS[1], ARGV[1]); &quot;</span> +</span><br><span class="line">                  <span class="string">&quot;return nil; &quot;</span> +</span><br><span class="line">              <span class="string">&quot;end; &quot;</span> +</span><br><span class="line">              <span class="string">&quot;if (redis.call(&#x27;hexists&#x27;, KEYS[1], ARGV[2]) == 1) then &quot;</span> +</span><br><span class="line">                  <span class="string">&quot;redis.call(&#x27;hincrby&#x27;, KEYS[1], ARGV[2], 1); &quot;</span> +</span><br><span class="line">                  <span class="string">&quot;redis.call(&#x27;pexpire&#x27;, KEYS[1], ARGV[1]); &quot;</span> +</span><br><span class="line">                  <span class="string">&quot;return nil; &quot;</span> +</span><br><span class="line">              <span class="string">&quot;end; &quot;</span> +</span><br><span class="line">              <span class="string">&quot;return redis.call(&#x27;pttl&#x27;, KEYS[1]);&quot;</span>,</span><br><span class="line">                Collections.&lt;Object&gt;singletonList(getName()), internalLockLeaseTime, getLockName(threadId));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>把上面的Lua代码的逻辑翻译成redis命令组成的伪代码：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">//获取锁</span><br><span class="line"><span class="keyword">if</span> exists <span class="variable">$lockName</span> == 0</span><br><span class="line">  hset <span class="variable">$lockName</span> <span class="variable">$threadUid</span> 1</span><br><span class="line">  pexpire <span class="variable">$lockName</span> <span class="variable">$expireTime</span></span><br><span class="line">  <span class="built_in">return</span> nil</span><br><span class="line">//重入锁</span><br><span class="line"><span class="keyword">if</span> hexists <span class="variable">$lockName</span> <span class="variable">$threadUid</span> == 1</span><br><span class="line">   hincrby <span class="variable">$lockName</span> <span class="variable">$threadUid</span> 1</span><br><span class="line">   pexpire <span class="variable">$lockName</span> <span class="variable">$expireTime</span></span><br><span class="line">  <span class="built_in">return</span> nil</span><br><span class="line">//获取失败</span><br><span class="line"><span class="built_in">return</span> pttl <span class="variable">$lockName</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>现在逻辑很清晰了吧，注意这里的锁使用了hash结构而不是string，因为实现的是可重入锁，需要记录加锁的线程标识，并维持一个计数器。</p>
<p>有的需求是要线程一直等待直到获取到锁的，Redisson中通过subscribe命令订阅锁相关的一个channel，收到通知后会再次尝试获取锁。</p>
<p>关于可重入锁,我在<a href="https://liam-blog.ml/2019/07/21/Scala-Concurrency-in-Practice-2/">Scala并发编程实战 2：Lock 锁</a>中有解释过：</p>
<blockquote>
<p>所谓可重入锁，也就是说一个线程可以在持有该锁的时候，再次获取该锁。可重入锁通常与一个计数器关联，第一次获取锁的时候，计数器从0变为1，再次获取锁，变为2，以此类推。释放锁的时候，计数器每次减1，直至减为0，该锁才真正释放给其他线程。</p>
</blockquote>
<p>另外需要注意的是，这里实现的锁是有过期时间的，如果需要一直持有锁，redisson中的实现是，会用一个定时任务不断去刷新过期时间。(可以想想，为什么不用一个永不过期的key来实现呢？)</p>
<h3 id="解锁"><a href="#解锁" class="headerlink" title="解锁"></a>解锁</h3><p>下面看解锁的源码。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">protected</span> RFuture&lt;Boolean&gt; <span class="title function_">unlockInnerAsync</span><span class="params">(<span class="type">long</span> threadId)</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> commandExecutor.evalWriteAsync(getName(), LongCodec.INSTANCE, RedisCommands.EVAL_BOOLEAN,</span><br><span class="line">            <span class="string">&quot;if (redis.call(&#x27;hexists&#x27;, KEYS[1], ARGV[3]) == 0) then &quot;</span> +</span><br><span class="line">                <span class="string">&quot;return nil;&quot;</span> +</span><br><span class="line">            <span class="string">&quot;end; &quot;</span> +</span><br><span class="line">            <span class="string">&quot;local counter = redis.call(&#x27;hincrby&#x27;, KEYS[1], ARGV[3], -1); &quot;</span> +</span><br><span class="line">            <span class="string">&quot;if (counter &gt; 0) then &quot;</span> +</span><br><span class="line">                <span class="string">&quot;redis.call(&#x27;pexpire&#x27;, KEYS[1], ARGV[2]); &quot;</span> +</span><br><span class="line">                <span class="string">&quot;return 0; &quot;</span> +</span><br><span class="line">            <span class="string">&quot;else &quot;</span> +</span><br><span class="line">                <span class="string">&quot;redis.call(&#x27;del&#x27;, KEYS[1]); &quot;</span> +</span><br><span class="line">                <span class="string">&quot;redis.call(&#x27;publish&#x27;, KEYS[2], ARGV[1]); &quot;</span> +</span><br><span class="line">                <span class="string">&quot;return 1; &quot;</span>+</span><br><span class="line">            <span class="string">&quot;end; &quot;</span> +</span><br><span class="line">            <span class="string">&quot;return nil;&quot;</span>,</span><br><span class="line">            Arrays.&lt;Object&gt;asList(getName(), getChannelName()), LockPubSub.UNLOCK_MESSAGE, internalLockLeaseTime, getLockName(threadId));</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>核心也是Lua实现，翻译下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">//锁是否存在</span><br><span class="line"><span class="keyword">if</span> exists <span class="variable">$lockName</span> == 0</span><br><span class="line">   publish <span class="variable">$channel</span> 0</span><br><span class="line">   <span class="built_in">return</span> 1</span><br><span class="line">//是否持有锁</span><br><span class="line"><span class="keyword">if</span> hexists <span class="variable">$lockName</span> <span class="variable">$threadUid</span> ==0</span><br><span class="line">  <span class="built_in">return</span> nil</span><br><span class="line"></span><br><span class="line">//减少持有计数</span><br><span class="line">counter = hincrby <span class="variable">$lockName</span> <span class="variable">$threadUid</span> -1</span><br><span class="line"><span class="keyword">if</span> counter &gt; 0</span><br><span class="line">  //刷新锁</span><br><span class="line">  pexpire <span class="variable">$lockName</span> <span class="variable">$expireTime</span></span><br><span class="line">  <span class="built_in">return</span> 0</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  //释放锁</span><br><span class="line">  del <span class="variable">$lockName</span></span><br><span class="line">  publish <span class="variable">$channel</span> 0</span><br><span class="line">  <span class="built_in">return</span> 1</span><br></pre></td></tr></table></figure>

<p>逻辑应该容易看懂，其中publish是为了通知其他线程锁释放了，可以来抢啦。</p>
<h2 id="分布式限流器的实现"><a href="#分布式限流器的实现" class="headerlink" title="分布式限流器的实现"></a>分布式限流器的实现</h2><p>如果一个服务有多个实例，做限流的话，通常有两种方式，一种是对每个实例单独限制，一种是对所有实例整体的流量做限制。<br>Redisson中对这两种都有实现，以限流器的类型来区分，第一种称作PER_CLIENT，第二种为OVERALL。</p>
<p>限流算法有若干种，Redisson采用的是令牌桶算法，核心实现同样是Lua：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> &lt;T&gt; RFuture&lt;T&gt; <span class="title function_">tryAcquireAsync</span><span class="params">(RedisCommand&lt;T&gt; command, Long value)</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> commandExecutor.evalWriteAsync(getName(), LongCodec.INSTANCE, command,</span><br><span class="line">            <span class="string">&quot;local rate = redis.call(&#x27;hget&#x27;, KEYS[1], &#x27;rate&#x27;);&quot;</span></span><br><span class="line">          + <span class="string">&quot;local interval = redis.call(&#x27;hget&#x27;, KEYS[1], &#x27;interval&#x27;);&quot;</span></span><br><span class="line">          + <span class="string">&quot;local type = redis.call(&#x27;hget&#x27;, KEYS[1], &#x27;type&#x27;);&quot;</span></span><br><span class="line">          + <span class="string">&quot;assert(rate ~= false and interval ~= false and type ~= false, &#x27;RateLimiter is not initialized&#x27;)&quot;</span></span><br><span class="line">          </span><br><span class="line">          + <span class="string">&quot;local valueName = KEYS[2];&quot;</span></span><br><span class="line">          + <span class="string">&quot;if type == &#x27;1&#x27; then &quot;</span></span><br><span class="line">              + <span class="string">&quot;valueName = KEYS[3];&quot;</span></span><br><span class="line">          + <span class="string">&quot;end;&quot;</span></span><br><span class="line">          </span><br><span class="line">          + <span class="string">&quot;local currentValue = redis.call(&#x27;get&#x27;, valueName); &quot;</span></span><br><span class="line">          + <span class="string">&quot;if currentValue ~= false then &quot;</span></span><br><span class="line">                 + <span class="string">&quot;if tonumber(currentValue) &lt; tonumber(ARGV[1]) then &quot;</span></span><br><span class="line">                     + <span class="string">&quot;return redis.call(&#x27;pttl&#x27;, valueName); &quot;</span></span><br><span class="line">                 + <span class="string">&quot;else &quot;</span></span><br><span class="line">                     + <span class="string">&quot;redis.call(&#x27;decrby&#x27;, valueName, ARGV[1]); &quot;</span></span><br><span class="line">                     + <span class="string">&quot;return nil; &quot;</span></span><br><span class="line">                 + <span class="string">&quot;end; &quot;</span></span><br><span class="line">          + <span class="string">&quot;else &quot;</span></span><br><span class="line">                 + <span class="string">&quot;assert(tonumber(rate) &gt;= tonumber(ARGV[1]), &#x27;Requested permits amount could not exceed defined rate&#x27;); &quot;</span></span><br><span class="line">                 + <span class="string">&quot;redis.call(&#x27;set&#x27;, valueName, rate, &#x27;px&#x27;, interval); &quot;</span></span><br><span class="line">                 + <span class="string">&quot;redis.call(&#x27;decrby&#x27;, valueName, ARGV[1]); &quot;</span></span><br><span class="line">                 + <span class="string">&quot;return nil; &quot;</span></span><br><span class="line">          + <span class="string">&quot;end;&quot;</span>,</span><br><span class="line">            Arrays.asList(getName(), getValueName(), getClientValueName()),</span><br><span class="line">            value);    </span><br></pre></td></tr></table></figure>

<p>简化为伪代码：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">//令牌桶容量</span><br><span class="line"><span class="variable">$rate</span> = hget <span class="variable">$configKey</span> rate</span><br><span class="line">//间隔（比如rate=10,interval=1000,就代表1000毫秒中允许10次，即QPS为10）</span><br><span class="line"><span class="variable">$interval</span> = hget <span class="variable">$configKey</span> interval</span><br><span class="line">//类型</span><br><span class="line"><span class="variable">$type</span> = hget <span class="variable">$configKey</span> <span class="built_in">type</span></span><br><span class="line">//检查限流器是否已经初始化</span><br><span class="line">assert(<span class="variable">$rate</span>,<span class="variable">$interval</span>,<span class="variable">$type</span>都存在)</span><br><span class="line"></span><br><span class="line">//<span class="variable">$valueName</span>是存储令牌桶的key</span><br><span class="line"><span class="variable">$valueName</span> = <span class="variable">$valueKey</span></span><br><span class="line">//如果是PER_CLIENT类型</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">type</span> == <span class="string">&#x27;1&#x27;</span></span><br><span class="line">   <span class="variable">$valueName</span> = <span class="variable">$clientValueKey</span></span><br><span class="line"></span><br><span class="line">//<span class="variable">$currentValue</span>是当前令牌数，<span class="variable">$acquireValue</span>是申请的令牌数</span><br><span class="line"><span class="variable">$currentValue</span> = get <span class="variable">$valueName</span></span><br><span class="line"><span class="keyword">if</span> <span class="variable">$currentValue</span> != <span class="literal">false</span></span><br><span class="line">    //令牌桶已经存在</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="variable">$currentValue</span> &lt; <span class="variable">$acquireValue</span></span><br><span class="line">        //令牌小于申请数，申请令牌失败</span><br><span class="line">        <span class="built_in">return</span> pttl <span class="variable">$valueName</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        //令牌 减去 申请数</span><br><span class="line">        decrby <span class="variable">$valueName</span> <span class="variable">$acquireValue</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    //令牌桶不存在</span><br><span class="line"></span><br><span class="line">    //检查申请数不能大于令牌桶容量</span><br><span class="line">    assert(<span class="variable">$rate</span>&gt;<span class="variable">$acquireValue</span>)</span><br><span class="line">    //创建令牌桶</span><br><span class="line">    <span class="built_in">set</span> <span class="variable">$valueName</span> <span class="variable">$rate</span> px <span class="variable">$interval</span></span><br><span class="line">    decrby <span class="variable">$valueName</span> <span class="variable">$acquireValue</span></span><br><span class="line">    <span class="built_in">return</span> nil</span><br></pre></td></tr></table></figure>

<p>值得一提的是，令牌桶的配置信息也保存到了redis中，可以看到rate，interval，type信息都是从一个hash结构中获取的，从获取令牌桶配置到更新令牌桶整个过程是原子化的。这是考虑到多个应用实例的情况下，每个实例拿到的限流器的配置可能出现不一致的情况，特别是在修改限流器配置的时候。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://github.com/redisson/redisson">Redisson github</a><br>转载请注明原文地址：<a href="https://liam-blog.ml/2020/03/29/redis-lock-rate-limiter/">Liam’s Blog</a></p>
]]></content>
      <tags>
        <tag>redis</tag>
        <tag>lock</tag>
        <tag>rate limiter</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark Core 解析 3：Shuffle</title>
    <url>/2019/12/29/spark-core-shuffle/</url>
    <content><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>所谓shuffle就是将数据按新的规则进行分区的过程，将数据分区从旧分区转变成新分区。</p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/20191229115009.png" alt="20191229115009.png"></p>
<span id="more"></span>

<p>这个过程分为两个阶段，第一个阶段称为shuffle write，每个计算节点需要对自己持有的那部分数据按新分区规则进行重新分区，并按新分区写入文件。<br>shuffle write完成后，同一个新分区的数据会分散在不同的节点上，这样是不利于接下来的计算的，必须把同一个新分区的数据汇集到一个节点才行，所以需要第二阶段shuffle read。<br>shuffle read阶段将按照新分区，拉取shuffle write生成的文件，相同新分区的数据汇集到同一节点，组合成新的分区，这样才算是实现了新的数据分区。</p>
<p>Spark中负责实现shuffle的组件是ShuffleManger，其核心是ShuffleWriter和ShuffleReader两块，分别负责shuffle write和read的实现。</p>
<p>从下图可以看出，Spark的ShuffleManger经过不断的发展，到目前只剩下了一种即SortShuffleManager，本文的分析即是基于这种实现。</p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/Shuffle-history.png" alt="Shuffle-history.png"></p>
<h3 id="ShuffleWriter概述"><a href="#ShuffleWriter概述" class="headerlink" title="ShuffleWriter概述"></a>ShuffleWriter概述</h3><p>顾名思义，ShuffleWriter就是Spark中负责shuffle write阶段的组件啦。</p>
<p>Spark2.x中虽然只有一种Shuffle管理器(SortShuffleManager)，但是支持3种不同行为的ShuffleWriter，分别是SortShuffleWriter、BypassMergeSortShuffleWriter、UnsafeShuffleWriter。</p>
<p>下面先简单介绍下这几种ShuffleWriter，然后再做详细分析。</p>
<h4 id="BypassMergeSortShuffleWriter"><a href="#BypassMergeSortShuffleWriter" class="headerlink" title="BypassMergeSortShuffleWriter"></a>BypassMergeSortShuffleWriter</h4><p>绕过排序和聚合的ShuffleWriter，这个是shuffle write最朴素的实现。<br>它将数据按新分区写到不同的文件，最后再把这些文件合并成一个文件，同时生成一个索引文件来标识数据块与分区的对应关系。</p>
<h4 id="SortShuffleWriter"><a href="#SortShuffleWriter" class="headerlink" title="SortShuffleWriter"></a>SortShuffleWriter</h4><p>这是基于排序的ShuffleWriter，在BypassMergeSortShuffleWriter的基础之上，它可以支持对数据进行聚合，而且会对数据进行分区排序。</p>
<p>PS:为什么shuffle write阶段要对数据进行排序？<br>因为reduce操作通常会要对数据排序，如果在map端进行一次初排，可以减轻reduce的压力。</p>
<h4 id="UnsafeShuffleWriter"><a href="#UnsafeShuffleWriter" class="headerlink" title="UnsafeShuffleWriter"></a>UnsafeShuffleWriter</h4><p>可以使用堆外内存的ShuffleWriter，它的行为相当于不带聚合功能的SortShuffleWriter，但是可以使用堆外内存提高性能。</p>
<p>另一个优化点是，支持对序列化后的二进制数据排序，不仅能减少内存的消耗(因为序列化后的数据更紧凑)，也能避免多次序列化反序列化(溢出到文件时需要序列化，合并文件时需要反序列化)的性能损耗。</p>
<h4 id="会采用哪个ShuffleWriter"><a href="#会采用哪个ShuffleWriter" class="headerlink" title="会采用哪个ShuffleWriter"></a>会采用哪个ShuffleWriter</h4><p>Spark自身会按如下规则对ShuffleWriter进行选择，从上往下依次判断：</p>
<ul>
<li>1 当分区数(按新规则的)小于等于配置<code>spark.shuffle.sort.bypassMergeThreshold(默认为200)</code>的值，使用BypassMergeSortShuffleWriter；</li>
<li>2 如果当前依赖的serializer的序列化结果支持重排(后面解释)，则使用UnsafeShuffleWriter；</li>
<li>3 否则采用SortShuffleWriter。</li>
</ul>
<h3 id="ShuffleReader概述"><a href="#ShuffleReader概述" class="headerlink" title="ShuffleReader概述"></a>ShuffleReader概述</h3><p>ShuffleReader只有一种即BlockStoreShuffleReader，顾名思义就是从BlockStore读取shuffle数据。<br>在进行shuffle read时，会异步拉取数据到内存或磁盘(大小超过一定阈值时)。拉取数据的同时对数据进行聚合(如果定义了聚合器)和排序(如果定义了排序器)操作。</p>
<p>到此为止，<strong>基本概念讲解完毕，下面开始讲原理。</strong></p>
<h2 id="Sorter-排序器"><a href="#Sorter-排序器" class="headerlink" title="Sorter 排序器"></a>Sorter 排序器</h2><p>在shuffle过程中，有一个步骤不仅比较耗费资源，也是性能瓶颈之一，这个步骤就是排序，用来排序的组件称为Sorter。</p>
<p>要理解这3种ShuffleWriter的区别，必须先了解Sorter，因为他们的排序过程依赖不同的Sorter，导致他们的行为各异。</p>
<p>SortShuffleWriter使用的是ExternalSorter，<br>UnsafeShuffleWriter采用的是ShuffleExternalSorter，BypassMergeSortShuffleWriter则没有使用Sorter。</p>
<h3 id="ExternalSorter-外部排序器"><a href="#ExternalSorter-外部排序器" class="headerlink" title="ExternalSorter 外部排序器"></a>ExternalSorter 外部排序器</h3><p>除了排序，这个排序器还支持数据的聚合和溢写到磁盘。它使用到了几个重要的<strong>数据结构</strong>，下面详细介绍下。</p>
<h4 id="AppendOnlyMap-（划重点）"><a href="#AppendOnlyMap-（划重点）" class="headerlink" title="AppendOnlyMap （划重点）"></a>AppendOnlyMap （划重点）</h4><p>AppendOnlyMap是一个支持新增和修改的哈希表，但是不支持删除元素。</p>
<p>底层存储结构很简单，就是一个数组<code>Array[AnyRef]</code>，元素的key和value紧挨着存放，像这样：<code>key0,value0,null,null,...,key1,value1,...</code>。假如AppendOnlyMap的容量为capacity，那么这个数组的大小就是2 * capacity。</p>
<p>key到数组索引的映射也很简单，就是对key计算hash值，然后对容量取模，最后乘以2，<code>hash(key) % capacity * 2</code>。如果发生了哈希冲突，则继续探测后面的位置，第一次探测+2的位置，第二次探测+2+4的位置，以此类推，可以总结出一个公式 $idx+2*\sum_{i&#x3D;1}^{k}i$ ，其中idx是根据hash值计算出的位置，k代表探测次数。</p>
<h5 id="获取元素"><a href="#获取元素" class="headerlink" title="获取元素"></a>获取元素</h5><p>如何取出元素呢，比如数据组名为data，key计算出索引为4，如果data(4)&#x3D;&#x3D;key，那么对应的值就是data(5)，如果data(4)!&#x3D;key，那继续检查data(4+2)、data(4+2+4)等等是否等于key。</p>
<p>PS：支持key为null的元素，单独存储在一个变量中；<br>容量有个约束，它一定是2的幂。</p>
<h5 id="插入元素"><a href="#插入元素" class="headerlink" title="插入元素"></a>插入元素</h5><p>先根据上面的方法，计算出key对应的数据索引idx，如果idx上没有存储数据，那就直接在idx存下key，idx+1的位置存下value。如果idx上已经存了数，刚好就是要插入的key值，那也好办，直接更新value即可。</p>
<p>若idx上已经存了另一个key值，那就发生冲突了。接下来尝试idx+2的位置，如果继续冲突，则继续尝试idx+2+4、idx+2+4+6、…的位置，直到成功。</p>
<h5 id="聚合元素"><a href="#聚合元素" class="headerlink" title="聚合元素"></a>聚合元素</h5><p>用来修改元素的API是<code>changeValue(key: K, updateFunc: (Boolean, V) =&gt; V): V</code>，处理流程基本与插入元素一样，只是当key已经存在时，会用updateFunc处理旧值与新值，如果传入的updateFunc是将新值与旧值merge的函数，那么就可以用changeValue方法来实现元素聚合的目的。</p>
<h5 id="内置排序"><a href="#内置排序" class="headerlink" title="内置排序"></a>内置排序</h5><p>进行排序的API是<code>destructiveSortedIterator(keyComparator: Comparator[K]): Iterator[(K, V)]</code>，需要传入用来比较key的比较器，返回一个迭代器。</p>
<p>函数首先要对底层数组进行整理，将各元素向前移动，使得元素紧挨着排列，也就是把数组中空着的位置用后面的元素填上，然后使用TimSort排序算法(插入排序与归并排序结合的一种算法)对数组进行排序(使用传入的Comparator比较key)，最后输出这个排序后数组的迭代器。</p>
<p>需要注意的是，经过排序后，AppendOnlyMap变成了destroyed状态，不再支持新增和修改元素，只能通过迭代器访问元素。</p>
<h5 id="小结一下"><a href="#小结一下" class="headerlink" title="小结一下"></a>小结一下</h5><p>AppendOnlyMap是一个类似map的数据结构，支持对相同元素进行聚合，同时也可以对元素进行排序，但是不支持删除元素。</p>
<h4 id="PartitionedAppendOnlyMap"><a href="#PartitionedAppendOnlyMap" class="headerlink" title="PartitionedAppendOnlyMap"></a>PartitionedAppendOnlyMap</h4><p>是AppendOnlyMap的子类，继承了AppendOnlyMap的功能，还另外实现了SizeTracker和WritablePartitionedPairCollection特质，主要是增加了集合大小估算(使用内存大小)、按分区和key排序、写入磁盘的功能。</p>
<h4 id="PartitionedPairBuffer"><a href="#PartitionedPairBuffer" class="headerlink" title="PartitionedPairBuffer"></a>PartitionedPairBuffer</h4><p>底层是与AppendOnlyMap使用的一样的数组，但是元素不按哈希值存储，而是顺序存储(这样排序前就不需要整理数据了)，同样支持排序，但是不支持对元素的聚合(因为采用的存储方式不支持随机存取元素)。<br>与PartitionedAppendOnlyMap类似，支持集合大小估算(使用内存大小)、按分区和key排序、写入磁盘的功能。</p>
<h4 id="ExternalSorter-原理"><a href="#ExternalSorter-原理" class="headerlink" title="ExternalSorter 原理"></a>ExternalSorter 原理</h4><p>ExternalSorter使用了PartitionedAppendOnlyMap和PartitionedPairBuffer作为<strong>数据buffer</strong>，或者说数据容器：</p>
<ul>
<li>当需要进行map端聚合的时候，使用PartitionedAppendOnlyMap存储数据，这样就拥有了对数据进行聚合和分区排序功能；</li>
<li>当不需要进行map端聚合的时候，使用PartitionedPairBuffer，就获得了性能更好(因为不需要整理数据)的分区排序功能。</li>
</ul>
<p>总的来说，ExternalSorter的功能有：缓冲数据、分区排序、输出排序后的数据到文件。</p>
<h5 id="溢写"><a href="#溢写" class="headerlink" title="溢写"></a>溢写</h5><p>另外，ExternalSorter实现了将数据溢出到磁盘的功能，这样即使内存无法装下所有的数据，也能进行分区排序。溢写到磁盘的数据，最终会与内存中的数据合并后再持久化到磁盘。</p>
<p>所谓溢写，是指内存装不下不断增加的数据了，只好把内存中的数据写到磁盘的过程。<br>ExternalSorter的溢写过程大致是：</p>
<ul>
<li>1 当buffer内存使用量大于阈值，并且申请不到更多内存的时候，启动溢写；</li>
<li>2 调用buffer的排序函数，并将数据写入磁盘；</li>
<li>3 创建一个新的buffer，继续接受新数据。</li>
</ul>
<p>输出最终结果的时候，会把所有的溢写文件，连同内存中的数据，通过归并排序(merge sort)合并成一个文件。</p>
<h3 id="ShuffleExternalSorter"><a href="#ShuffleExternalSorter" class="headerlink" title="ShuffleExternalSorter"></a>ShuffleExternalSorter</h3><p>这也是一种外部排序器，支持分区排序和数据溢出到磁盘，但是不支持数据聚合，也不支持合并溢写文件。溢写文件合并的工作由UnsafeShuffleWriter完成。<br>比较特别的是，这个排序器可以使用堆外(off-heap)内存，因为使用了Unsafe类操作内存。Unsafe类不仅能操作off-heap内存，也能操作on-heap内存，所以ShuffleExternalSorter是既能使用堆内内存，也能使用堆外内存的。</p>
<h3 id="ShuffleReader"><a href="#ShuffleReader" class="headerlink" title="ShuffleReader"></a>ShuffleReader</h3><p>ShuffleReader只有一种即BlockStoreShuffleReader。<br>在进行shuffle read时，会先创建ShuffleBlockFetcherIterator，通过ShuffleClient开始异步拉取数据到内存或磁盘(大小超过一定阈值时)。拉取数据的同时对数据进行聚合(如果定义了聚合器)和排序(如果定义了排序器)操作。其中聚合操作使用的是ExternalAppendOnlyMap，排序则是使用ExternalSorter。</p>
<h2 id="经典的SortShuffle过程"><a href="#经典的SortShuffle过程" class="headerlink" title="经典的SortShuffle过程"></a>经典的SortShuffle过程</h2><h3 id="简化版本"><a href="#简化版本" class="headerlink" title="简化版本"></a>简化版本</h3><p>shuffle write阶段：</p>
<ul>
<li>1 SortShuffleWriter不断从RDD的迭代器取出数据并存入buffer。如果有定义聚合操作，会采用PartitionedAppendOnlyMap作为buffer，边插入边聚合数据，否则使用PartitionedPairBuffer，只插入数据不做聚合。存入buffer过程中，如果内存不够，会触发spill，将数据排序后溢写到磁盘；</li>
<li>2 对buffer中的数据按新分区和key进行排序，如果有spill文件，则进行合并，然后输出一个数据文件(data file)和一个索引文件(index file)，返回MapStatus。</li>
</ul>
<p>shuffle read阶段：</p>
<ul>
<li>1 BlockStoreShuffleReader先通过mapOutputTracker获取需要拉取的block信息，然后开始拉取数据到buffer中；</li>
<li>2 拉取数据的同时，如果该ShuffleDependency有定义聚合器，则会通过ExternalAppendOnlyMap对数据进行聚合；接下来，如果该ShuffleDependency有定义排序器，则会使用ExternalSorter进行排序，最终会返回一个迭代器。</li>
</ul>
<h3 id="详细版本"><a href="#详细版本" class="headerlink" title="详细版本"></a>详细版本</h3><p>假若使用的是SortShuffleWriter和BlockStoreShuffleReader，接下来对照2.4版本的源码，做个整体流程分析。</p>
<p>这一切从ShuffleMapTask开始… （不知道什么是ShuffleMapTask，请看<a href="https://liam-blog.ml/2019/11/07/spark-core-scheduler/">Spark Core解析 2：Scheduler 调度体系</a>）</p>
<ul>
<li>1 ShuffleMapTask的runTask方法被Executor的某个线程执行，<strong>task开始执行</strong>；</li>
<li>2 runTask方法调用SortShuffleManager.getWriter方法，创建SortShuffleWriter实例，然后调用SortShuffleWriter.write方法，<strong>开启shuffle write阶段</strong>；</li>
<li>3 write方法中会new一个ExternalSorter实例，并调用这个实例的insertAll方法，并传入RDD(所属stage最后一个RDD)对应的一个分区的Iterator，<strong>进入数据缓冲聚合阶段</strong>；</li>
<li>4 insertAll方法中，每从Iterator中取一个值，就把这个值存入内存中的缓冲数据结构buffer。从Iterator中取值会触发RDD上用户定义的计算逻辑，取出的这一条数据其实是从数据源读出并经过各种父RDD的计算逻辑后得到的结果。如果有定义聚合操作，会采用PartitionedAppendOnlyMap作为buffer，边插入边聚合数据，否则使用PartitionedPairBuffer，只插入数据不做聚合。存入buffer过程中，如果内存不够，会触发spill，将数据排序后溢写到磁盘，然后创建新的buffer，继续存入数据；</li>
<li>5 <strong>数据全部存入buffer后，开始排序输出阶段。</strong>SortShuffleWriter.write方法调用ExternalSorter.writePartitionedFile方法，对buffer中的数据按新分区和key进行排序(如果有spill文件，则利用heap结构进行merge sort)，然后输出一个数据文件(data file)和一个索引文件(index file)，索引文件中记录了数据文件中每个分区的数据开始的offset。（注意：buffer的输入数据虽然只有一个分区，但是输出会按新分区方式输出多个分区的数据。）</li>
<li>6 <strong>write方法写完数据后</strong>，返回MapStatus（包含shuffleServerId和分区信息），最终MapStatus会发送给DAGScheduler，然后被MapOutputTracker记录任务信息，</li>
<li>7 <strong>shuffle write结束，下面说说shuffle read。</strong><br>shuffle read既可能发生在ShuffleMapTask也可能发生在ResultTask，这取决于task所属stage包含的RDD的dependency<br>是否含有ShuffleDependency，对于ShuffleDependency，RDD的compute函数会调用BlockStoreShuffleReader.read方法，开启read流程。</li>
<li>8 read函数中会先通过mapOutputTracker获取需要拉取的block信息，然后创建ShuffleBlockFetcherIterator，ShuffleBlockFetcherIterator被创建后会立即开始拉取数据(通过ShuffleClient)，拉取过程是异步的，拉取到的数据会暂存到buffer中。</li>
<li>9 拉取数据的同时，如果该ShuffleDependency有定义聚合器，则会通过ExternalAppendOnlyMap对数据进行聚合；接下来，如果该ShuffleDependency有定义排序器，则会使用ExternalSorter进行排序。</li>
<li>10 read函数最终会返回一个Iterator，成为task的数据来源。</li>
</ul>
<p>小结：如果有n个ShuffleMapTask，write阶段最终会生成2n个临时文件(不含spill文件)，read阶段除了spill文件不会产生临时数据文件。</p>
<h2 id="黑科技UnsafeShuffle过程"><a href="#黑科技UnsafeShuffle过程" class="headerlink" title="黑科技UnsafeShuffle过程"></a>黑科技UnsafeShuffle过程</h2><p>UnsafeShuffleWriter.write方法，间接调用ShuffleExternalSorter.insertRecord方法，传入<em>序列化后</em>的数据record，<strong>数据缓冲过程开始</strong>；</p>
<ul>
<li>缓冲阶段第1步，根据情况进行溢写。<br>ShuffleExternalSorter.insertRecord方法中，先检查记录数，达到阈值，则进行spill溢写；溢写过程大致为：调用ShuffleInMemorySorter.getSortedIterator对存入的recordAddress地址排序，这里只是按partitionId排序(RadixSort和TimSort两种算法)，按照排序好的recordAddress，依次输出数据(仍然是序列化后的)到一个文件，并记录每个partition的offset到SpillInfo中；</li>
<li>缓冲阶段第2步，申请内存块page。<br>如果ShuffleExternalSorter的currentPage空间不足，会先申请page(即MemoryBlock)；申请过程会依次使用到TaskMemoryManager.allocatePage和MemoryManager.tungstenMemoryAllocator().allocate，依据MemoryManager使用的MemoryAllocator是HeapMemoryAllocator还是UnsafeMemoryAllocator，申请到的MemoryBlock是on-heap和off-heap类型地址的其中一种；</li>
<li>缓冲阶段第3步，数据写入page。<br>currentPage空间足够后，依次将record字节大小，record本身复制到currentPage中以pageCursor开始的地址段中，并增加pageCursor游标地址；</li>
<li>缓冲阶段第4步，保存record地址。<br>通过taskMemoryManager.encodePageNumberAndOffset得到recordAddress，连同partitionId存入ShuffleInMemorySorter；</li>
</ul>
<p>当数据全部缓冲完毕，<strong>开始排序输出阶段</strong>。</p>
<ul>
<li>接下来write方法会间接调用ShuffleExternalSorter.writeSortedFile，将内存中剩下的数据全部溢写；</li>
<li>UnsafeShuffleWriter.mergeSpills方法将spill的文件全部合并成一个，这里会利用之前保存的SpillInfo，将相同partition的数据合到一起；(这里合并有三种实现：transferTo-based fast merge，fileStream-based fast merge，slow merge)</li>
<li>通过shuffleBlockResolver.writeIndexFileAndCommit创建索引文件；</li>
<li>返回mapStatus；</li>
<li><strong>shuffle write阶段结束，read阶段与SortShuffle相同</strong></li>
</ul>
<p>小结：<br>如果配置了<code>spark.memory.offHeap.enabled</code>和<code>spark.memory.offHeap.size</code>参数，则会使用off-heap缓冲数据；</p>
<p>数据和地址指针分开存储，排序只是对地址指针进行。</p>
<p>UnsafeShuffleWriter的黑科技主要体现在：</p>
<ul>
<li>write全程只在第一步做了数据序列化，后面操作的都是序列化后的数据，甚至写文件时也没有再进行序列化；</li>
<li>使用off-heap内存存储数据，大大减少JVM GC压力，而且避免了IO时数据在堆内堆外内存之间的拷贝；</li>
<li>使用地址指针进行排序，减少了序列化反序列化的性能消耗；</li>
<li>spill文件合并时，‘transferTo-based fast merge’使用NIO提供的transferTo方法，文件数据直接在内核空间(kernel-space)拷贝合并，提升了性能。</li>
</ul>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://book.douban.com/subject/30157181/">Spark内核设计的艺术</a><br>转载请注明原文地址：<a href="https://liam-blog.ml/2019/12/29/spark-core-shuffle/">Liam’s Blog</a></p>
]]></content>
      <tags>
        <tag>Spark</tag>
        <tag>Spark Core</tag>
        <tag>shuffle</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka Fundamental Concepts</title>
    <url>/2021/05/08/Kafka-Fundamental-Concepts/</url>
    <content><![CDATA[<h1 id="What-is-Kafka"><a href="#What-is-Kafka" class="headerlink" title="What is Kafka?"></a>What is Kafka?</h1><blockquote>
<p><a href="https://www.confluent.io/what-is-apache-kafka/">Apache Kafka</a> is an event streaming platform used to collect, process, store, and integrate data at scale. It has numerous use cases including distributed logging, stream processing, data integration, and pub&#x2F;sub messaging.</p>
</blockquote>
<span id="more"></span>

<h1 id="Kafka-Architecture-Fundamental-Concepts"><a href="#Kafka-Architecture-Fundamental-Concepts" class="headerlink" title="Kafka Architecture - Fundamental Concepts"></a>Kafka Architecture - Fundamental Concepts</h1><p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/1503038310607_9038_1503038310779.png" alt="1503038310607_9038_1503038310779"></p>
<ul>
<li>Topic: Use different topics to hold different kinds of events.Here we have only one topic named TopicA.</li>
<li>Partitioning &#x2F; Topic-Partition: Partitioning takes the single topic log and breaks it into multiple logs, each of which can live on a separate node in the Kafka cluster. This way, the work of storing messages, writing new messages, and processing existing messages can be split among many nodes in the cluster.</li>
<li>Broker: Kafka is composed of a network of machines called brokers.</li>
<li>Producer and Consumer: These are client applications that contain your code, putting messages into topics and reading messages from topics. </li>
<li>Zookeeper: A external service to store metadata of Kafka.</li>
<li>Consumer Group: We can put consumers in a group to avoid that different consumers get same message.</li>
<li>Replication, leader and follower:  We need to copy partition data to several other brokers to keep it safe. Those copies are called <strong>follower</strong> replicas, whereas the main partition is called the <strong>leader</strong> replica. When you produce data to the leader—in general, reading and writing are done to the leader—the leader and the followers work together to replicate those new writes to the followers.</li>
</ul>
<h1 id="More-about-leader-and-follower"><a href="#More-about-leader-and-follower" class="headerlink" title="More about leader and follower"></a>More about leader and follower</h1><p>Let’s look into only one partition.</p>
<p>All replicas, including leader and followers, are called <strong>AR</strong> (Assigned Replicas). After leader received some messages, followers will pull these messages from leader, usually followers can sync up the messages quickly, these replicas are called <strong>ISR</strong> (In-Sync Replicas). If the replica lag too much, it’s going to be <strong>OSR</strong> (Out-of-Sync Replicas). Definitely, the number of AR equal to ISR + OSR.</p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/image-20210505104957944.png" alt="image-20210505104957944"></p>
<p>In each partition, Kafka use offset to locate messages, just like the index of array.</p>
<p><strong>LEO</strong> is short for Log End Offset, it means the next offset to append new message. In the  picture, the leader has received 5 messages, so the LEO is 5. The LEO of follower1 is 5 as well. However, the follower2 haven’t caught up the latest message, so its LEO is 4.</p>
<p>In this case, the consumers can only get messages from 0th to 3rd for the time being. Kafka only allow consumers get messages which have been saved by all ISRs. In order to indicate the range of messages which can be consumed, Kafka introduced <strong>HW</strong>( High Watermark).It should be 4 here, like the LEO using the next offset of latest message.</p>
<p>To show the details of partitions, you can use the command below.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$KAFKA_HOME/bin/kafka-topics.sh --describe --topic test --bootstrap-server &lt;your brokers&gt;</span><br><span class="line">Topic: test	PartitionCount: 4	ReplicationFactor: 3	Configs: segment.bytes=1073741824</span><br><span class="line">	Topic: test	Partition: 0	Leader: 1005	Replicas: 1005,1004,1006	Isr: 1005,1004,1006</span><br><span class="line">	Topic: test	Partition: 1	Leader: 1006	Replicas: 1006,1005,1004	Isr: 1006,1005,1004</span><br><span class="line">	Topic: test	Partition: 2	Leader: 1004	Replicas: 1004,1006,1005	Isr: 1004,1006,1005</span><br><span class="line">	Topic: test	Partition: 3	Leader: 1005	Replicas: 1005,1006,1004	Isr: 1005,1006,1004</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>How to solve dependency conflicts with Maven</title>
    <url>/2021/09/24/How-to-solve-dependency-conflicts-with-Maven/</url>
    <content><![CDATA[<h2 id="What-are-dependency-conflicts"><a href="#What-are-dependency-conflicts" class="headerlink" title="What are dependency conflicts?"></a>What are dependency conflicts?</h2><p>Here is an example, this project has 2 dependencies, and both of them depend on the Guava library with different versions.</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Project dependencies --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.example<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>study-maven-2<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.example<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>study-maven-3<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.example<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>study-maven-2<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.google.guava<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>guava<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>30.1.1-jre<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.example<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>study-maven-3<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.google.guava<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>guava<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>29.0-jre<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>As you all know, we can’t have two classes with the same package and name in one project. So there would be conflicts if different versions of a package are imported. Obviously, in this case, only one version of Guava will be imported.</p>
<span id="more"></span>

<h2 id="How-to-find-dependency-conflicts？"><a href="#How-to-find-dependency-conflicts？" class="headerlink" title="How to find dependency conflicts？"></a>How to find dependency conflicts？</h2><p>I’d like to introduce 2 useful tools to you. The first one is ‘maven-dependency-plugin’. It’s a Maven plugin, so you can use it directly. The command is listed below:</p>
<p><code>mvn dependency:tree -Dverbose -Dincludes=com.google.guava:guava</code></p>
<p>The ‘dependency’ is the plugin prefix, the ‘tree’ is the plugin goal. </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ study-maven ---</span><br><span class="line">[INFO] org.example:study-maven:jar:1.0-SNAPSHOT</span><br><span class="line">[INFO] +- org.example:study-maven-2:jar:1.0-SNAPSHOT:compile</span><br><span class="line">[INFO] |  \\- com.google.guava:guava:jar:30.1.1-jre:compile</span><br><span class="line">[INFO] \\- org.example:study-maven-3:jar:1.0-SNAPSHOT:compile</span><br><span class="line">[INFO]    \\- (com.google.guava:guava:jar:29.0-jre:compile - omitted for conflict with 30.1.1-jre)</span><br></pre></td></tr></table></figure>

<p>The result tells us that 2 dependencies imported the guava package, but only version 30 is available in this project. Version 29 is omitted.</p>
<p>Another tool is an IntelliJ IDEA plugin, called <em>Maven Helper</em>. It’s more convenient if you are using this development editor. You can see the conflicts directly in its panel.</p>
<img src="https://raw.githubusercontent.com/Liam8/img/master/blog/image-20210924183132709.png" alt="image-20210924183132709.png" style="zoom: 33%;" />

<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/image-20210924190701716.png" alt="image-20210924190701716"></p>
<h2 id="How-Maven-chooses-the-version-of-dependency"><a href="#How-Maven-chooses-the-version-of-dependency" class="headerlink" title="How Maven chooses the version of dependency?"></a>How Maven chooses the version of dependency?</h2><p>Actually, the rule is quite simple. You just need to remember 2 rules.</p>
<p>The first one is: <strong>nearest definition wins</strong></p>
<p>“nearest definition” means that the version used will be the closest one to your project in the tree of dependencies.</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">POM</span><br><span class="line">|-- A</span><br><span class="line">|   `-- B 1.0</span><br><span class="line">|-- C</span><br><span class="line">   `-- D</span><br><span class="line">      `-- B 2.0</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>From this dependency tree, we can get 2 dependency paths: <code>root-A-B1.0</code> and <code>root-C-D-B2.0</code>. You can see the first path is shorter than the second one, so according to this rule, version 1.0 will be chosen by Maven.</p>
<p>If two dependency versions are at the same depth, then you’ll need the second rule: <strong>first declaration wins</strong>. It means which dependency imported earlier will be chosen.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">POM</span><br><span class="line">|-- A</span><br><span class="line">|   `-- B 1.0</span><br><span class="line">|-- C</span><br><span class="line">    `-- B 2.0</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Version 1.0 is imported earlier, so it will be the final winner.</p>
<p>More details: <a href="https://maven.apache.org/guides/introduction/introduction-to-dependency-mechanism.html">Introduction to the Dependency Mechanism</a></p>
<p>Do you remember the example I mentioned before? The project imported 2 dependencies, study-maven-2 and study-maven-3. Both of these 2 libraries imported Guava directly.</p>
<p>Let’s have a look at the dependency tree of the project. You can see that version 30 is available, and another version of Guava is omitted. Because version 30 is imported firstly.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ study-maven ---</span><br><span class="line">[INFO] org.example:study-maven:jar:1.0-SNAPSHOT</span><br><span class="line">[INFO] +- org.example:study-maven-2:jar:1.0-SNAPSHOT:compile</span><br><span class="line">[INFO] |  \- com.google.guava:guava:jar:30.1.1-jre:compile</span><br><span class="line">[INFO] \- org.example:study-maven-3:jar:1.0-SNAPSHOT:compile</span><br><span class="line">[INFO]    \- (com.google.guava:guava:jar:29.0-jre:compile - omitted for conflict with 30.1.1-jre)</span><br></pre></td></tr></table></figure>

<p>But there is an exceptional situation, it’s not a usual case; it happens when you importing the same library with a different version in one POM file. Then the latter one will win.</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.google.guava<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>guava<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>29.0-jre<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.google.guava<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>guava<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>27.0.1-jre<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ study-maven ---</span><br><span class="line">[INFO] org.example:study-maven:jar:1.0-SNAPSHOT</span><br><span class="line">[INFO] \\- com.google.guava:guava:jar:27.0.1-jre:compile</span><br></pre></td></tr></table></figure>



<h2 id="How-to-specify-the-version-we-want"><a href="#How-to-specify-the-version-we-want" class="headerlink" title="How to specify the version we want?"></a>How to specify the version we want?</h2><h3 id="3-solutions"><a href="#3-solutions" class="headerlink" title="3 solutions"></a>3 solutions</h3><p>Here I’ll give you 3 solutions.</p>
<p>The first way to achieve this goal is that excluding the version you don’t want when importing the dependency. You need to add some exclusion configurations with the dependency importing. You can do it manually or with the Maven Helper plugin.</p>
<p><img src="https://raw.githubusercontent.com/Liam8/img/master/blog/image-20210924193152686.png" alt="image-20210924193152686"></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.example<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>study-maven-2<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>guava<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.google.guava<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.example<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>study-maven-3<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>After you do that, you can check the dependency tree again.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ study-maven ---</span><br><span class="line">[INFO] org.example:study-maven:jar:1.0-SNAPSHOT</span><br><span class="line">[INFO] \\- org.example:study-maven-3:jar:1.0-SNAPSHOT:compile</span><br><span class="line">[INFO]    \\- com.google.guava:guava:jar:29.0-jre:compile</span><br></pre></td></tr></table></figure>

<p>The second way is importing the version you want directly in your project, according to the rule we mentioned before, the directly imported version will be the available one. Let’s check the dependency tree again, now version 29 is the winner.</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.example<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>study-maven-2<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.example<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>study-maven-3<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.google.guava<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>guava<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>29.0-jre<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ study-maven ---</span><br><span class="line">[INFO] org.example:study-maven:jar:1.0-SNAPSHOT</span><br><span class="line">[INFO] +- org.example:study-maven-2:jar:1.0-SNAPSHOT:compile</span><br><span class="line">[INFO] |  \\- (com.google.guava:guava:jar:30.1.1-jre:compile - omitted for conflict with 29.0-jre)</span><br><span class="line">[INFO] +- org.example:study-maven-3:jar:1.0-SNAPSHOT:compile</span><br><span class="line">[INFO] |  \\- (com.google.guava:guava:jar:29.0-jre:compile - omitted for conflict with 30.1.1-jre)</span><br><span class="line">[INFO] \\- com.google.guava:guava:jar:29.0-jre:compile</span><br></pre></td></tr></table></figure>

<p>The 3rd solution is using dependency management, you can assign a version directly.</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.example<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>study-maven-2<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.example<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>study-maven-3<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencyManagement</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.google.guava<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>guava<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>29.0-jre<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencyManagement</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ study-maven ---</span><br><span class="line">[INFO] org.example:study-maven:jar:1.0-SNAPSHOT</span><br><span class="line">[INFO] +- org.example:study-maven-2:jar:1.0-SNAPSHOT:compile</span><br><span class="line">[INFO] |  \\- com.google.guava:guava:jar:29.0-jre:compile (version managed from 30.1.1-jre)</span><br><span class="line">[INFO] \\- org.example:study-maven-3:jar:1.0-SNAPSHOT:compile</span><br><span class="line">[INFO]    \\- (com.google.guava:guava:jar:29.0-jre:compile - version managed from 30.1.1-jre; omitted for duplicate)</span><br></pre></td></tr></table></figure>

<p>You can see, now version 29 is the winner.</p>
<p>Please attention that the dependency management’s priority is lower than the directly imported one. It means the dependency management can not change the version you imported directly.</p>
<h3 id="What-if-we-have-to-keep-two-versions"><a href="#What-if-we-have-to-keep-two-versions" class="headerlink" title="What if we have to keep two versions?"></a>What if we have to keep two versions?</h3><p>Some libraries only work with the old version of Guava, but some of them only work with the new version.</p>
<p>Let’s say study-maven-2 only works with Guava 30.1, but study-maven-3 can not work without Guava 29.<br>Here are the steps:</p>
<ol>
<li>Create a new project;</li>
<li>Relocate all classes in package ‘com.google.common’ by maven-shade-plugin;</li>
<li>Import the shaded one.</li>
</ol>
<p>Here what we need is the ‘maven-shade-plugin’. This plugin can relocate some classes, by changing its package name.</p>
<p>Here is an example, we’d like to relocate all classes in package ‘com.google.common’ to another package.</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.example<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>shaded-study-maven-3<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.1-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.example<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>study-maven-3<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.1-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">relocations</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>com.google.common<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>org.example.shaded.com.google.common<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">relocations</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>What happened with relocation?</p>
<p>Your code will use the relocated classes after being compiled.</p>
<p>Before relocation, your code was importing the class of Guava from the original package; After relocation, your code is importing the same class from a new package.</p>
<p>Before:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.example;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.google.common.base.Preconditions;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Demo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        Preconditions.checkNotNull(args);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>After:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Source code recreated from a .class file by IntelliJ IDEA</span></span><br><span class="line"><span class="comment">// (powered by FernFlower decompiler)</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> com.example;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.example.shaded.com.google.common.base.Preconditions;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Demo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">Demo</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        Preconditions.checkNotNull(args);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>After you install or deploy the package, you can import the shaded library, there won’t be conflict.</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.example<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>study-maven-2<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.example<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>shaded-study-maven-3<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.1-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="What-if-your-dependency-conflict-with-the-runtime-library"><a href="#What-if-your-dependency-conflict-with-the-runtime-library" class="headerlink" title="What if your dependency conflict with the runtime library?"></a>What if your dependency conflict with the runtime library?</h3><p>In a runtime environment, like Spark cluster is using guava 19.0, but you need to use 30.x version.</p>
<p>You can not shade the library provided by the runtime environment, but you can shade your package.</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.google.guava<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>guava<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>30.1.1-jre<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">relocations</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>com.google.common<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>org.example.shaded.com.google.common<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">relocations</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>There is no best solution here, so you need to make your choice case by case.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Now you know more about the solution to deal with conflicts.</p>
<p>Let’s summarize briefly what we’ve looked at.</p>
<p>Firstly, we’ve talked about how to find dependency conflicts.</p>
<p>Secondly, we’ve got to know how Maven works with conflicts.</p>
<p>Lastly, we’ve discussed the solutions.</p>
]]></content>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title>Details you need to know about Apache Parquet</title>
    <url>/2020/05/31/details-you-need-to-know-about-Apache-Parquet/</url>
    <content><![CDATA[<p>Parquet is a columnar file format that supports nested data. Lots of data systems support this data format because of it’s great advantage of performance.</p>
<span id="more"></span>

<h2 id="File-format"><a href="#File-format" class="headerlink" title="File format"></a>File format</h2><p><img src="https://raw.github.com/apache/parquet-format/master/doc/images/FileLayout.gif" alt="parquet-file-format"></p>
<p>First we should known is that Apache Parquet is a binary encoding like Apache Thrift and Protocol Buffers which are not human-redable, it’s very different from some texual format like JSON, XML and CSV.</p>
<p>In order to identify the beginning and ending of the Parquet file, it use a Magic Number(4 special bytes) as separator. Following the first magic number, there are several Row Groups and then Footer. FileMetaData is placed in Footer, because metadata is written after the data is written. Row Groups are about datas.</p>
<blockquote>
<p>There are three types of metadata: file metadata, column (chunk) metadata and page header metadata. All thrift structures are serialized using the TCompactProtocol.</p>
</blockquote>
<p>ThriftCompactProtocol is another binary encoding from <a href="https://thrift.apache.org/">Apache Thrift</a> project.</p>
<p>Some important conceptions listed below.</p>
<blockquote>
<p>Block (hdfs block): This means a block in hdfs and the meaning is unchanged for describing this file format. The file format is designed to work well on top of hdfs.</p>
<p>File: A hdfs file that must include the metadata for the file. It does not need to actually contain the data.</p>
<p>Row group: A logical horizontal partitioning of the data into rows. There is no physical structure that is guaranteed for a row group. A row group consists of a column chunk for each column in the dataset.</p>
<p>Column chunk: A chunk of the data for a particular column. These live in a particular row group and is guaranteed to be contiguous in the file.</p>
<p>Page: Column chunks are divided up into pages. A page is conceptually an indivisible unit (in terms of compression and encoding). There can be multiple page types which is interleaved in a column chunk.</p>
<p><strong>Hierarchically, a file consists of one or more row groups. A row group contains exactly one column chunk per column. Column chunks contain one or more pages.</strong></p>
</blockquote>
<h2 id="Metadata"><a href="#Metadata" class="headerlink" title="Metadata"></a>Metadata</h2><p>Let’s deep into the metadata.</p>
<p><img src="https://github.com/apache/parquet-format/raw/master/doc/images/FileFormat.gif" alt="metadata"></p>
<p>The definition file: <a href="https://github.com/apache/parquet-format/blob/master/src/main/thrift/parquet.thrift">Parquet thrift definition</a></p>
<h3 id="FileMetaData"><a href="#FileMetaData" class="headerlink" title="FileMetaData"></a>FileMetaData</h3><p>Here is the definition:</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Description for file metadata</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">struct</span> FileMetaData &#123;</span><br><span class="line">  <span class="comment">/** Version of this file **/</span></span><br><span class="line">  <span class="number">1</span>: required i32 version</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Parquet schema for this file.  This schema contains metadata for all the columns.</span></span><br><span class="line"><span class="comment">   * The schema is represented as a tree with a single root.  The nodes of the tree</span></span><br><span class="line"><span class="comment">   * are flattened to a list by doing a depth-first traversal.</span></span><br><span class="line"><span class="comment">   * The column metadata contains the path in the schema for that column which can be</span></span><br><span class="line"><span class="comment">   * used to map columns to nodes in the schema.</span></span><br><span class="line"><span class="comment">   * The first element is the root **/</span></span><br><span class="line">  <span class="number">2</span>: required list&lt;SchemaElement&gt; schema;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Number of rows in this file **/</span></span><br><span class="line">  <span class="number">3</span>: required i64 num_rows</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Row groups in this file **/</span></span><br><span class="line">  <span class="number">4</span>: required list&lt;RowGroup&gt; row_groups</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Optional key/value metadata **/</span></span><br><span class="line">  <span class="number">5</span>: optional list&lt;KeyValue&gt; key_value_metadata</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** String for application that wrote this file.  This should be in the format</span></span><br><span class="line"><span class="comment">   * &lt;Application&gt; version &lt;App Version&gt; (build &lt;App Build Hash&gt;).</span></span><br><span class="line"><span class="comment">   * e.g. impala version 1.0 (build 6cf94d29b2b7115df4de2c06e2ab4326d721eb55)</span></span><br><span class="line"><span class="comment">   **/</span></span><br><span class="line">  <span class="number">6</span>: optional <span class="type">string</span> created_by</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Sort order used for the min_value and max_value fields of each column in</span></span><br><span class="line"><span class="comment">   * this file. Each sort order corresponds to one column, determined by its</span></span><br><span class="line"><span class="comment">   * position in the list, matching the position of the column in the schema.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * Without column_orders, the meaning of the min_value and max_value fields is</span></span><br><span class="line"><span class="comment">   * undefined. To ensure well-defined behaviour, if min_value and max_value are</span></span><br><span class="line"><span class="comment">   * written to a Parquet file, column_orders must be written as well.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * The obsolete min and max fields are always sorted by signed comparison</span></span><br><span class="line"><span class="comment">   * regardless of column_orders.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="number">7</span>: optional list&lt;ColumnOrder&gt; column_orders;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>The field ‘num_rows’ is very useful when data reader wanna to count the data, for instance, when SparkSQL count on some paritioned table, Spark just sum all the ‘num_rows’ of each parquet file belong to those filtered partitions.</p>
<p>The ‘schema’ is the most important part of this metadata, it is defined by a list of SchemaElement, that’s means each field is represented by a SchemaElement.</p>
<h4 id="SchemaElement"><a href="#SchemaElement" class="headerlink" title="SchemaElement"></a>SchemaElement</h4><p>Here is the structure of SchemaElement.</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Represents a element inside a schema definition.</span></span><br><span class="line"><span class="comment"> *  - if it is a group (inner node) then type is undefined and num_children is defined</span></span><br><span class="line"><span class="comment"> *  - if it is a primitive type (leaf) then type is defined and num_children is undefined</span></span><br><span class="line"><span class="comment"> * the nodes are listed in depth first traversal order.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">struct</span> SchemaElement &#123;</span><br><span class="line">  <span class="comment">/** Data type for this field. Not set if the current element is a non-leaf node */</span></span><br><span class="line">  <span class="number">1</span>: optional Type <span class="keyword">type</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** If type is FIXED_LEN_BYTE_ARRAY, this is the byte length of the vales.</span></span><br><span class="line"><span class="comment">   * Otherwise, if specified, this is the maximum bit length to store any of the values.</span></span><br><span class="line"><span class="comment">   * (e.g. a low cardinality INT col could have this set to 3).  Note that this is</span></span><br><span class="line"><span class="comment">   * in the schema, and therefore fixed for the entire file.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="number">2</span>: optional i32 type_length;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** repetition of the field. The root of the schema does not have a repetition_type.</span></span><br><span class="line"><span class="comment">   * All other nodes must have one */</span></span><br><span class="line">  <span class="number">3</span>: optional FieldRepetitionType repetition_type;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Name of the field in the schema */</span></span><br><span class="line">  <span class="number">4</span>: required <span class="type">string</span> name;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Nested fields.  Since thrift does not support nested fields,</span></span><br><span class="line"><span class="comment">   * the nesting is flattened to a single list by a depth-first traversal.</span></span><br><span class="line"><span class="comment">   * The children count is used to construct the nested relationship.</span></span><br><span class="line"><span class="comment">   * This field is not set when the element is a primitive type</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="number">5</span>: optional i32 num_children;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** When the schema is the result of a conversion from another model</span></span><br><span class="line"><span class="comment">   * Used to record the original type to help with cross conversion.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="number">6</span>: optional ConvertedType converted_type;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Used when this column contains decimal data.</span></span><br><span class="line"><span class="comment">   * See the DECIMAL converted type for more details.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="number">7</span>: optional i32 scale</span><br><span class="line">  <span class="number">8</span>: optional i32 precision</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** When the original schema supports field ids, this will save the</span></span><br><span class="line"><span class="comment">   * original field id in the parquet schema</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="number">9</span>: optional i32 field_id;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * The logical type of this SchemaElement</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * LogicalType replaces ConvertedType, but ConvertedType is still required</span></span><br><span class="line"><span class="comment">   * for some logical types to ensure forward-compatibility in format v1.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="number">10</span>: optional LogicalType logicalType</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>The data type of each data column is determined by both ‘type’ and ‘logicalType’ field.</p>
<p>The ‘type’ field support several primitive types:</p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">BOOLEAN: 1 bit boolean</span><br><span class="line">INT32: 32 bit signed ints</span><br><span class="line">INT64: 64 bit signed ints</span><br><span class="line">INT96: 96 bit signed ints</span><br><span class="line">FLOAT: IEEE 32-bit floating point values</span><br><span class="line">DOUBLE: IEEE 64-bit floating point values</span><br><span class="line">BYTE_ARRAY: arbitrarily long byte arrays</span><br></pre></td></tr></table></figure>

<p>Logical types are some type based on primitive type, the field ‘logicalType’ tells data reader to which LogicalType should the primitive type data be transfered.</p>
<blockquote>
<p>Logical types are used to extend the types that parquet can be used to store, by specifying how the primitive types should be interpreted. This keeps the set of primitive types to a minimum and reuses parquet’s efficient encodings. For example, strings are stored as byte arrays (binary) with a UTF8 annotation.</p>
</blockquote>
<p>Many types supported by ‘logicalType’. Such as:</p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">STRING,ENUM,UUID,</span><br><span class="line">DATE,TIME,TIMESTAMP,INTERVAL</span><br><span class="line">INT,DECIMAL</span><br><span class="line">JSON,BSON</span><br><span class="line">LIST,MAP</span><br><span class="line">NULL</span><br></pre></td></tr></table></figure>

<blockquote>
<p>INT annotation can be used to specify the maximum number of bits in the stored value. The annotation has two parameter: bit width and sign, such as:INT(8, true), INT(16, true), INT(32, true), INT(64, true).</p>
</blockquote>
<p>For more details about LogicalType see <a href="https://github.com/apache/parquet-format/blob/master/LogicalTypes.md">Parquet Logical Type Definitions</a></p>
<h4 id="ColumnMetaData"><a href="#ColumnMetaData" class="headerlink" title="ColumnMetaData"></a>ColumnMetaData</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Description for column metadata</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">struct</span> ColumnMetaData &#123;</span><br><span class="line">  <span class="comment">/** Type of this column **/</span></span><br><span class="line">  <span class="number">1</span>: required Type <span class="keyword">type</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Set of all encodings used for this column. The purpose is to validate</span></span><br><span class="line"><span class="comment">   * whether we can decode those pages. **/</span></span><br><span class="line">  <span class="number">2</span>: required list&lt;Encoding&gt; encodings</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Path in schema **/</span></span><br><span class="line">  <span class="number">3</span>: required list&lt;<span class="type">string</span>&gt; path_in_schema</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Compression codec **/</span></span><br><span class="line">  <span class="number">4</span>: required CompressionCodec codec</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Number of values in this column **/</span></span><br><span class="line">  <span class="number">5</span>: required i64 num_values</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** total byte size of all uncompressed pages in this column chunk (including the headers) **/</span></span><br><span class="line">  <span class="number">6</span>: required i64 total_uncompressed_size</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** total byte size of all compressed pages in this column chunk (including the headers) **/</span></span><br><span class="line">  <span class="number">7</span>: required i64 total_compressed_size</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Optional key/value metadata **/</span></span><br><span class="line">  <span class="number">8</span>: optional list&lt;KeyValue&gt; key_value_metadata</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Byte offset from beginning of file to first data page **/</span></span><br><span class="line">  <span class="number">9</span>: required i64 data_page_offset</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Byte offset from beginning of file to root index page **/</span></span><br><span class="line">  <span class="number">10</span>: optional i64 index_page_offset</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Byte offset from the beginning of file to first (only) dictionary page **/</span></span><br><span class="line">  <span class="number">11</span>: optional i64 dictionary_page_offset</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** optional statistics for this column chunk */</span></span><br><span class="line">  <span class="number">12</span>: optional Statistics statistics;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Set of all encodings used for pages in this column chunk.</span></span><br><span class="line"><span class="comment">   * This information can be used to determine if all data pages are</span></span><br><span class="line"><span class="comment">   * dictionary encoded for example **/</span></span><br><span class="line">  <span class="number">13</span>: optional list&lt;PageEncodingStats&gt; encoding_stats;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Each ColumnChunk has only one ColumnMetaData, so one ColumnMetaData defined how to store one column’s data in one row group.</p>
<p>Some fields should be noticed:</p>
<ul>
<li>encodings: just for validation, each column’s encoding is defined in DataPageHeader(see below);</li>
<li>codec: the compression algorithm used, such as SNAPPY, GZIP, LZO and so on;</li>
<li>statistics: statistics information for the column of the row group. Some useful field are showed below, they are very useful for distinct counting or filtering. With the ‘max_value’ and ‘min_value’, SparkSQL can do filter push-down by skipping some row groups.</li>
</ul>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> Statistics &#123;</span><br><span class="line">    ...</span><br><span class="line">   <span class="comment">/** count of null value in the column */</span></span><br><span class="line">   <span class="number">3</span>: optional i64 null_count;</span><br><span class="line">   <span class="comment">/** count of distinct values occurring */</span></span><br><span class="line">   <span class="number">4</span>: optional i64 distinct_count;</span><br><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Min and max values for the column, determined by its ColumnOrder.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * Values are encoded using PLAIN encoding, except that variable-length byte</span></span><br><span class="line"><span class="comment">    * arrays do not include a length prefix.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   <span class="number">5</span>: optional binary max_value;</span><br><span class="line">   <span class="number">6</span>: optional binary min_value;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="PageHeader"><a href="#PageHeader" class="headerlink" title="PageHeader"></a>PageHeader</h4><p>PageHeader is kind of like parent class of DataPageHeader, IndexPageHeader and DictionaryPageHeader, contains some common fields.</p>
<p>Each data page has a DataPageHeader, let’s look into it below.</p>
<h5 id="DataPageHeader"><a href="#DataPageHeader" class="headerlink" title="DataPageHeader"></a>DataPageHeader</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * New page format allowing reading levels without decompressing the data</span></span><br><span class="line"><span class="comment"> * Repetition and definition levels are uncompressed</span></span><br><span class="line"><span class="comment"> * The remaining section containing the data is compressed if is_compressed is true</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">struct</span> DataPageHeaderV2 &#123;</span><br><span class="line">  <span class="comment">/** Number of values, including NULLs, in this data page. **/</span></span><br><span class="line">  <span class="number">1</span>: required i32 num_values</span><br><span class="line">  <span class="comment">/** Number of NULL values, in this data page.</span></span><br><span class="line"><span class="comment">      Number of non-null = num_values - num_nulls which is also the number of values in the data section **/</span></span><br><span class="line">  <span class="number">2</span>: required i32 num_nulls</span><br><span class="line">  <span class="comment">/** Number of rows in this data page. which means pages change on record boundaries (r = 0) **/</span></span><br><span class="line">  <span class="number">3</span>: required i32 num_rows</span><br><span class="line">  <span class="comment">/** Encoding used for data in this page **/</span></span><br><span class="line">  <span class="number">4</span>: required Encoding encoding</span><br><span class="line"></span><br><span class="line">  <span class="comment">// repetition levels and definition levels are always using RLE (without size in it)</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/** length of the definition levels */</span></span><br><span class="line">  <span class="number">5</span>: required i32 definition_levels_byte_length;</span><br><span class="line">  <span class="comment">/** length of the repetition levels */</span></span><br><span class="line">  <span class="number">6</span>: required i32 repetition_levels_byte_length;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**  whether the values are compressed.</span></span><br><span class="line"><span class="comment">  Which means the section of the page between</span></span><br><span class="line"><span class="comment">  definition_levels_byte_length + repetition_levels_byte_length + 1 and compressed_page_size (included)</span></span><br><span class="line"><span class="comment">  is compressed with the compression_codec.</span></span><br><span class="line"><span class="comment">  If missing it is considered compressed */</span></span><br><span class="line">  <span class="number">7</span>: optional <span class="type">bool</span> is_compressed = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** optional statistics for this column chunk */</span></span><br><span class="line">  <span class="number">8</span>: optional Statistics statistics;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>encoding: This field is about how to store <strong>primitive type</strong> data with bytes, for example, a PLAIN encoding means to use 4 bytes, PLAIN_DICTIONARY means to use a dictionary.<br>For more details see <a href="https://github.com/apache/parquet-format/blob/master/Encodings.md">Parquet encoding definitions</a></li>
<li>statistics: statistics information for this page</li>
</ul>
<p>ps: The comment of statistics is wrong, it should be ‘optional statistics for the data in this page’, I have fixed that in my <a href="https://github.com/apache/parquet-format/pull/159">PR</a>.</p>
<p><strong>How encoding work with compression ?</strong></p>
<p>First, the data be encoded , second, encoded output is then compressed with a generic compression algorithm specified in ColumnMetaData like Snappy.</p>
<p>There is another description.</p>
<blockquote>
<p>Encoding: It’s more at application level where data representation is changed. The encoding can also minimize space usage which can give us a kind of compression.<br>Compression : In general it’s the Technic to reduce storage for given data in bytes irrespective of underline data is already encoded or not.</p>
</blockquote>
<h2 id="Parquet-tools"><a href="#Parquet-tools" class="headerlink" title="Parquet-tools"></a>Parquet-tools</h2><p>Parquet-tools is a convenient offical tool to play with Parquet file.</p>
<p>If you use MacOS and homebrew, just install it by <code>brew install parquet-tools</code> .</p>
<p>To show metadata: <code>parquet-tools meta yourfile.parquet</code></p>
<p>Or use hadoop distribution: <code>hadoop jar ./parquet-tools-&lt;VERSION&gt;.jar &lt;command&gt;</code></p>
<p>You’ll get something like this:</p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">creator:     parquet-mr version 1.10.1 (build a89df8f9932b6ef6633d06069e50c9b7970bebd1)</span><br><span class="line">extra:       org.apache.spark.sql.parquet.row.metadata = &#123;&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[&#123;&quot;name&quot;:&quot;v&quot;,&quot;type&quot;:&quot;i [more]...</span><br><span class="line"></span><br><span class="line">file schema: spark_schema</span><br><span class="line">-------------------------------------------------------------------------------------------------------------------</span><br><span class="line">v:           REQUIRED INT32 R:0 D:0</span><br><span class="line">sq:          REQUIRED INT32 R:0 D:0</span><br><span class="line">str:         OPTIONAL BINARY O:UTF8 R:0 D:1</span><br><span class="line"></span><br><span class="line">row group 1: RC:5000 TS:94040</span><br><span class="line">-------------------------------------------------------------------------------------------------------------------</span><br><span class="line">v:            INT32 SNAPPY DO:0 FPO:4 SZ:20056/20050/1.00 VC:5000 ENC:PLAIN,BIT_PACKED</span><br><span class="line">sq:           INT32 SNAPPY DO:0 FPO:20060 SZ:20030/20050/1.00 VC:5000 ENC:PLAIN,BIT_PACKED</span><br><span class="line">str:          BINARY SNAPPY DO:0 FPO:40090 SZ:21960/53940/2.46 VC:5000 ENC:RLE,PLAIN,BIT_PACKED</span><br></pre></td></tr></table></figure>

<p>More details: <a href="https://github.com/apache/parquet-mr/tree/master/parquet-tools">Parquet Tools</a></p>
<h2 id="Use-Parquet-with-Spark"><a href="#Use-Parquet-with-Spark" class="headerlink" title="Use Parquet with Spark"></a>Use Parquet with Spark</h2><p>You can find some guide here: <a href="https://spark.apache.org/docs/latest/sql-data-sources-parquet.html">Spark SQL Guide &gt; Parquet Files</a>.</p>
<p>With Parquet file, spark can do some optimizations.</p>
<blockquote>
<ul>
<li><strong>Column projection</strong><br>  The idea behind this feature is simple: just read the data for columns that the query needs to process and skip the rest of the data. Column-oriented data formats like Parquet can implement this feature quite naturally.</li>
<li><strong>Predicate push down</strong><br>  Predicate push down is another feature of Spark and Parquet that can improve query performance by reducing the amount of data read from Parquet files. Predicate push down works by evaluating filtering predicates in the query against metadata stored in the Parquet files.</li>
</ul>
</blockquote>
<p><strong>Does predicate push down available for nest field ?</strong></p>
<p>Yes. If the field schema is <code>user(id, name)</code>, data will be stored in two columns as <code>user.id</code> and <code>user.name</code>, then both id and name have thier own statistic info in metadata.</p>
<h2 id="Use-Parquet-with-HDFS"><a href="#Use-Parquet-with-HDFS" class="headerlink" title="Use Parquet with HDFS"></a>Use Parquet with HDFS</h2><p>When using Parquet with HDFS, you should care about the row group size.</p>
<blockquote>
<p>Row group size: Larger row groups allow for larger column chunks which makes it possible to do larger sequential IO. Larger groups also require more buffering in the write path (or a two pass write). We recommend large row groups (512MB - 1GB). Since an entire row group might need to be read, we want it to completely fit on one HDFS block. Therefore, HDFS block sizes should also be set to be larger. An optimized read setup would be: 1GB row groups, 1GB HDFS block size, 1 HDFS block per HDFS file.</p>
</blockquote>
<p>The row group size is controlled by config named <code>parquet.block.size</code>, the default value is 128MB,  </p>
<p>You can set it in Spark as below:<br><code>sc.hadoopConfiguration.setInt(&quot;parquet.block.size&quot;,256*1024*1024)</code><br>or<br><code>df.write.option(&quot;parquet.block.size&quot;,256*1024*1024)</code></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://parquet.apache.org/documentation/latest/">Apache Parquet</a></p>
<p><a href="https://db-blog.web.cern.ch/blog/luca-canali/2017-06-diving-spark-and-parquet-workloads-example">Diving into Spark and Parquet Workloads, by Example</a></p>
]]></content>
      <tags>
        <tag>Spark</tag>
        <tag>Apache Parquet</tag>
      </tags>
  </entry>
  <entry>
    <title>Extend Spark Data Source with DataFrame</title>
    <url>/2022/04/25/Extend-Spark-Data-Source/</url>
    <content><![CDATA[<p>In this article, we’ll implement a spark data source for reading and writing a Google spreadsheet, so that you’ll know how to extend the data source of Spark by yourself.</p>
<h2 id="What’s-a-customized-data-source-like"><a href="#What’s-a-customized-data-source-like" class="headerlink" title="What’s a customized data source like?"></a>What’s a customized data source like?</h2><h3 id="read-data-from-Google-Spreadsheet-into-a-DataFrame"><a href="#read-data-from-Google-Spreadsheet-into-a-DataFrame" class="headerlink" title="read data from Google Spreadsheet into a DataFrame"></a>read data from Google Spreadsheet into a DataFrame</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> data = spark.read.format(<span class="string">&quot;google-spreadsheet&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;credentialsPath&quot;</span>, credentialFile)</span><br><span class="line">  .option(<span class="string">&quot;spreadsheetId&quot;</span>, spreadsheetId)</span><br><span class="line">  .option(<span class="string">&quot;sheetName&quot;</span>, sheetName1)</span><br><span class="line">  .load()</span><br></pre></td></tr></table></figure>

<h3 id="write-data-of-a-DataFrame-into-Google-Spreadsheet"><a href="#write-data-of-a-DataFrame-into-Google-Spreadsheet" class="headerlink" title="write data of a DataFrame into Google Spreadsheet"></a>write data of a DataFrame into Google Spreadsheet</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">df.write.format(<span class="string">&quot;google-spreadsheet&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;credentialsPath&quot;</span>, credentialFile)</span><br><span class="line">  .option(<span class="string">&quot;spreadsheetId&quot;</span>, spreadsheetId)</span><br><span class="line">  .option(<span class="string">&quot;sheetName&quot;</span>, sheetName)</span><br><span class="line">  .mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>)</span><br><span class="line">  .save()</span><br></pre></td></tr></table></figure>

<span id="more"></span>

<h2 id="Implement"><a href="#Implement" class="headerlink" title="Implement"></a>Implement</h2><p>In total, we’ll implement 4 files:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">GoogleSpreadsheetDataSource.scala</span><br><span class="line"></span><br><span class="line">GoogleSpreadsheetDataSourceException.scala</span><br><span class="line"></span><br><span class="line">GoogleSpreadsheetDataSourceReader.scala</span><br><span class="line"></span><br><span class="line">GoogleSpreadsheetDataSourceWriter.scala</span><br></pre></td></tr></table></figure>

<ul>
<li><p>GoogleSpreadsheetDataSource is the main class to define a DataSource.</p>
</li>
<li><p>GoogleSpreadsheetDataSourceReader and GoogleSpreadsheetDataSourceWriter tell Spark how to read and write data with a Sheet separately.</p>
</li>
<li><p>GoogleSpreadsheetDataSourceException is just a customized exception.</p>
</li>
</ul>
<p>You can check the full code here <a href="https://github.com/Liam8/spark-google-spreadsheet-datasource">Liam8&#x2F;spark-google-spreadsheet-datasource</a>.</p>
<h3 id="main-class"><a href="#main-class" class="headerlink" title="main class"></a>main class</h3><p>Let’s start with the main class.</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GoogleSpreadsheetDataSource</span> <span class="keyword">extends</span> <span class="title">ReadSupport</span> <span class="keyword">with</span> <span class="title">WriteSupport</span> <span class="keyword">with</span> <span class="title">DataSourceRegister</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createReader</span></span>(options: <span class="type">DataSourceOptions</span>): <span class="type">DataSourceReader</span> = &#123;</span><br><span class="line">    createReader(<span class="literal">null</span>, options)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createReader</span></span>(schema: <span class="type">StructType</span>, options: <span class="type">DataSourceOptions</span>): <span class="type">DataSourceReader</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">GoogleSpreadsheetDataSourceReader</span>(</span><br><span class="line">      options.get(<span class="string">&quot;spreadsheetId&quot;</span>).get(),</span><br><span class="line">      options.get(<span class="string">&quot;sheetName&quot;</span>).get(),</span><br><span class="line">      options.get(<span class="string">&quot;credentialsPath&quot;</span>).get(),</span><br><span class="line">      options.getInt(<span class="string">&quot;bufferSizeOfEachPartition&quot;</span>, <span class="number">100</span>),</span><br><span class="line">      <span class="type">Option</span>(schema),</span><br><span class="line">      options.getBoolean(<span class="string">&quot;firstRowAsHeader&quot;</span>, <span class="literal">true</span>),</span><br><span class="line">      options.getInt(<span class="string">&quot;parallelism&quot;</span>,</span><br><span class="line">        <span class="type">SparkSession</span>.getActiveSession.get.sparkContext.defaultParallelism)</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">shortName</span></span>(): <span class="type">String</span> = <span class="string">&quot;google-spreadsheet&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createWriter</span></span>(</span><br><span class="line">    writeUUID: <span class="type">String</span>, schema: <span class="type">StructType</span>, mode: <span class="type">SaveMode</span>, options: <span class="type">DataSourceOptions</span></span><br><span class="line">  ): <span class="type">Optional</span>[<span class="type">DataSourceWriter</span>] = &#123;</span><br><span class="line">    <span class="type">Optional</span>.of(<span class="keyword">new</span> <span class="type">GoogleSpreadsheetDataSourceWriter</span>(</span><br><span class="line">      mode,</span><br><span class="line">      options.get(<span class="string">&quot;spreadsheetId&quot;</span>).get(),</span><br><span class="line">      options.get(<span class="string">&quot;sheetName&quot;</span>).get(),</span><br><span class="line">      options.get(<span class="string">&quot;credentialsPath&quot;</span>).get(),</span><br><span class="line">      options.getInt(<span class="string">&quot;bufferSizeOfEachPartition&quot;</span>, <span class="number">10</span>),</span><br><span class="line">      schema,</span><br><span class="line">      options.getBoolean(<span class="string">&quot;firstRowAsHeader&quot;</span>, <span class="literal">true</span>)</span><br><span class="line">    ))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">GoogleSpreadsheetDataSource</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">buildSheet</span></span>(credentialsPath: <span class="type">String</span>): <span class="type">Sheets</span> =</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Sheets</span>.<span class="type">Builder</span>(</span><br><span class="line">      <span class="type">GoogleNetHttpTransport</span>.newTrustedTransport,</span><br><span class="line">      <span class="type">JacksonFactory</span>.getDefaultInstance,</span><br><span class="line">      <span class="keyword">new</span> <span class="type">HttpCredentialsAdapter</span>(<span class="type">GoogleCredentials</span>.fromStream(</span><br><span class="line">        <span class="keyword">this</span>.getClass.getClassLoader.getResourceAsStream(credentialsPath)</span><br><span class="line">      ).createScoped(<span class="type">SheetsScopes</span>.<span class="type">SPREADSHEETS</span>))</span><br><span class="line">    ).setApplicationName(<span class="string">&quot;GoogleSpreadsheetDataSourceReader&quot;</span>).build()</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>In this class, we can get options from DataSourceOptions to create the Reader and Writer. If you only want to read data, you don’t need to extend the WriteSupport.</p>
<h3 id="Reader"><a href="#Reader" class="headerlink" title="Reader"></a>Reader</h3><p>In the Reader we mainly need to deal with two things, one is how to generate partitions, and another one is how to read data in one partition.</p>
<p>The most important thing is to implement the DataSourceReader.</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">planInputPartitions</span></span>(): util.<span class="type">List</span>[<span class="type">InputPartition</span>[<span class="type">InternalRow</span>]] = &#123;</span><br><span class="line">    <span class="keyword">val</span> sheets = sheetService.spreadsheets().get(spreadsheetId)</span><br><span class="line">      .setFields(<span class="string">&quot;sheets.properties&quot;</span>).execute().getSheets</span><br><span class="line">    <span class="comment">// sheetName is case insensitive</span></span><br><span class="line">    <span class="keyword">val</span> tmpSheetName = sheetName.toLowerCase</span><br><span class="line">    <span class="keyword">val</span> sheet = sheets.asScala.find(_.getProperties.getTitle.toLowerCase == tmpSheetName).getOrElse(</span><br><span class="line">      <span class="keyword">throw</span> <span class="type">GoogleSpreadsheetDataSourceException</span>(<span class="string">s&quot;Can&#x27;t find the sheet named <span class="subst">$sheetName</span>&quot;</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> rowCount = sheet.getProperties.getGridProperties.getRowCount</span><br><span class="line">    <span class="keyword">val</span> step = <span class="type">Math</span>.ceil(rowCount / parallelism).toInt</span><br><span class="line">    <span class="keyword">val</span> start = <span class="keyword">if</span> (firstRowAsHeader) <span class="number">2</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">    <span class="type">Range</span>.inclusive(start, rowCount, step).map &#123; i =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">GoogleSpreadsheetInputPartition</span>(</span><br><span class="line">        credentialsPath,</span><br><span class="line">        spreadsheetId,</span><br><span class="line">        sheetName,</span><br><span class="line">        i,</span><br><span class="line">        <span class="type">Math</span>.min(i + step - <span class="number">1</span>, rowCount),</span><br><span class="line">        bufferSizeOfEachPartition,</span><br><span class="line">        originalSchema,</span><br><span class="line">        prunedSchema</span><br><span class="line">      ).asInstanceOf[<span class="type">InputPartition</span>[<span class="type">InternalRow</span>]]</span><br><span class="line">    &#125;.toList.asJava</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>In the method <code>planInputPartitions</code>, we create a list of <code>GoogleSpreadsheetInputPartition</code>.</p>
<p><code>GoogleSpreadsheetInputPartition</code> is a subclass of <code>InputPartition</code> that holds the metadata of one partition. </p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GoogleSpreadsheetInputPartition</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">  credentialsPath: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">  spreadsheetId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">  sheetName: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">  startOffset: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">  endOffset: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">  bufferSize: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">  schema: <span class="type">StructType</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">  prunedSchema: <span class="type">Option</span>[<span class="type">StructType</span>]</span></span></span><br><span class="line"><span class="params"><span class="class"></span>) <span class="keyword">extends</span> <span class="title">InputPartition</span>[<span class="type">InternalRow</span>] </span>&#123;</span><br></pre></td></tr></table></figure>

<p>The <code>startOffset: Int, endOffset: Int</code> means from which offset or row current partition should read and where to stop.</p>
 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rowCount = sheet.getProperties.getGridProperties.getRowCount</span><br><span class="line"><span class="keyword">val</span> step = <span class="type">Math</span>.ceil(rowCount / parallelism).toInt</span><br></pre></td></tr></table></figure>

<p>Here we get the total number of rows in the sheet, and then divide it by parallelism to get how many rows should be read in one partition. Then we can calculate the <code>startOffset</code> and <code>endOffset</code> off all pages.</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GoogleSpreadsheetDataSourceReader</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">  spreadsheetId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">  sheetName: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">  credentialsPath: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">  bufferSizeOfEachPartition: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">  var schema: <span class="type">Option</span>[<span class="type">StructType</span>] = <span class="type">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">  firstRowAsHeader: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">  parallelism: <span class="type">Int</span></span></span></span><br><span class="line"><span class="params"><span class="class"></span>) <span class="keyword">extends</span> <span class="title">DataSourceReader</span> <span class="keyword">with</span> <span class="title">SupportsPushDownRequiredColumns</span> </span></span><br></pre></td></tr></table></figure>

<p>Where we really do the data reading is in the class <code>GoogleSpreadsheetInputPartitionReader</code>, I override two methods here <code>override def get(): InternalRow</code> and <code>override def next(): Boolean</code>. It’s very similar like implement an iterator: <code>next()</code> returns true or false to tell if any more data to read; <code>get()</code> to get the next row of data.</p>
<p>To improve the performance, the <code>next()</code> here read a batch of data from the sheet, and then buffs it. So the <code>get()</code> just return a row of data from the buffer.</p>
<h3 id="Writer"><a href="#Writer" class="headerlink" title="Writer"></a>Writer</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GoogleSpreadsheetDataWriter</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">  saveMode: <span class="type">SaveMode</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">  schema: <span class="type">StructType</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">  credentialsPath: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">  spreadsheetId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">  sheetName: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">  bufferSize: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">  firstRowAsHeader: <span class="type">Boolean</span></span></span></span><br><span class="line"><span class="params"><span class="class"></span>) <span class="keyword">extends</span> <span class="title">DataWriter</span>[<span class="type">InternalRow</span>]</span></span><br></pre></td></tr></table></figure>

<p>In file <code>GoogleSpreadsheetDataSourceWriter.scala</code> the most important class is <code>GoogleSpreadsheetDataWriter</code>. In this class, we have a method <code>override def write(record: InternalRow): Unit</code> to write data of one partition into an external system. In my implementation, I put data into a buffer first, and write data into the sheet when the buffer is full.</p>
<h3 id="Register-Datasource"><a href="#Register-Datasource" class="headerlink" title="Register Datasource"></a>Register Datasource</h3><p>If we want to make the data source can be used as the built-in data source like:<code>spark.read.format(&quot;google-spreadsheet&quot;)</code>, we need to register it first. There is a file to do this <code>/src/main/resources/META-INF/services/org.apache.spark.sql.sources.DataSourceRegister</code>.</p>
<p>The content of this file is:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">com.github.liam8.spark.datasource.googlesheet.<span class="type">GoogleSpreadsheetDataSource</span></span><br></pre></td></tr></table></figure>

<h2 id="End"><a href="#End" class="headerlink" title="End"></a>End</h2><p>In a work, the core part of the data source implementation is reading and writing data of one partition. The Spark framework will do most of the rest things for you.</p>
<p>Following this way to extend the Spark data source, basically, you can let Spark read and write data with any storage.</p>
<p><a href="https://github.com/Liam8/spark-google-spreadsheet-datasource">Check the repo in Github</a></p>
]]></content>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
</search>
